he Cloud-native
rchitecture
hite Paper by
Alibaba Cloud
〔阿里云
白皮书
于云长于去爆发于云
业数字化转型最短路径编写说明
编写单位:阿里云计算有限公司
顾问组成员
阿里云
蒋江伟贾扬清丁宇
蚂蚁集团
何征宇
编写组成员(按目录排序)
阿里云
李小平易立司徒放杨浩然李响
罗毅张磊李艳林张瓅玶赵燕标
赵林谢纯良邱戈川员海滨崔飞飞
张勇付宇轩许晓斌雷卷
特步
王海能
定义贡献者(征集活动优选)
陈启涛、莫哈韦、郑富林、王炜、杨树宇、李皓伟CONTEN
The cloud-natlve Archltecture
WhIte Paper by AlIbaba Cloud
云原生架构
为什么需要云原生架构
5
阿里云云原生产品介绍
5云原生产品家族
容器产品家族
2
微服务产品家族
云原生架构的定义
Serverless产品家族
云原生架构定义
47 Service Mesh产品家族
云原生架构原则
消息产品家族
主要架构模式
云原生数据库产品家族
13典型的云原生架构反模式
9云原生数仓产品家族
3
主要云原生技术
6
原生架构实践案例
15容器技术
0案例一:申通快递核心业务系统云原生化
18云原生微服务
案例
案例二:完美日记电商业务案例
23 Serverless
7开放应用模型(OAM)
56案例三:特步业务中台案例(零售、公共云)
Service Mesh技术
案例四:中国联通号卡业务云化案例(传统业
务,专有云)
DevOps
云原生中间件
0案例五: Timing App的 Serverless实践案例
阿里巴巴云原生架构设计
7
云原生架构未来发展趋势
o ACNA( Alibaba Cloud Native
63容器技术发展趋势
Architecting)架构设计方法
67基于云原生的新一代应用编程界面
41企业战略视角
Serverless发展趋势
业务发展视角
2组织能力视角
云原生技术架构视角
3架构持续演进闭环
云原生架构成熟度模型序
U⊥- native
Architecture
The cloud-native Architecture
hite Paper by Alibaba Cloud
回顾过去十年,数字化转型将科技创新与
对云计算服务方式与互联网架构进行整体性升
商业元素不断融合、重构,重新定义了新业态
级,深刻改变着整个商业世界的∏根基。
下的增长极。商业正在从大工业时代的固化范
虽然云原生概念的产生由来已久,但对于
式进化成面向创新型商业组织与新商业物种的
云原生的定义、云原生架构的理解却众说纷纭。
崭新模式。随着数字化转型在中国各行业广泛
到底什么是云原生?容器就代表云原生吗?云
深入,不管是行业巨头,还是中小微企业都不
原生时代互联网分布式架构如何发展?云原生
得不面对数字化变革带来的未知机遇与挑战。
与开源、云计算有什么关系?开发者和企业为
数字化转型的十年,也是云计算高速发展
什么一定要选择云原生架构?面对这些问题,
的十年,这期间新技术不断演进、优秀开源项
每个人都有着不同回答。鉴于此,阿里云结合
目大量涌现,云原生领域进入“火箭式”发展
自身云原生开源、云原生技术、云原生产
阶段。通过树立技术标准与构建开发者生态,
云原生架构以及企业客户上云实践经验,给出
开源将云计算实施逐渐标准化,大幅降低了开
了自己的答案,并通过这本白皮书与社会分享
发者对于云平台的学习成本与接入成本。这都
自己的思考与总结,旨在帮助越来越多的企业
让开发者更加聚焦于业务本身并借助云原生技
顺利找到数字化转型最短路径。
术与产品实现更多业务创新,有效提升企业增
未来十年,云计算将无处不在,像水电煤
长效率,爆发出前所未有的生产力与创造力。
样成为数字经济时代的基础设施,云原生让
可以说,当云计算重构整个产业的同时
云计算变得标准、开放、简单高效、触手可及。
也赋予了企业崭新的增长机遇。正如集装箱的
如何更好地拥抱云计算、拥抱云原生架构、用
出现加速了贸易全球化进程,以容器为代表的
技术加速创新,将成为企业数字化转型升级成
云原生技术作为云计算的服务新界面加速云计
功的关键。
算普及的同时,也在推动着整个商业世界飞速
云计算的下一站,就是云原生
演进。上云成为企业持续发展的必然选择,全
面使用开源技术、云服务构建软件服务的时代
架构的下一站,就是云原生架构;
已经到来。作为云时代释放技术红利的新方式
希望所有的开发者、架构师和技术决策者
云原生技术在通过方法论、工具集和最佳实践
共同定义、共同迎接云原生时代。
重塑整个软件技术栈和生命周期,云原生架构为什么需要云原生架构?
发展背景
计算机软件技术架构进化有两大主要驱动因素,一个是底层硬件升级,另一个是顶层业务发展诉求。
正如伴随着x86硬件体系的成熟,很多应用不再使用昂贵、臃肿的大中型机,转而选择价格更为低廉的
以×86为主的硬件体系,也由此诞生了包括 CORBA、EJB、RPC在内的各类分布式架构;后由于互
联网业务飞速发展,人们发现传统OE架构已经不能满足海量业务规模的并发要求,于是又诞生了阿里
巴巴 Dubbo& RocketMC、 Spring Cloud这样的互联网架构体系
云计算从工业化应用到如今,已走过十五个年头,然而大量应用使用云的方式仍停滞在传统DC时代
虚拟机代替了原来的物理机:使用文件保存应用数据,大量自带的三方技术组件,没有经过架构改造(如
微服务改造)的应用上云,传统的应用打包与发布方式等等。对于如何使用这些技术,没有绝对的对与错,
只是在云的时代不能充分利用云的强大能力,不能从云技术中获得更高的可用性与可扩展能力,也不能
利用云提升发布和运维的效率,是一件非常遗憾的事情
回顾近年来商业世界的发展趋势,数字化转型的出现使得企业中越来越多的业务演变成数字化业务
数字化对于业务渠道、竞争格局、用户体验等诸多方面都带来更加严苛的要求,这就要求技术具备更快
的迭代速度,业务推出速度从按周提升到按小时,每月上线业务量从“几+/月”提升到“几百/天”。
大量数字化业务重构了企业的业务流水线,企业要求这些业务不能有不可接受的业务中断,否则会对客
户体验以及营收可能造成巨大影响
对于企业的CO或者|T主管而言,原来企业内部|建设以“烟筒”模式比较多,每个部门甚至每
个应用都相对独立,如何管理与分配资源成了难题。大家都基于最底层C设施独自向上构建,都需要
单独分配硬件资源,这就造成资源被大量占用且难以被共享。但是上云之后,由于云厂商提供了统一的
aaS能力和云服务,大幅提升了企业laaS层的复用程度,C|O或者主管自然而然想到laS以上层
的系统也需要被统一,使资源、产品可被不断复用,从而能够进一步降低企业运营成本。
所有这些问题都指向一个共同点,那就是云的时代需要新的技术架构,来帮助企业应用能够更好地利
用云计算优势,充分释放云计算的技术红利,让业务更敏捷、成本更低的同时又可伸缩性更灵活,而这
些正好就是云原生架构专注解决的技术点。云原生架构的定义
今天云原生的定义有众多版本,云原生架构的理解也不尽相同,阿里将根据自身的云原生技术、产品
和上云实践,给出自己的理解。
云原生架构定义
从技术的角度,云原生架构是基于云原生技术的一组架构原则和设计模式的集合,旨在将云应用中的
非业务代码部分进行最大化的剥离,从而让云设施接管应用中原有的大量非功能特性(如弹单性、韧性、安全、
可观测性、灰度等),使业务不再有非功能性业务中断困扰的同时,具备轻量、敏捷、高度自动化的特点
传统架构
云原生架构
代码
代码
开发人员
非功能性能力
业务代码编写
三方软件使用
非功能性能力
业务代码编写
三方软件调用
基础设施运维
业务运维
Paas
云原生架构与传统架构的对比
上图展示了在代码中通常包括三部分:业务代码、三方软件、处理非功能特性的代码。其中“业务代
码”指实现业务逻辑的代码;“三方软件”是业务代码中依赖的所有三方库,包括业务库和基础库;“处
理非功能性的代码”指实现高可用、安全、可观测性等非功能性能力的代码。
三部分中只有业务代码是核心,是对业务真正带来价值的,另外两个部分都只算附属物,但随着软件
规模的增大、业务模块规模变大、部署环境増多、分布式复杂性増强,使得今天的软件枃建变得越来越复杂,
6阿里云
对开发人员的技能要求也越来越高。云原生架构相比较传统架构进了一大步,从业务代码中剥离了大量
非功能性特性(不会是所有,比如易用性还不能剥离)到|aS和PaS中,从而减少业务代码开发人
员的技术关注范围,通过云厂商的专业性提升应用的非功能性能力。
此外,具备云原生架构的应用可以最大程度利用云服务和提升软件交付能力,进一步加快软件开发:
1)代码结构发生巨大变化
云原生架构产生的最大影响就是让开发人员的编程模型发生了巨大变化。今天大部分的编程语言中,
都有文件、网络、线程等元素,这些元素为充分利用单机资源带来好处的同时,也提升了分布式编程的
复杂性;因此大量框架、产品涌现,来解决分布式环境中的网络调用问题、高可用问题、CP∪争用问题、
分布式存储问题
在云的环境中,比如“如何获取存储”变成了若干服务,包括对象存储服务、块存储服务和没有随机
访问的文件存储服务。云不仅改变了开发人员获得这些存储能力的界面,还在于云产品在这些 OpenAP丨
或者开源SDκ背后把分布式场景中的高可用挑战、自动扩缩客挑战、安全挑战、运维升级挑战等都处理
了,应用的开发人员就不用在其代码中处理节点宕机前如何把本地保存的内容同步到远端的问题,也不
用处理当业务峰值到来时如何对存储节点进行扩容的问题,而应用的运维人员不用在发现 zero day安全
问题时紧急对三方存储软件进行升级
云把三方软硬件的能力升级成了服务,开发人员的开发复杂度和运维人员的运维工作量都得到极大降
低。显然,如果这样的云服务用得越多,那么开发和运维人员的负担就越少,企业在非核心业务实现上
从必须的负担变成了可控支出。在一些开发能力强的公司中,对这些三方软硬件能力的处理往往是交给
应用框架(或者说公司内自己的中间件)来做的;在云的时代云厂商提供了更具SLA的服务,使得所有
软件公司都可以由此获益。
这些使得业务代码的开发人员技能栈中,不再需要掌握文件及其分布式处理技术,不再需要掌握各种
复杂的网络技术……简化让业务开发变得更敏捷、更快速
2)非功能性特性的大量委托
任何应用都提供两类特性,功能性特性和非功能性特性。功能性特性是真正为业务带来价值的代码,
比如如何建立客户资料、如何处理订单、如何支付等等;即使是一些通用的业务功能特性,比如组织管理
业务字典管理、搜索等等也是紧贴业务需求的。非功能性特性是没有给业务带来直接业务价值,但通常
又是必不可少的特性,比如高可用能力、容灾能力、安全特性、可运维性、易用性、可测试性、灰度发
布能力等等。The cloud-nctive Architecture White Paper
不能说云计算解决了所有非功能性问题,但确实大量非功能性特性,特别是分布式环境下复杂非功能
性问题,被云产品处理掉了。以大家最头疼的高可用为例,云产品在多个层面为应用提供了解决方案:
虛机:当虚机检测到底层硬件异常时,自动帮助应用做热迁移,迁移后的应用不需重新启动而仍然具
备对外服务的能力,应用对整个迁移过程都不会有任何感知
容器:有时应用所在的物理机是正常的,只是应用自身的问题(比如bug、资源耗尽等)而无法正常
对外提供服务。容器通过监控检查探测到进程状态异常,从而实施异常节点的下线、新节点上线和生产
流量的切换等操作,整个过程自动完成而无需运维人员千预;
云服务:如果应用把“有状态”部分都交给了云服务(如缓存、数据库、对彖存储等),加上全局对
象的持有小型化或具备从磁盘快速重建能力,由于云服务本身是具备极强的高可用能力,那么应用本身
会变成更薄的“无状态”应用,因为高可用故障带来的业务中断会降至分钟级;如果应用是N-M的对等
架构架构模式,那么结合 Load Balancer产品可获得几乎无损的高可用能力!
3)高度自动化的软件交付
软件一旦开发完成,需要在公司内外部各类环境中部署和交付,以将软件价值交给最终客户。软件交
付的困难在于开发环境到生产环境的差异(公司环境到客户环境之间的差异)以及软件交付和运维人员
的技能差异,填补这些差异的是一大堆安装手册、运维手册和培训文档。容器以一种标准的方式对软件
打包,容器及相关技术则帮助屏蔽不同环境之间的差异,进而基于容器做标准化的软件交付。
对自动化交付而言,还需要一种能够描述不同环境的工具,让软件能够“理解”目标环境、交付內容、
配置清单并通过代码去识别目标环境的差异,根据交付内容以“面向终态”的方式完成软件的安装、配置
运行和变更。
基于云原生的自动化软件交付相比较当前的人工软件交付是一个巨大的进步。以微服务为例,应用微
服务化以后,往往被部署到成千上万个节点上,如果系统不具备高度的自动化能力,任何一次新业务的
上线,都会带来极大的工作量挑战,严重时还会导致业务变更超过上线窗口而不可用。
2云原生架构原则
云原生架构本身作为一种架构,也有若千架构原则作为应用架构的核心架构控制面,通过遵从这些架
构原则可以让技术主管和架构师在做技术选择时不会出现大的偏差。阿里云
1)服务化原则
当代码规模超出小团队的合作范围时,就有必要进行服务化拆分了,包括拆分为微服务架构、小服务
( Mini service)架构,通过服务化架构把不同生命周期的模块分离出来,分别进行业务迭代,避免迭
代频繁模块被慢速模块拖慢,从而加快整体的进度和稳定性。同时服务化架枃以面向接口编程,服务内
部的功能高度内聚,模块间通过公共功能模块的提取增加软件的复用程度。
分布式环境下的限流降级、熔断隔仓、灰度、反压、零信仼安全等,本质上都是基于服务流量(而非
网络流量)的控制策略,所以云原生架构强调使用服务化的目的还在于从架构层面抽象化业务模块之间
的关系,标准化服务流量的传输,从而帮助业务模块进行基于服务流量的策略控制和治理,不管这些服
务是基于什么语言开发的。
2)弹性原则
大部分系统部署上线需要根据业务量的估算,准备一定规模的机器,从提出采购申请,到供应商洽谈、
机器部署上电、软件部署、性能压测,往往需要好几个月甚至一年的周期;而这期间如果业务发生变化了,
重新调整也非常困难。弹性则是指系统的部署规模可以随着业务量的变化自动伸缩,无须根据事先的容
量规划准备固定的硬件和软件资源。好的弹性能力不仅缩短了从采购到上线的时间,让企业不用操心额
外软硬件资源的成本支出(闲置成本),降低了企业的「成本,更关键的是当业务规模面临海量突发性
扩张的时候,不再因为平时软硬件资源储备不足而“说不”,保障了企业收益
3)可观测原则
今天大部分企业的软件规模都在不断增长,原来单机可以对应用做完所有调试,但在分布式环境下需
要对多个主机上的信息做关联,才可能回答清楚服务为什么宕机、哪些服务违反了其定义的SLO、目前
的故障影响哪些用户、最近这次变更对哪些服务指标带来了影响等等,这些都要求系统具备更强的可观
测能力。可观测性与监控、业务探活、APM等系统提供的能力不同,前者是在云这样的分布式系统中,
主动通过日志、链路跟踪和度量等手段,让一次APP点击背后的多次服务调用的耗时、返回值和参数都
清晰可见,甚至可以下钻到每次三方软件调用、SQL请求、节点拓扑、网络响应等,这样的能力可以使
运维、开发和业务人员实时掌握软件运行情况,并结合多个维度的数据指标,获得前所未有的关联分析
能力,不断对业务健康度和用户体验进行数字化衡量和持续优化。
韧性原则
当业务上线后,最不能接受的就是业务不可用,让用户无法正常使用软件,影响体验和收入。韧性代
表了当软件所依赖的软硬件组件岀现各种异常时,软件表现出来的抵御能力,这些异常通常包括硬件故障、The cloud-nctive Architecture White Paper
硬件资源瓶颈(如CPU/网卡带宽耗尽)、业务流量超出软件设计能力、影响机房工作的故障和灾难、
软件bug、黑客攻击等对业务不可用带来致命影响的因素。
韧性从多个维度诠释了软件持续提供业务服务的能力,核心目标是提升软件的MTBF( Mean time
Between Failure,平均无故障时间)。从架构设计上,韧性包括服务异步化能力、重试/限流/降级/
熔断/反压、主从模式、集群模式、AZ内的高可用、单元化、跨 region容灾、异地多活容灾等。
5)所有过程自动化原则
技术往往是把“双刃剑”,容器、微服务、 DevOps、大量第三方组件的使用,在降低分布式复杂性
和提升迭代速度的同时,因为整体增大了软件技术栈的复杂度和组件规模,所以不可避免地带来了软件
交付的复杂性,如果这里控制不当,应用就无法体会到云原生技术的优势。通过|aC( Infrastructure as
Code)、 Gitops、OAM( Open Application Model)、 Kubernetes operator和大量自动化交付工
具在C|CD流水线中的实践,一方面标准化企业内部的软件交付过程,另一方面在标准化的基础上进行
自动化,通过配置数据自描述和面向终态的交付过程,让自动化工具理解交付目标和环境差异,实现整
个软件交付和运维的自动化。
6)零信任原则
零信任安全针对传统边界安全架构思想进行了重新评估和审视,并对安全架构思路给出了新建议。其
核心思想是,默认情况下不应该信任网络内部和外部的任何人/设备/系统,需要基于认证和授权重构访
问控制的信任基础,诸如|P地址、主机、地理位置、所处网络等均不能作为可信的凭证。零信仼对访问
控制进行了范式上的颠覆,引导安全体系架构从“网络中心化”走向“身份中心化”,其本质诉求是以
身份为中心进行访问控制
零信任第一个核心问题就是 Identity,赋予不同的Enty不同的 Identity,解决是谁在什么环境下访
问某个具体的资源的问题。在研发、测试和运维微服务场景下, Identity及其相关策略不仅是安全的基础,
更是众多(资源,服务,环境)隔离机制的基础;在员工访问企业内部应用的场景下, Identity及其相关
策略提供了灵活的机制来提供随时随地的接入服务。
7)架构持续演进原则
今天技术和业务的演进速度非常快,很少有一开始就清晰定义了架构并在整个软件生命周期里面都适
用,相反往往还需要对架构进行一定范围內的重构,因此云原生架构本身也应该和必须是一个具备持续
演进能力的架构,而不是一个封闭式架构。除了增量迭代、目标选取等因素外,还需要考虑组织(例如
架构控制委员会)层面的架构治理和风险控制,特别是在业务高速迭代情况下的架构、业务、实现平衡
10〔阿里云
关系。云原生架构对于新建应用而言的架枃控制策略相对容易选择(通常是选择弹性、敏捷、成本的维度)
但对于存量应用向云原生架构迁移,则需要从架构上考虑遗留应用的迁出成本/风险和到云上的迁入成本
/风险,以及技术上通过微服务/应用网关、应用集成、适配器、服务网格、数据迁移、在线灰度等应用
和流量进行细颗粒度控制。
3主要架构模式
B
云原生架构有非常多的架构模式,这里选取一些对应用收益更大的主要架构模式进行讨论
1)服务化架构模式
服务化架构是云时代构建云原生应用的标准架构模式,要求以应用模块为颗粒度划分一个软件,以接
口契约(例如1DL)定义彼此业务关系,以标准协议(HTTP、gRPC等)确保彼此的互联互通,结合
DDD(领域模型驱动)、TDD(测试驱动开发)、容器化部署提升每个接口的代码质量和迭代速度。服
务化架构的典型模式是微服务和小服务( Mini service)模式,其中小服务可以看做是一组关系非常密切
的服务的组合,这组服务会共享数据,小服务模式通常适用于非常大型的软件系统,避免接口的颗粒度
太细而导致过多的调用损耗(特别是服务间调用和数据一致性处理)和治理复杂度。
通过服务化架构,把代码模块关系和部署关系进行分离,每个接口可以部署不同数量的实例,单独扩
缩容,从而使得整体的部署更经济。此外,由于在进程级实现了模块的分离,每个接口都可以单独升级,
从而提升了整体的迭代效率。但也需要注意,服务拆分导致要维护的模块数量增多,如果缺乏服务的自
动化能力和治理能力,会让模块管理和组织技能不匹配,反而导致开发和运维效率的降低。
Mesh化架构模式
Mesh化架构是把中间件框架(比如RPC、缓存、异步消息等)从业务进程中分离,让中间件SDK
与业务代码进一步解耦,从而使得中间件升级对业务进程没有影响,甚至迁移到另外一个平台的中间件
也对业务透明。分离后在业务进程中只保留很“薄”的cent部分, Client通常很少变化,只负责与
Mesh进程通讯,原来需要在SDK中处理的流量控制、安全等逻辑由Mesh进程完成。整个架构如下
图所示。The cloud-nctive Architecture White Paper
容器/主机
业务进程
容器/主机
业务代码
业务进程
RPC Redis MQ
DB
SDKSDKSDKSDK
业务代码
SDK协议标准协议
DK
Redis
SDK
Mesh进程
(流量控制、安全策略、微隔离…)
SDK协议
新协议、加密,、染色…
网络
网络
传统架构
Mesh化架构
实施Mesh化架构后,大量分布式架构模式(熔断、限流、降级、重试、反压、隔仓……)都由
Mesh进程完成,即使在业务代码的制品中并没有使用这些三方软件包;同时获得更好的安全性(比如
零信任架构能力)、按流量进行动态环境隔离、基于流量做冒烟/回归测试等。
Serverless模式
和大部分计算模式不同, Serverless将“部署”这个动作从运维中“收走”,使开发者不用关心应
用在哪里运行,更不用关心装什么OS、怎么配置网络、需要多少CPU
从架构抽象上看,当业务
流量到来/业务事件发生时,云会启动或调度一个已启动的业务进程进行处理,处理完成后云自动会关闭
/调度业务进程,等待下一次触发,也就是把应用的整个运行时都委托给云。
今天 Serverless还没有达到任何类型的应用都适用的地步,因此架构决策者需要关心应用类型
是否适合于 Serverless运算。如果应用是有状态的,云在进行调度时可能导致上下文丢失,毕竟
Serverless的调度不会帮助应用做状态同步;如果应用是长时间后台运行的密集型计算任务,会得不到
太多 Serverless的优势;如果应用涉及到频繁的外部(网络或者存储,以及服务间调用),也因为
繁重的1O负担、时延大而不适合。 Serverless非常适合于事件驱动的数据计算任务、计算时间短的请
求/响应应用、没有复杂相互调用的长周期任务。
4存储计算分离模式
分布式环境中的CAP困难主要是针对有状态应用,因为无状态应用不存在C(一致性)这个维度
因此可以获得很好的A(可用性)和P(分区容错性),因而获得更好的弹性。在云环境中,推荐把各
12〔阿里云
类暂态数据(如 session)、结构化和非结构化持久数据都采用云服务来保存,从而实现存储计算分离。
但仍然有一些状态如果保存到远端缓存,会造成交易性能的明显下降,比如交易会话数据太大、需要不
断根据上下文重新获取等,则可以考虑通过采用 Event Log+快照(或 Check Point)的方式,实现重
启后快速增量恢复服务,减少不可用对业务的影响时长。
5)分布式事务模式
微服务模式提倡毎个服务使用私有的数据源,而不是像单体这样共享数据源,但往往大颗粒度的业务
需要访问多个微服务,必然带来分布式事务问题,否则数据就会岀现不一致。架构师需要根据不同的场
景选择合适的分布式事务模式。
传统采用XA模式;虽然具备很强的一致性:但是性能差;
基于消息的最终一致性(BASE)通常有很高的性能,但是通用性有限,且消息端只能成功而不能触发消息
生产端的事务回滚;
TcC模式完全由应用层来控制事务,事务隔离性可控,也可以做到比较高效;但是对业务的侵入性非常强
设计开发维护等成本很高;
SAGA模式与TCC模式的优缺点类似但没有ty这个阶段,而是每个正向事务都对应一个补偿事务,也是
开发维护成本高;
开源项目 SEATA的AT模式非常高性能且无代码开发工作量,且可以自动执行回滚操作,同时也存在一些
使用场景限制。
可观测架构
可观测架构包括 Logging、 Tracing、 Metrics三个方面,其中 Logging提供多个级别( verbose
debug/warning/error/fatal)的详细信息跟踪,由应用开发者主动提供; Tracing提供一个请求从前端
到后端的完整调用链路跟踪,对于分布式场景尤其有用; Metrics则提供对系统量化的多维度度量。
架构决策者需要选择合适的、支持可观测的开源框架(比如 Open Tracing、 Open Telemetry),
并规范上下文的可观测数据规范(例如方法名、用户信息、地理位置、请求参数等),规划这些可观测
数据在哪些服务和技术组件中传播,利用日志和 tracing信息中的 span id/trace id,确保进行分布式链
路分析时有足够的信息进行快速关联分析。
由于建立可观测性的主要目标是对服务SLo( Service Level Objective)进行度量,从而优化
SLA,因此架构设计上需要为各个组件定义清晰的SLO,包括并发度、耗时、可用时长、容量等。
7)事件驱动架构
13The cloud-nctive Architecture White Paper
事件驱动架构(EDA, Event Driven Architecture)本质上是一种应用/组件间的集成架构模式
典型的事件驱动架构如下图
Subscribe
Event
Producer
Topic
Event
Consumer
事件和传统的消息不同,事件具有 schema,所以可以校验 event的有效性,同时EDA具备QoS
保障机制,也能够对事件处理失败进行响应。事件驱动架构不仅用于(微)服务解耦,还可应用于下面
的场景中
增强服务韧性:由于服务间是异步集成的,也就是下游的任何处理失败甚至宕机都不会被上游感知,自然也
就不会对上游带来影响;
cQRs( Command Query Responsibility Segregation):把对服务状态有影响的命令用事件来发起,
而对服务状态没有影响的查询才使用同步调用的AP接口;结合EDA中的 Event Sourcing可以用于维护
数据变更的一致性,当需要重新构建服务状态时,把EDA中的事件重新“播放”一遍即可;
数据变化通知:在服务架构下,往往一个服务中的数据发生变化,另外的服务会感兴趣,比如用户订单
完成后,积分服务、信用服务等都需要得到事件通知并更新用户积分和信用等级
构建开放式接口:在EDA下,事件的提供者并不用关心有哪些订阅者,不像服务调用的场景—数据的产
生者需要知道数据的消费者在哪里并调用它,因此保持了接口的开放性;
事件流处理:应用于大量事件流(而非离散事件)的数据分析场景,典型应用是基于Kaka的日志处理
基于事件触发的响应:在|oT时代大量传感器产生的数据,不会像人机交互一样需要等待处理结果的返回
天然适合用EDA来构建数据处理应用。
4典型的云原生架构反模式
技术往往像一把双刃剑,阿里在自己和帮助客户做云原生架构演进的时候,会充分考虑根据不同的场
景选择不同的技术,下面是我们总结的一些典型云原生架构反模式。
1)庞大的单体应用
庞大单体应用的最大问题在于缺乏依赖隔离,包括代码耦合带来的责任不清、模块间接口缺乏治理而
带来变更影响扩散、不同模块间的开发进度和发布时间要求难以协调
子模块不稳定导致整个应用
都变慢、扩容时只能整体扩容而不能对达到瓶颈的模块单独扩睿…因此当业务模块可能存在多人开发
1〔阿里云
的时候,就需要考虑通过服务化进行一定的拆分,梳理聚合根,通过业务关系确定主要的服务模块以及
些模块的边界、清晰定义模块之间的接口,并让组织关系和架构关系匹配。
阿里巴巴在淘宝业务快速发展阶段,就遇到过上百人维护一个核心单体应用,造成了源码冲突、多团
队间协同代价高的严重问题。为了支撑不断增长的流量,该应用需要不断增加机器,很快后端数据库连
接很快就到达了上限。在还没有“微服务”概念的2008年,阿里巴巴决定进行服务化架构拆分,当时
思路就是微服务架构,第一个服务从用户中心开始建设,后续交易、类目、店铺、商品、评价中心等服
务陆续从单体中独立出来,服务之间通过远程调用通信,每个中心由独立团队专门维护,从而解决了研
发协同问题,以及按规模持续水平扩展的问题。
2)单体应用“硬拆”为微服务
服务的拆分需要适度,过分服务化拆分反而会导致新架构与组织能力的不匹配,让架构升级得不到技
术红利,典型的例子包括
小规模软件的服务拆分:软件规模不大,团队人数也少,但是为了微服务而微服务,强行把耦合度高、代码
量少的模块进行服务化拆分,一次性的发布需要拆分为多个模块分开发布和维护;
数据依赖:服务虽然拆分为多个,但是这些服务的数据是紧密耦合的,于是让这些服务共享数据库,导致数
据的变化往往被扇出到多个服务中,造成服务间数据依赖;
性能降低:当耦合性很强的模块被拆分为多个微服务后,原来的本地调用变成了分布式调用,从而让响应时
间变大了上千倍,导致整个服务链路性能急剧下降。
3)缺乏自动化能力的微服务
软件架构中非常重要的一个维度就是处理软件复杂度问题,一旦问题规模提升了很多,那就必须重新
考虑与之适应的新方案。在很多软件组织中,开发、测试和运维的工作单位都是以进程为单位,比如把
整个用户管理作为一个单独的模块进行打包、发布和运行;而迸行了微服务拆分后,这个用户管理模块
可能被分为用户信息管理、基本信息管理、积分管理、订单管理等多个模块,由于仍然是每个模块分别
打包、发布和运行,开发、测试和运维人员的人均负责模块数就直线上升,造成了人均工作量增大,也
就增加了软件的开发成本。
实际上,当软件规模进一步变大后,自动化能力的缺失还会带来更大的危害。由于接口增多会带来测
试用例的增加,更多的软件模块排队等待测试和发布,如果缺乏自动化会造成软件发布时间变长,在多
环境发布或异地发布时更是需要专家来处理环境差异带来的影响。同时更多的进程运行于一个环境中,
缺乏自动化的人工运维容易给环境带来不可重现的影响,而一旦发生人为运维错误又不容易“快速止血
造成了故障处理时间变长,以及使得日常运维操作都需要专家才能完成。所有这些问题导致软件交付时
变长、风险提升以及运维成本的增加。
15主要云原生技术
容器技术
1)容器技术背景与价值
容器作为标准化软件单元,它将应用及其所有依赖项打包,使应用不再受环境限制,在不同计算环境
间快速、可靠地运行。
APP
APP
Bin/Library
Bin/Library
APP
Operating System Operating System
Bin/ Library Bin/ Library
Bin/ Library
APP
APP
APP
Hypervisor
Container Runtime
Operating System
Operating System
Operating System
Hardware
Hardware
Hardware
Traditional Deployment
Virtualized Deployment
Container Deployment
传统、虛拟化及容器部署模式比较
虽然2008年Lnux就提供了 groups资源管理机制、 Linux Namespace视图隔离方案,让应
用得以运行在独立沙箱环境中,避免相互间冲突与影晌;但直到 Docker容器引擎的开源,才很大程度
上降低了容器技术的使用复杂性,加速了容器技术普及。 Docker容器基于操作系统虚拟化技术,共享操
作系统内核、轻量、没有资源损耗、秒级启动,极大提升了系统的应用部署密度和弹性。更重要的是,
Docker提出了创新的应用打包规范
Docker镜像,解耦了应用与运行环境,使应用可以在不同计
算环境间一致、可靠地运行。借助容器技术呈现了一个优雅的抽象场景:让开发所需要的灵活性、开放
性和运维所关注的标准化、自动化达成相对平衡。容器镜像迅速成为了应用分发的工业标准。
16阿里云
随后开源的 Kubernetes,凭借优秀的开放性、可扩展性以及活跃开发者社区,在客器编排之战中脱颢而岀,成
为分布式资源调度和自动化运维的事实标准。 Kubernetes屏蔽了|aaS层基础架构的差异并凭借优良的可移植性
帮助应用一致地运行在包括数据中心、云、边缘计算在内的不同环境。企业可以通过 Kubernetes,结合自身业务特
征来设计自身云架构,从而更好支持多云/混合云,免去被厂商锁定的顾虑。伴随着容器技术逐步标准化,进一步促
进了容器生态的分工和协同。基于 Kubernetes,生态社区开始构建上层的业务抽象,比如服务网格lsto、机器学
习平台 Kubeflow、无服务器应用框架 Native等。
在过去几年,容器技术获得了越发广泛的应用的同时,三个核心价值最受用户关注
敏捷
器技术提升企业∏架构敏捷性的同时,让业务迭代更加迅捷,为创新探索提供了坚实的技术保障。比如
疫情期间,教育、视频、公共健康等行业的在线化需求突现爆发性高速增长,很多企业通过容器技术适时把握了
突如其来的业务快速增长机遇。据统计,使用容器技术可以获得3~10倍交付效率提升,这意味着企业可以更快
速的迭代产品,更低成本进行业务试错。
弹性
在互联网时代,企业∏系统经常需要面对促销活动、突发事件等各种预期内外的爆发性流量増长。通过容
器技术,企业可以充分发挥云计算弹性优势,降低运维成本。一般而言,借助容器技术,企业可以通过部署密度
提升和弹性降低50%的计算成本。以在线教育行业为例,面对疫情之下指数级增长的流量,教育信息化应用工
具提供商希沃 Seewo利用阿里云容器服务ACK和弹性容器实例ECl大大满足了快速扩容的迫切需求,为数
十万名老师提供了良好的在线授课环境,帮助百万学生进行在线学习。
可移植性
容器已经成为应用分发和交付的标准技术,将应用与底层运行环境进行解耦; Kubernetes成为资源调度
和编排的标准,屏蔽了底层架构差异性,帮助应用平滑运行在不同基础设施上。CNCF云原生计算基金会推岀
了 Kubernetes-致性认证;进一步保障了不同K8s实现的兼容性,这也让企业愿意采用容器技术来构建云时
代应用基础设施。
2)容器编排
Kubernetes已经成为容器编排的事实标准,被广泛用于自动部署,扩展和管理容器化应用。 Kubernetes提
供了分布式应用管理的核心能力
资源调度:根据应用请求的资源量CPU、 Memory,或者GPU等设备资源,在集群中选择合适的节点来运
行应用
应用部署与管理:支持应用的自动发布与应用的回滚,以及与应用相关的配置的管理;也可以自动化存储卷
的编排,让存储卷与容器应用的生命周期相关联
17The cloud-nctive Architecture White Paper
自动修复: Kubernetes可以会监测这个集群中所有的宿主机;当宿主机或者○s出现故障,节点健康检查
会自动进行应用迁移;K8s也支持应用的自愈,极大简化了运维管理的复杂性
服务发现与负载均衡:通过 Service资源岀现各种应用服务,结合DNS和多种负载均衡机制,支持容器化
应用之间的相互通信;
弹性伸缩:K8s可以监测业务上所承担的负载,如果这个业务本身的CPU利用率过高,或者响应时间过长,
它可以对这个业务进行自动扩容。
Kubernetes的控制平面包含四个主要的组件: API Server、 Controller、 Scheduler以及etcd。如
下图所示:
Cloud
Provider
Netword Edge
kubectl
Container Runtime
kubelet
controller
kub
Balancer
manager
API
End Users
server
Container Runtime
Kubernetes在容器编排中有几个关键设计理念
声明式AP:开发者可以关注于应用自身,而非系统执行细节。比如 Deployment(无状态应用)
Statefulset(有状态应用)、Job(任务类应用)等不同资源类型,提供了对不同类型工作负载的抽象;对
Kubernetes实现而言,基于声明式AP的eve- - triggered实现比“ege- -triggered”方式可以提
供更加健壮的分布式系统实现。
可扩展性架构:所有K8s组件都是基于一致的、开放的AP实现和交互;三方开发者也可通过CRD( Custom
Resource Definition) Operator等方法提供领域相关的扩展实现,极大提升了K8s的能力。
18〔阿里云
可移植性:K8s通过一系列抽象如 Loadbalance service(负载均衡服务)、CN(容器网络接口)、CS|(容
器存储接口),帮助业务应用可以屏蔽底层基础设施的实现差异,实现容器灵活迁移的设计目标
云原生微服务
品
1)微服务发展背景
过去开发一个后端应用最为直接的方式就是通过单一后端应用提供并集成所有的服务,即单体模式。随着
业务发展与需求不断增加,单体应用功能愈发复杂,参与开发的工程师规模可能由最初几个人发展到十几人
应用迭代效率由于集中式研发、测试、发布、沟通模式而显著下滑。为了解决由单体应用模型衍生的过度集中
式项目迭代流程,微服务模式应运而生。
微服务模式将后端单体应用拆分为松耦合的多个子应用,每个子应用负责一组子功能。这些子应用称为“微
服务”,多个“微服务”共同形成了一个物理独立但逻辑完整的分布式微服务体系。这些微服务相对独立,通
过解耦研发、测试与部署流程,提高整体迭代效率。此外,微服务模式通过分布式架构将应用水平扩展和冗余
部署,从根本上解决了单体应用在拓展性和稳定性上存在的先天架构缺陷。但也要注意到微服务模型也面临着
分布式系统的典型挑战:如何高效调用远程方法、如何实现可靠的系统容量预估、如何建立负载均衡体系、如
何面向松耦合系统进行集成测试、如何面向大规模复杂关联应用的部署与运维
在云原生时代,云原生微服务体系将充分利用云资源的高可用和安全体系,让应用获得更有保障的弹性、
可用性与安全性。应用构建在云所提供的基础设施与基础服务之上,充分利用云服务所带来的便捷性、稳定性,
降低应用架构的复杂度。云原生的微服务体系也将帮助应用架构全面升级,让应用天然具有更好的可观测性
可控制性、可容错性等特性。
微服务设计约束
相较于单体应用,微服务架构的架构转变,在提升开发、部署等环节灵活性的同时,也提升了在运維、监
控环节的复杂性。在结合实践总结后,我们认为设计一个优秀的微服务系统应遵循以下设计约束
微服务个体约束
个设计良好的微服务应用,所完成的功能在业务域划分上应是相互独立的。与单体应用强行绑定语言和技术栈
相比,这样做的好处是不同业务域有不同的技术选择权,比如推荐系统采用 Python实现效率可能比Java要高效得
多。从组织上来说,微服务对应的团队更小,开发效率也更高
个微服务团队一顿能吃掉两张披萨饼”、
微服务应用应当能至少两周完成一次迭代”,都是对如何正确划分微服务在业务域边界的隐喻和标准。总结来说,微
服务的“微”并不是为了微而微,而是按照问题域对单体应用做合理拆分。
19The cloud-nctive Architecture White Paper
进一步,微服务也应具备正交分解特性,在职责划分上专注于特定业务并将之做好,即SOLD原则中单一职责
原则(SRP, Single Responsibility Principle)。如果当一个微服务修改或者发布时,不应该影响到同一系统里另
个微服务的业务交互。
微服务与微服务之间的横向关系
在合理划分好微服务间的边界后,主要从微服务的可发现性和可交互性处理服务间的横向关系。
微服务的可发现性是指当服务A发布和扩缩容的时候,依赖服务A的服务B如何在不重新发布的前提下,如何
能够自动感知到服务A的变化?这里需要引入第三方服务注册中心来满足服务的可发现性;特别是对于大规模微服
务集群,服务注册中心的推送和扩展能力尤为关键。
微服务的可交互性是指服务A采用什么样的方式可以调用服务B。由于服务自治的约束,服务之间的调用需
要采用与语言无关的远程调用协议,比如REST协议很好的满足了“与语言无关”和“标准化”两个重要因素,
但在高性能场景下,基于DL的二进制协议可能是更好的选择。另外,目前业界大部分微服务实践往往没有达到
HATEOAS启发式的REST调用,服务与服务之间需要通过事先约定接口来完成调用。为了进一步实现服务与服
务之间的解耦,微服务体系中需要有一个独立的元数据中心来存储服务的元数据信息,服务通过查询该中心来理解发
起调用的细节。
伴随着服务链路的不断变长,整个微服努系统也就变得越来越脆弱,因此面向失败设计的原则在微服务体系中就
显得尤为重要。对于微服务应用个体,限流、熔断、隔仓、负载均衡等增强服务韧性的机制成为了标配。为进一步提
升系统吞吐能力、充分利用好机器资源,可以通协程、Rx模型、异步调用、反压等手段来实现。
微服务与数据层之间的纵向约束
在微服务领域,提倡数据存储隔离(DsS, Data Storage Segregation)原则,即数据是微服务的私有资产,
对于该数据的访问都必须通过当前微服务提供的AP|来访问。如若不然,则造成数据层产生耦合,违背了高内聚低
耦合的原则。同时,出于性能考虑,通常采取读写分离(CQRS)手段。
同样的,由于容器调度对底层设施稳定性的不可预知影响,微服务的设计应当尽量遵循无状态设计原则,这意味
着上层应用与底层基础设施的解耦,微服务可以自由在不同容器间被调度。对于有数据存取(即有状态)的微服务而
言,通常使用计算与存储分离方式,将数据下沉到分布式存储,通过这个方式做到一定程度的无状态化。
全局视角下的微服务分布式约束
从微服务系统设计一开始,就需要考虑以下因素
高效运维整个系统,从技术上要准备全自动化的CCD流水线满足对开发效率的诉求,并在这个基础上支持
蓝绿、金丝雀等不同发布策略,以满足对业务发布稳定性的诉求。
面对复杂系统,全链路、实时和多维度的可观测能力成为标配。为了及时、有效地防范各类运维风险,需要
从微服务体系多种事件源汇聚并分析相关数据,然后在中心化的监控系统中进行多维度展现。伴随着微服务
拆分的持续,故障发现时效性和根因精确性始终是开发运维人员的核心诉求。
20〔阿里云
3)云原生微服务典型架构
自从微服务架构理念在2011年提出以来,典型的架构模式按岀现的先后顺序大致分为四代。
Service A
ervice
B
Business
Business
ogIc
ogIc
Container A
Container B
第一代微服务架构中,应用除了需要实现业务逻辑之外,还需要自行解决上下游寻址、通讯,以及容错等问题
随着微服务规模扩大,服务寻址逻辑的处理变得越来越复杂,哪怕是同一编程语言的另一个应用,上述微服务的基础
能力都需要重新实现一遍。
Discovery service
Service A
Service B
Business
Business
logIc
logIc
Discovery and
Discovery and
fault tolerance
fault tolerance
Container A
Container B
21The cloud-nctive Architecture White Paper
在第二代微服务架构中,引入了旁路服务注册中心作为协调者来完成服务的自动注册和发现。服务之间的通讯以
及容错机制开始模块化,形成独立服务框架。但是随着服务框架内功能日益增多,用不同语言的基础功能复用显得十
分困难,这也就意味着微服务的开发者被迫被绑定在某种特定语言上,从而违背了微服务的敏捷迭代原则。
Discovery service
Sidecar
Sidecar
Discovery and
Discovery and
Service A
fault tolerance
fault tolerance
Service B
Traffic
Traffic
Business
management
management
Business
logic
Container A
Container B
2016年岀现了第三代微服务架构一服务网格,原来被模块化到服务框架里的微服务基础能力,被进一步的从
SDK演进成为一个独立进程- Sidecar。这个变化使得第二代架构中多语言支持问题得以彻底解决,微服务基础
能力演进和业务逻辑迭代彻底解耦。这个架构就是在云原生时代的微服务架构- Cloud Native Microservices,边
车( Sidecar)进程开始接管微服务应用之间的流量,承载第二代中服务框架的功能,包括服务发现、调用容错,到
丰富的服务治理功能,例如:权重路由、灰度路由、流量重放、服务伪装等。
Discovery service
Sidecar
Sidecar
Discovery and
Discovery and
Service A
fault tolerance
fault tolerance
Service B
Traffic
Traffic
Business
management
management
Business
logic
logic
Function as a service
近两年开始,随着 AWS Lambda的出现,部分应用开始尝试利用 Serverless来架构微服务,这种方式被称之
为第四代微服务架构。在这个架构中,微服务进一步由一个应用简化为微逻辑( Micrologic),从而对边车模式提出
了更高诉求,更多可复用的分布式能力从应用中剥离,被下沉到边车中,例如:状态管理、资源绑定、链路追踪、事
务管理、安全等等。同时,在开发侧开始提倡面向 localhost编程的理念,提供标准AP丨屏蔽掉底层资源、服务
基础设施的差异,进一步降低微服务开发难度。这个也就是目前业界提出的多运行时微服务架构(Mut- Runtime
Microservices)。
22阿里云
主要微服务技术
Apache Dubbo作为源自阿里巴巴的一款开源高性能RPC框架,特性包括基于透明接口的RPC、智能负载均
衡、自动服务注册和发现、可扩展性高、运行时流量路由与可视化的服务治理。经过数年发展已是国内使用最广泛的
微服务框架并构建了强大的生态体系。为了巩固 Dubbo生态的整体竞争力,2018年阿里巴巴陆续开源了 Spring
Cloud alibaba(分布式应用框架)、 Nacos(注册中心&配置中心)、 Sentinel(流控防护)、 Seat(分布式事务)、
Chaosblade(故障注入),以便让用户享受阿里巴巴十年沉淀的微服务体系,获得简单易用、高性能、高可用等核心
能力。Dubo在v3中发展 Service Mesh,目前 Dubbo协议已经被 Envoy支持,数据层选址、负载均衡和服务
治理方面的工作还在继续,控制层目前在继续丰富 Isto/pilot- discovery中
Spring Cloud作为开发者的主要微服务选择之一,为开发者提供了分布式系统需要的配置管理、服务发现、断
路器、智能路由、微代理、控制总线、一次性 Token、全局锁、决策竟选、分布式会话与集群状态管理等能力和开
发工具。
Eclipse MicroProfile作为Java微服务开发的基础编程模型,它致力于定义企业Java微服务规范
Micro Profile提供指标、AP丨文档、运行状况检查、容错与分布式跟踪等能力,使用它创建的云原生微服务可以自
由地部署在任何地方,包括 Service Mesh架构。
Tars是腾讯将其内部使用的微服务框架TAF( Total Application Framework)多年的实践成果总结而成的开
源项目,在腾讯内部有上百个产品使用,服务内部数干名C++、Java、 Golan、Node.Js与PHP开发者。Tars
包含一整套开发框架与管理平台,兼顾多语言、易用性、高性能与服务治理,理念是让开发更聚焦业务逻辑,让运维
更高效。
SOFAStack( Scalable Open Financial Architecture Stack)是由蚂蚁金服开源的一套用于快速构建金融
级分布式架构的中间件,也是在金融场景里锤炼出来的最佳实践。MOSN是 SOFAStack的组件,它一款采用Go
语言开发的 Service Mesh数据平面代理,功能和定位类似 Envoy,旨在提供分布式、模块化、可观测、智能化的
代理能力。MOSN支持 Envoy和 Isto的APl,可以和 Isto集成。
Dapr( Distributed Application Runtime,分布式应用运行时)是微软新推出的,一种可移植的、 Serverless
的、事件驱动的运行时,它使开发人员可以轻松构建弹性,无状态和有状态微服务,这些服务运行在云和边缘上,并
包含多种语言和开发框架。
23The cloud-nctive Architecture White po
Serverless
技术特点
随着以 Kubernetes为代表的云原生技术成为云计算的容器界面, Kubernetes成为云计算的新一代操作
系统。面向特定领域的后端云服务(BaaS)则是这个操作系统上的服务APl,存储、数据库、中间件、大数据、
A|等领域的大量产品与技术都开始提供全托管的云形态服务,如今越来越多用户已习惯使用云服务,而不是自
己搭建存储系统、部署数据库软件。
当这些BaaS云服务日趋完善时, Serverless因为屏蔽了服务器的各种运维复杂度,让开发人员可以将
更多精力用于业务逻辑设计与实现,而逐渐成为云原生主流技术之一。 Serverless计算包含以下特征
全托管的计算服务,客户只需要编写代码构建应用,无需关注同质化的、负担繁重的基于服务器等基础设施
的开发、运维、安全、高可用等工作
通用性,结合云 Baas AP|的能力,能够支撑云上所有重要类型的应用
自动的弹性伸缩,让用户无需为资源使用提前进行容量规划
按量计费,让企业使用成本得有效降低,无需为闲置资源付费。
函数计算( Function as a service)是 Serverless中最具代表性的产品形态。通过把应用逻辑拆分多
个函数,每个函数都通过事件驱动的方式触发执行,例如当对象存储(OSS)中产生的上传/删除对象等事件,
能够自动、可靠地触发FaaS函数处理且每个环节都是弹性和高可用的,客户能够快速实现大规模数据的实时
并行处理。同样的,通过消息中间件和函数计算的集成,客户可以快速实现大规模消息的实时处理
目前函数计算这种 Serverless形态在普及方面仍存在一定困难,例如
函数编程以事件驱动方式执行,这在应用架构、开发习惯方面,以及研发交付流程上都会有比较大的改变;
函数编程的生态仍不够成熟,应用开发者和企业内部的研发流程需要重新适配;
细颗粒度的函数运行也引发了新技术挑战,比如冷启动会导致应用响应延迟,按需建立数据库连接成本高等。
针对这些情况,在 Serverless计算中又诞生出更多其他形式的服务形态,典型的就是和容器技术进行
融合创新,通过良好的可移植性,容器化的应用能够无差别地运行在开发机、自建机房以及公有云环境中
基于容器工具链能够加快解决 Serverless的交付。云厂商如阿里云提供了弹性容器实例(EC|)以及更上
层的 Serverless应用引擎(SAE), Google提供了 Cloudrun服务,这都帮助用户专注于容器化应用
构建,而无需关心基础设施的管理成本。此外 Google也开源了基于 Kubernetes的 Serverless应用框架
Native。
相对函数计算的编程模式,这类 Serverless应用服务支持容器镜像作为载体,无需修改即可部署在
Serverless环境中,可以享受到 Serverless带来的全托管免运维、自动弹性伸缩、按量计费等优势。下面
是传统的弹性计算服务、基于容器的 Serverless应用服务和函数计算的对比
24阿里云
服务分类
弹性计算
Serverless应用
代表产
阿里云SAE
弹性计算ECS
Google Native
函数计算
虚拟化
硬件虚拟化
全容器
安全容器或应用运行时
交付模式
虚拟机镜像
容器镜像
函数
应用兼容性
扩容单位
虚拟机
容器实例
函数实例
弹性效率
分钟级
秒级
毫秒级
计费模式
实例运行时长
实例运行时长或
请求处理时长
请求处理时长
2)常见场景
近两年来 Serverless近年来呈加速发展趋势,用户使用 Serverless架构在可靠性、成本和研发运维效
率等方面获得显著收益。
小程序 Web/Mobile/AP后端服务
在小程序、Web/ Moible应用、AP服务等场景中,业务逻辑复杂多变,迭代上线速度要求高,而且这类在线应用,
资源利用率通常小于30%,尤其是小程序等长尾应用,资源利用率更是低于10%。 Serverless免运维,按需付费
的特点非常适合构建小程序 Web/Mobile/AP后端系统,通过预留计算资源+实时自动伸缩,开发者能够快速构建
延时稳定、能承载高频访问的在线应用。在阿里内部,使用 Serverless构建后端服务是落地最多的场景,包括前端
全栈领域的 Serverless For Frontends,机器学习算法服务,小程序平台实现等等
静态内容
HTMUCSS/JS
CDN
对象存储
动态内容
客户端
》函数计算
云数据库
Serverless
NOSQL
存储
25The cloud-nctive Architecture White Paper
大规模批处理任务
在构建典型仼务批处理系统时,例如大规模音视频文件转码服务,需要包含计算资源管理、任务优先级调度、任
务编排、任务可靠执行、任务数据可视化等一系列功能。如果从机器或者容器层开始构建,用户通常使用消息队列进
行任务信息的持久化和计算资源分配,使用 Kubernetes等容器编排系统实现资源的伸缩和容错,自行搭建或集成
监控报警系统。而通过 Serverless计算平台,用户只需要专注于任务处理逻辑的处理,而且 Serverless计算的极
致弹性可以很好地满足突发任务下对算力的需求。
通过将对象存储和 Serverless计算平台集成的方式,能实时响应对象创建、删除等操作,实现以对象存储为
中心的大规模数据处理。用户既可以通过增量处理对象存储上的新增数据,也可以创建大量函数实例来并行处理存
量数据。
豆示
客户端
对象存储
函数计算
甚于事件驱动架构的在线应用和离线数据处理
典型 Serverless计算服务通过事件驱动的方式,可以广泛地与云端各种类型服务集成,用户无需管理服务器
等基础设施和编写集成多个服务的“胶水”代码,就能够轻松构建松耦合、基于分布式事件驱动架构的应用。
通过和事件总线的集成,无论是一方BaaS云服务,还是三方的SaaS服务,或者是用户自建的系统,所有
事件都可以快速便捷地被函数计算处理。例如通过和AP|网关集成,外部请求可以转化为事件,从而触发后端函数
处理。通过和消息中间件的事件集成,用户能快速实现对海量消息的处理
通知
发货
客户端
函数计算
Serverless
退款
预留库存
支付
26阿里云
开发运维自动化
通过定时触发器,用户用函数的方式就能够快速实现定时任务,而无须管理执行任务的底层服务器。通过将定时
触发器和监控系统的时间触发器集成,用户可以及时接收机器重启、宕机、扩容等laaS层服务的运维事件,并自动
触发函数执行处理。
3)技术关注点
计算资源弹性调度
为了实现精准、实时的实例伸缩和放置,必须把应用负载的特征作为资源调度依据,使用“白盒”调度策略,由
Serverless平台负责管理应用所需的计算资源。平台要能够识别应用特征,在负载快速上升时,及时扩容计算资源,
保证应用性能稳定;在负载下降时,及时回收计算资源,加快资源在不同租户函数间的流转,提高数据中心利用率。
因此更实时、更主动、更智能的弹性伸缩能力是函数计算服务获得良好用户体验的关键。通过计算资源的弹性调度,
帮助用户完成指标收集、在线决策、离线分析、决策优化的闭环。
在创建新实例时,系统需要判断如何将应用实例放置在下层计算节点上。放置算法应当满足多方面的目标
容错:当有多个实例时,将其分布在不同的计算节点和可用区上,提高应用的可用性
资源利用率:在不损失性能的前提下,将计算密集型、O密集型等应用调度到相同计算节点上,尽可能充分
利用节点的计算、存储和网络资源。动态迁移不同节点上的碎片化实例,进行“碎片整理”,提高资源利用率。
性能:例如复用启动过相同应用实例或函数的节点、利用缓存数据加速应用的启动时间。
数据驱动:除了在线调度,系统还将天、周或者更大时间范围的数据用于离线分析。离线分析的目的是利用
全量数据验证在线调度算法的效果,为参数调优提供依据,通过数据驱动的方式加快资源流转速度,提高集
群整体资源利用率。
负载均衡和流控
资源调度服务是 Serverless系统的关键链路。为了支撑毎秒近百万次的资源调度请求,系统需要对资源调度服
务的负载进行分片,横向扩展到多台机器上,避免单点瓶颈。分片管理器通过监控整个集群的分片和服务器负载情况,
执行分片的迁移、分裂、合并操作,从而实现集群处理能力的横向扩展和负载均衡。
在多租户环境下,流量隔离控制是保证服务质量的关键。由于用户是按实际使用的资源付费,因此计算资源要通
过被不同用户的不同应用共享来降低系统成本。这就需要系统具备出色的隔离能力,避免应用相互干扰。
安全性
Serverless计算平台的定位是通用计算服务,要能执行任意用户代码,因此安全是不可逾越的底线。系统应从
权限管理、网络安全、数据安全、运行时安全等各个维度全面保障应用的安全性。轻量安全容器等新的虚拟化技术实
现了更小的资源隔离粒度、更快的启动速度、更小的系统开销,使数据中心的资源使用变得更加细粒度和动态化,从
而更充分地利用碎片化资源。The cloud-nctive Architecture White Paper
4开放应用模型(OAM)
1)开放应用模型的背景
随着 Kubernetes技术体系逐渐成熟,整个云原生生态正在积极思考与探索如何达成云原生技术理念
以应用为中心”的终极愿景。2019年末,阿里云联合微软共同发布了 Open Application Model(OAM)
开源项目,其主要目标是解决从 Kubernetes项目到“以应用为中心”的平台之间最关键环节一一标准化应用
定义。
2)开放应用模型的定义
容器技术以“彻底改变了软件打包与分发方式”迅速得到大量企业的广泛使用。不过软件打包与分发方式
的革新,并没有能够让软件本身的定义与描述发生本质变化;基于 Kubernetes的应用管理体验,还没有让业
务研发与运维的工作变得足够简单。最典型的例子, Kubernetes至今都没有“应用”这个概念,它提供的是
更细粒度的“工作负载”原语,比如 Deployment或者 Daemon set。在实际环境中,一个应用往往由一系
列独立组件组成,比如一个“PHP应用容器”和一个“数据库实例”组成电商网站;一个“参数服务节点”
和一个“工作节点”组成机器学习训练任务;一个由“ Deployment+ Statefulset+HPA+ Service+
ngress”组成微服务应用。
OAM的第一个设计目标就是补充“应用”这一概念,建立对应用和它所需的运维能力定义与描述的标准
规范。换而言之,OAM既是标准“应用定义”同时也是帮助封装、组织和管理 Kubernetes中各种“运维能力
在具体设计上,OAM的描述模型是基于 Kubernetes APl的资源模型( Kubernetes resource model)
来构建的,它强调一个现代应用是多个资源的集合,而非一个简单工作负载。例如在PHP电商网站的OAM
语境中,一个PHP容器和它所依赖的数据库以及它所需要使用的各种云服务,都是一个“电商网站”应用的
组成部分。同时,OAM把这个应用所需的“运维策略”也认为是应用的一部分,比如这个PHP容器所需的
PA(水平自动扩展策略)
Application
php
Redis
scaling
auto
scaling
auto
route
/index
route
/index
security group public
security group public
28阿里云
OAM项目的第二个设计目标就是提供更高层级的应用层抽象和以应关注点分离的定义模型。 Kubernetes作
为一个面向基础设施工程师的系统级项目,主要负责提供松耦合的基础设施语义,这就使得用户编写 Kubernetes
YAML文件时,往往会感觉这些文件里的关注点非常底层。实际上,对于业务研发人员和运维人员而言,他们并不
想配置这些如此底层的资源信息,而希望有更高维度的抽象。这就要求一个真正面向最终用户侧的应用定义,一个能
够为业务研发和应用运维人员提供各自所需的应用定义原语。
开放应用模型的核心概念
OAM主要定义了三个具体的概念和对应的标准,它们包括应用组件依赖、应用运维特征和应用配置
组件依赖:OAM定义和规范了组成应用的组件( Component)。例如,一个前端 Web server容器、数
据库服务、后端服务容器等;
应用运维特征:OAM定义和规范了应用所需的运维特征(Trat)的集合。例如,弹性伸缩和 ngress等运
维能力
应用配置:OAM定义和规范了应用实例化所需的配置机制,从而能够将上述这些描述转化为具体应用实例。
具体来说,运维人员可以定义和使用应用配置( Application Configuration)来组合上述的组件和相应的特征,
以构建可部署的应用交付实例。
例如,一个由PHP容器和Reds实例组成的应用,在OAM的框架和规范下,就可以用如下的示意图来表达
出来
Application
h
scaling: auto
route: /index
security group: public
scaling: manual
Redis
rollout strategy: canary
trait
security group: private
scope
What to run?
How to operate?
而在上述模块化的应用定义基础上,OAM模型还强调整个模型的关注点分离特性。即业务研发人员负责定义与
维护组件( Component)来描述服务单元,而运维人员定义运维特征(Trat)并将其附加到组件上,构成OAM可交
付物- Application Configuration。这种设计使OAM在能够无限接入 Kubernetes各种能力同时,为业务研发
与运维人员提供最佳使用体验和最低心智负担。
29The cloud-nctive Architecture White Paper
在 Kubernetes上使用开放应用模型
在上述应用定义模型规范基础上,OAM提供了标准的 OAM Kubernetes插件(即: OAM Kubernetes核心
依赖库项目),从而能够“一键式”的为任何 Kubernetes集群加上基于OAM模型的应用定义与应用管理能力。
具体来说,这个插件的功能包括
功能一:无缝对接现有K8sAP资源
OAM Kubernetes核心依赖库支持将任何现有CRD被声明为 Workload或者 Trait,而无需做任何改
动。这也意味着任何 Kubernetes原生AP资源也可以被声明为 Workload或者 Trait。通过这种设计,现有
Kubernetes集群里所有能力进行OAM化变得非常容易。
Component
apiversion: core oam. dev/vlalpha2
kind
metadata
name
组件
annotations
kload
完全从研发视角定义的应用模块
工作负载:应用模块的具体描述
workload
apiversion: openfaas. com/v1
kind: Function
name: nodeinfo
handler: node main js
image: function/ nodeinfo
Component A
httpproxyhttp://proxy1.corp.com3128
noproxyhttp://gateway/
A群意
功能二: Workload与Trat标准化交互机制
OAM Kubernetes插件保证了OAM可以模块式接入、部署和管理任何 Kubernetes工作负载和运维能力。
而这些工作负载和运维能力之间的交互需要标准化、统一化; Workload与 Trait标准化交互机制应运而生。比如
Deployment和HPA(自动水平扩展控制器)的协作关系中, Deployment在OAM模型中就属于 Workload,
而HPA则属于Trat。在OAM当中, Application Configuration里引用的 Workload和Trat也必须通过协作
方式来操作具体 Kubernetes资源。
在 OAM Kubernetes核心依赖库中,过 DuckTyping(鸭子类型)机制,在Trai对象上自动记录与之绑定
的 Workload关系,从而实现了工作负载( Workload)和运维能力( Trait)之间的双向记录关系
给定任何一工作负载( Workload),系统可以直接获取到同它绑定的所有运维能力( Trait)
30阿里云
给定任何一个运维能力( Trait),系统可以直接获取到它所要作用于的所有工作负载( Workload)。这
种双向记录关系对于在一个大规模的生产环境中保证运维能力的可管理性、可发现性和应用稳定性是至关
重要的。
除此之外, OAM Kubernetes插件还提供几个非常重要的基础功能,包括
Component版本管理:对于任何一次 Component变更,OAM平台都将记录下来其变更历史,从而
允许运维通过 Trait来进行回滚、蓝绿发布等运维操作。
Component间依赖关系与参数传递:该功能将解决部署亟需的组件间依赖问题,包括 Component之
间的依赖和传输传递,以及Trat与 Component之间的依赖和参数传递。
Component运维策略:该功能将允许研发在 Component中声明对运维能力的诉求,指导运维人员或
者系统给这个 Component绑定和配置合理的运维能力。
开放应用模型的未来
相比于传统PaaS封闭、不能同“以 Operator为基础的云原生生态”衔接的现状,基于OAM和
Kubernetes构建的现代云原生应用管理平台的本质是一个“以应用为中心”的 Kubernetes,保证应用平台能够
无缝接入整个云原生生态。同时,OAM进一步屏蔽掉容器基础设施的复杂性和差异性,为平台使用者带来低心智负
担的、标准化的、一致化的应用管理与交付体验,让一个应用描述可以完全不加修改的在云、边、端等任何环境下直
接交付运行起来。
Ul/Dashboard
Cloud
Traffic
Cannary
Container
Functi
Blue-Green
Resources
Management
Scaling
AB testing
Components
Traits/Scope
Application Configuration
Kubernetes(+OAM核心依赖库)
Kubermetes Plugins
个基于OAM构建的 Kubernetes应用管理平台
此外,OAM还定义了一组核心工作负载/运维特征/应用范畴,作为应用程序交付平台的基石。当模块化的
Workload和 Trait越来越多,就会形成组件市场。而OAM就像是这个组件市场的管理者,通过处理组件之间的关
系,把许多组件集成为一个产品交付给用户。OAM加持下的 Kubernetes应用拼图,可以像乐高积木一样灵活组
装底层能力、运维特征以及开发组件。The cloud-nctive Architecture White Paper
OAM社区还同混合云管理项目 Crossplane建立了深度合作,从而保证符合OAM规范的待运行程序、运维
能力和它所依赖的云服务,可以组成一个整体,在混合云环境中无缝漂移。这种无关平台的应用定义范式,使得应用
研发人员只需通过OAM规范来描述应用程序,应用程序就可以在任何 Kubernetes群集或者 Serverless应用平
台甚至边绿环境上运行而无需对应用描述做任何修改。OAM社区与整个云原生生态一起,正在将标准应用定义和标
准化的云服务管理能力统一起来,使“云端应用交付”的故事真正成为现实。
Service Mesh技术
1)技术特点
Service mesh是分布式应用在微服务软件架构之上发展起来的新技术,旨在将那些微服务间的连接、安全、流
量控制和可观测等通用功能下沉为平台基础设施,实现应用与平台基础设施的解耦。这个解耦意味着开发者无需关注
微服务相关治理问题而聚焦于业务逻辑本身,提升应用开发效率并加速业务探索和创新。换句话说,因为大量非功能
性从业务进程剥离到另外进程中, Service mesh以无侵入的方式实现了应用轻量化,下图展示了 Service Mesh的
典型架构
Service A
Service B
ngress
trafic
Discovery Configuration Certficates
Control plane
istiod
Pilot
Istio Architecture
在这张架构图中, Service A调用 Service b的所有请求,都被其下的 Proxy(在Enoy中是 Sidecar)截获,
代理 Service a完成到 Service b的服务发现、熔断、限流等策略,而这些策略的总控是在 Control Plane上配置。
32〔阿里云
从架构上,lsto可以运行在虚拟机或容器中, Isto的主要组件包括pio(服务发现、流量管理)Mⅸer(访问控制、
可观测性)、 Citadel(终端用户认证、流量加密);整个服务网格关注连接和流量控制、可观测性、安全和可运维性。
虽然相比较没有服务网格的场景多了4个|PC通讯的成本,但整体调用的延迟随着软硬件能力的提升而并不会带来
显著的影响,特别是对于百亳秒级别的业务调用而言可以控制在2%以内。从另一方面,服务化的应用并没有做任
何改造,就获得了强大的流量控制能力、服务治理能力、可观测能力、4个9以上高可用、容灾和安全等能力,加上
业务的横向扩展能力,整体收益仍然是远大于额外|PC通讯支出。
服务网格的技术发展上数据平面与控制平面间的协议标准化是必然趋势。大体上, Service Mesh的技术发展围
绕着“事实标准”去展开共建各云厂商共同采纳的开源软件。从接口规范的角度:sto采纳了 Envoy所实现的
xDS协议,将该协议当作是数据平面和控制平面间的标准协议; Microsoft提出了 Service Mesh Interface(SM),
致力于让数据平面和控制平面的标准化做更高层次的抽象,以期为sto、 Linked等 Service Mesh解决方案在服
务观测、流量控制等方面实现最大程度的开源能力复用。∪DPA(Unⅳ ersal Data Plane AP)是基于xDS协议
而发展起来,以便根据不同云厂商的特定需求便捷地进行扩展并由DS去承载
此外数据平面插件的扩展性和安全性也得到了社区的广泛重视。从数据平面角度, Envoy得到了包括 Google
BM、Csco、 Micros、阿里云等大厂的参与共建以及主流云厂商的采纳而成为了事实标准。在 Envoy的软件设
计为插件机制提供了良好扩展性的基础之上,目前正在探索将Wasm技术运用于对各种插件迸行隔离,避免因为某
插件的软件缺陷而导致整个数据平面不可用。Wasm技术的优势除了提供沙箱功能外,还能很好地支持多语言
最大程度地让掌握不同编程语言的开发者可以使用自己所熟悉的技能去扩展 Envoy的能力。在安全方面, Service
Mesh和零信任架构天然有很好的结合,包括 POD Identity、基于mTLs的链路层加密、在RPC上实施RBAC
的ACL、基于 Identity的微隔离环境(动态选取一组节点组成安全域)
2)行业应用情况
根据 Gartner研究报告,lsio有望成为 Service mesh的事实标准,而 Service mesh本身也将成为容器服
务技术的标配技术组件。即便如此, Service Mesh目前在市场仍处于早期采用( Early adoption)阶段。
除 Istic以外, Google与AWS分别推出了各自的云服务 Traffic Director、 App Mesh。这两个 Service
Mesh产品与lso虽有所不同,但与lsto同样地采纳了 Envoy作为数据平面。此外,阿里云、腾讯云、华为云也
都推出了 Service mesh产品,同样采用Enoy技术作为数据面并在此基础上提供了应用发布、流量管控、APM
等能力
3)主要技术
2017年发起的服务网格lsto开源项目,清晰定义了数据平面(由开源软件Eηoy承载)和管理平面(lsto自
身的核心能力)。 stio为微服务架构提供了流量管理机制,同时亦为其它增值功能(包括安全性、监控、路由、连
接管理与策略等)创造了基础。 Isto利用久经考验的 Lyft Envoy代理进行构建,可在无需对应用程序代码作出任何
33The cloud-nctive Architecture White Paper
发动的前提下实现可视性与控制能力。2019年lsto所发布的1.12版已达到小规模集群上线生产环境水平,但其性
能仍受业界诟病。开源社区正试图通过架构层面演进改善这一问题。由于 Isto是建构于 Kubernetes技术之上,所
以它天然地可运行于提供 Kubernetes容器服务的云厂商环境中,同时lsto成为了大部分云厂商默认使用的服务网
格方案。
除了sto外,也有 Linked、 Consu这样相对小众的 Service mesh解决方案。 Linker在数据平面采用
了Rust编程语言实现了 inked- proxy,控制平面与lsto一样采用Go语言编写。最新的性能测试数据显示,
Linker在时延、资源消耗方面比 stio更具优势。 Consu在控制面上直接使用C。 nsul server,在数据面上可以
选择性地使用Enoy。与lsto不同的是, Linked和 Consu在功能上不如sto完整。
Conduit作为 Kubernetes的超轻量级 Service mesh,其目标是成为最快、最轻、最简单且最安全的
Service mesh。它使用Rust构建了快速、安全的数据平面,用Go开发了简单强大的控制平面,总体设计围绕着
性能、安全性和可用性进行。它能透明地管理服务之间的通信,提供可测性、可靠性、安全性和弹性的支持。虽然与
Linked相仿,数据平面是在应用代码之外运行的轻量级代理,控制平面是一个高可用的控制器,然而与 Linked
不同的是, Conduit的设计更加倾向于 Kubernetes中的低资源部署。
6 DevOps
概述
Devops就是为了提高软件研发效率,快速应对变化,持续交付价值的的一系列理念和实践,其基本思想就是
持续部署(CD),让软件的构建、测试、发布能够更加快捷可靠,以尽量缩短系统变更从提交到最后安全部署到生产
系统的时间。
要实现持续部署(CD),就必须对业务进行端到端分析,把所有相关部门的操作统一考虑进行优化,利用所可用
的技术和方法,用一种理念来整合资源。 DevOps理念从提出到现在,已经深刻影响了软件开发过程。 DevOps提
倡打破开发、测试和运维之间的壁垒,利用技术手段实现各个软件开发环节的自动化甚至智能化,被证实对提高软件
生产质量、安全,缩短软件发布周期等都有非常明显的促进作用,也推动了「技术的发展。
2 DevOps原则
要实施 DevOps,需要遵循一些基本原则,这些原则被简写为CAMS,是如下四个英文单词的缩写
文化( Culture)
自动化( Automation)
度量( Measurement)
共享( Sharing
34阿里云
下面我们分别来谈一谈这几个方面。
文化
谈到 Devops,一般大家关注的都是技术和工具,但实际上要解决的核心问题是和业务、和人相关的问题。提
高效率,加强协作,就需要不同的团队之间更好的沟通。如果每个人能够更好的相互理解对方的目标和关切的对象,
那么协作的质量就可以明显的提高。
DevOps实施中面对的首要矛盾在于不同团队的关注点完全不一样。运维人员希望系统运行可靠,所以系统稳
定性和安全性是第一位。而开发人员则想着如何尽快让新功能上线,实现创新和突破,为客户提供更大价值。不同的
业务视角,必然导致误会和摩擦,导致双方都觉得对方在阻挠自己完成工作。要实施 DevOps,就首先要让开发和
运维人员认识到他们的目标是一致的,只是工作岗位不同,需要共担责任。这就是 DevOps需要首先在文化层面解
决的问题。只有解决了认知问题,才能打破不同团队之间的鸿沟,实现流程自动化,把大家的工作融合成一体。
自动化
Devops的持续集成的目标就是小步快跑,快速迭代,频繁发布。小系统跑起来很容易,但一个大型系统往往
牵涉几十人到几百人的合作,要让这个协作过程流畅运行不是一件容易的事情。要把这个理念落实,就需要规范化和
流程化,让可以自动化的环节实现自动化。
实施 Devops,首先就要分析已有的软件开发流程,尽量利用各种工具和平台,实现开发和发布过程的自动化。
经过多年发展,业界已经有一套比较成熟的工具链可以参考和使用,不过具体落地还需因地制宜。
在自动化过程中,需要各种技术改造才能达到预期效果。例如如果容器镜像是为特定环境构建的,那么就无法实
现镜像的复用,不同环境的部署需要重新枃建,浪费时间。这时就需要把环境特定的配置从镜像中剥离岀来,用一
配置管理系统来管理配置。
度量
通过数据可以对每个活动和流程进行度量和分析,找到工作中存在的瓶颈和漏洞以及对于危急情况的及时报警等。
通过分析,可以对团队工作和系统进行调整,让效率改进形成闭环。
度量首先要解决数据准确性、完整性和及时性问题,其次要建立正确的分析指标。 DevOps过程考核的标准应
该鼓励团队更加注重工具的建设,自动化的加速和各个环节优化,这样才能最大可能发挥度量的作用。
要实现真正的协作,还需要团队在知识层面达成一致。通过共享知识,让团队共同进步
可见度 visibility:让每个人可以了解团队其它人的工作。这样可以知道是否某一项工作会影响另一部分。
通过相互反馈,让问题尽早暴器。
35The cloud-nctive Architecture White Paper
透明性 transparency:让每个人都明白工作的共同目标,知道为什么要干什么。缺乏透明性就会导致工
作安排失调。
知识的传递 transfer of knowledge:知识的传递是为了解决两个问题,一个是为了避免某个人成为单
点,从而导致一个人的休假或离职,就导致工作不能完成。另一个是提高团队的集体能力,团队的集体能
力要高于个人的能力。
要实现知识共享有很多方法。在敏捷开发中,通过日站会来共享进度。在开发中,通过代码、文档和注释来共享
知识。 Chatops则是让在一个群里的人都看到正在进行的操作和操作的结果。其它的所有会议、讨论和非正式的交
流等当然也是为了知识共享。从广意上讲,团队协作就是知识不断积累和分享的过程。落实 Devops要努力建设
个良好的文化氛围并通过工具支持让所有的共享更加方便高效。
文化、自动化、度量和共享四个方面相辅相成,独立而又相互联系,所以要落实 DevOps时,要统一考虑。通
过CAMS也认识到,CCD仅仅是实现 DevOps中很小的一部分。 DevOps不仅仅是一组工具,更重要是代表了
种文化,一种心智。
3)理性的期待
对于 Devops要有合乎理性的期待。 Devops提供了一套工具,工具能够起多大的作用,其最重要的影响有两
,一是使用工具的人,另外就是对于需要解决的问题本身复杂性的掌握。虽然 Devops已经被广泛接受和认可,
但其在实际应用中的成熟度还待进一步提高。在一份关于 Devops的调查报告中,把 Devops进化分为三个级别,
80%被调查团队处于中等评级,而处于高级阶段和低级阶段的都在10%左右。报告分析,这是因为通过实现自动
化以达到中等评级是相对比较容易,而达到高等评级则需要在文化和共享方面的努力,这相对来说更加难以掌握和实
施。这也符合组织文化变革中的J型曲线。在经过了初期的效率增长之后,自动化的深化进入了瓶颈期。更进一步
的效率提升需要对组织、架构进行更加深入的调整。这段期间可能还会有效率下降、故障率攀升的问题,从这一方面
讲,实际 Devops落地和深化还有很长路要走。
持续改进带来卓越
与高效!高效能和
技术债和增长的复
精英效能组织利用
杂性导致变更流程专业知识,并从他
自动化导致需要手中会额外增加人工
们身处的环境中学
习,以使生产力发
自动化帮助低效能
工处理的测试需求控制环节和层级,
团队开始转型,并组织提升至中等效增加。大量的技术
减缓工作速度
生飞跃
识别出速赢方案
能组织
债阻碍了工作进展
36阿里云
前面比较抽象的讲了一下 DevOps的理念和原则,下面我们从更加技术的层面来分析 DevOps。
4|aC和 Gitops
lac和声明式运维
前面说过, DevOps所面对的矛盾就是开发和运维团队之间的矛盾。因为两个团队的关注点完全不同,或者说
是冲突的。在这种背景下,laC提出系统建设的核心理念,兼顾高效和安全,让运维系统的建设更加有序。
运维平台一般都经历过如下几个发展阶段:手工、脚本、工具、平台、智能化运维等。现有运维平台虽然很多实
现方式;但总体来说分为两类
指令式
声明式
两者的特点如下
指令式
声明式
用户目标
用户操作
系统边界
系统边界
执行引擎
基础操作层
基础操作层
采用声明式接口的方式相比较与指令式多一个执行引擎,把用户的目标转化为可以执行的计划
最开始的运维系统一般都是指令式的,通过编写脚本来完成于运维动作,包括部署、升级、改配置、缩扩容等。
脚本的优点就是简单、高效、直接等,相对于更早之前的手工运维,这是一种极大的效率提高。在分布式系统和云计
算起步阶段,采用这种模式进行运维是完全合理的。基于这个方法,各个部门都会相应建立一些系统和工具来加速工
具的开发和使用。
不过随着系统复杂性逐步提高,指令式的运维方式的弊端也逐渐显现出来。简单高效的优点同时也变成了最大缺
点。因为方式简单,所以无法实现复杂的控制逻辑;因为高效,如果有bug,那么在进行破坏时也同样高效。往往一
小失误就会导致大面积服务瘫痪:一个变更脚本中的bug,可能会导致严重事故。
在复杂的运维场景下,指令式的运维方式具有变更操作副作用:不透明、指令性接口一般不具有幂等性、难以实
现复杂的变更控制、知识难以积累和分享、变更缺乏并发性等缺点。针对这种情况,人们提岀了声明式的编程理念。
这是一个很简单的概念,用户仅仅通过一种方式描述其要到达的目的,而并不具体说明如何达到目标。声明式接口实
际上代表了一种思维模式:把系统的核心功能进行抽象和封装,让用户在一个更高的层次上进行操作。
37The cloud-nctive Architecture White Paper
声明式接口是一种和云计算时代相契合的思维范式。前面列出的指令式的缺点都可以由声明式接口来弥补。
幂等性:运维终态被反复提交也不会具有任何副作用。
声明式最明显的优点是变更审核简单明了。配置中心会保存历史上的所有版本的配置文件。通过对比新的
配置和上一个版本,可以非常明确看到配置的具体变更。一般来说,每次变更的范围不是很大,所以审核
比较方便。通过审核可以拦截很多人为失误。通过把所有的变更形式都统一为对配置文件的变更,无论是
机器的变更、网络的变更还是软件版本和应用配置的变更等。在人工审核之外,还可以通过程序来检测用
户配置是否合乎要求,从而捕捉用户忽略掉的一些系统性的限制,防患于未然。
复杂性抽象:系统复杂性越来越高,系统间相互依赖和交互越来越广泛,以及由于操作者无法掌握所有可
能的假设条件、依赖关系等而带来的运维复杂性。解决这个问题的唯一思路,就是要把更多逻辑和知识沉
淀到运维平台中,从而有效降低用户使用难度和操作风险。
Stops
Gitops作为|aC运维理念的一种具体落地方式,就是使用Gt来存储关于应用系统的最终状态的声明式描述
GitOps的核心是一个 GitOps引擎,它负责监控Git中的状态,每当它发现状态有改变,它就负责把目标应用系统
中的状态以安全可靠的方式迁移到目标状态,实现部署、升级、配置修改、回滚等操作。
Gⅱt中存储有对于应用系统的完整描述以及所有修改历史。方便重建的同时,也便于对系统的更新历史进行查看,
符合 DevOps所提倡的透明化原则。同时, Gitops也具有声明式运维的所有优点。
和 GitOps配套的一个基本假设是不可变基础设施,所以 Gitops和 Kubernetes运维可以非常好的配合。
Config change
◆→→豳
Git
Synchronize
Kubernetes
Gitops引擎需要比较当前态和Gt中的终态间的差别,然后以一种可靠的方式把系统从任何当前状态转移到终」
态,所以 Gitops系统的设计还是比较复杂的。对于用户的易用,实则是因为围绕 Stops有一套完整的工具和平台
的支持。
5)云原生时代的 Devops
相对于传统基础设施,云具有更加灵活的调度策略,接近无限的资源、丰富的服务供用户选择、使用,这些
都极大方便了软件的建设。而云原生开源生态的建设,基本统一了软件部署和运维的基本模式。更重要的是,云原生
技术的快速演进,技术复杂性不断下沉到云,赋能开发者个体能力,不断提升了应用开发效率。
38〔阿里云
首先是容器技术和 Kubernetes服务编排技术的结合,解决了应用部署自动化、标准化、配置化问题。CNCF
打破了云上平台的壁垒,使建设跨平台的应用成为可能,成为事实上的云上应用开发平台的标准,极大简化了多云部署。
完整开发流程涉及到很多步骤,而环节越多,一次循环花费的时间越长,效率就越低。微服务通过把巨石
应用拆解为若干单功能的服务,减少了服务间的耦合性,让开发和部署更加便捷,可以有效降低开发周期,提高
部署灵活性。 Service mesh让中间件的升级和应用系统的升级完全解耦,在运维和管控方面的灵活性获得提升。
Serverless让运维对开发透明,对于应用所需资源进行自动伸缩。FaaS是 Serverless的一种实现,则更加简化
了开发运维的过程,从开发到最后测试上线都可以在一个集成开发环境中完成。无论哪一种场景,后台的运维平台的
工作都是不可以缺少的,只是通过技术让扩容、容错等技术对开发人员透明,让效率更高。
7云原生中间件
在云原生时代,传统中间件技术也演化升级为云原生中间件,云原生中间件主要包括网格化的服务架构、事件驱
动技术、 Serverless等技术的广泛应用,其中网格化( Service Mesh)、微服务和 Serverless前面都讲过了,这
里主要讲云原生技术带来的不同点。
云原生中间件最大的技术特点就是中间件技术从业务进程中分离,变成与开发语言无关的普惠技术,只与应用自
身架构和采用的技术标准有关,比如一个PHP开发的REST应用也会自动具备流量灰度发布能力、可观测能力,
即使这个应用并没有采用任何服务化编程框架。
微服务架构一般包含下列组件:服务注册发现中心、配置中心、服务治理、服务网格、AP管理、运行时监控
链路跟踪等。随着κ ubernetes的流行, Kubernetes提供的基础部署运维和弹性伸缩能力已经可以满足多数中小
企业的微服务运维要求。微服务与 Kubernetes集成会是一个大趋势。
服务注册发现和配置中心的功能主要致力于解决微服务在分布式场景下的服务发现和分布式配置管理两个核心
问题。随着云原生技术的发展,服务发现领域出现了两个趋势,一个是服务发现标准化(sto),一个是服务下沉
CoreDNS);配置管理领域也有两个趋势,一个是标准化( Config Map
个是安全( Secret)
提到事件驱动就必须先讲消息服务,消息服务是云计算PaaS领域的基础设施之一,主要用于解决分布式应
用的异步通信、解耦、削峰填谷等场景。消息服务提供一种BaaS化的消息使用模式,用户无需预先购买服务器
和自行搭建消息队列,也无需预先评估消息使用容量,只需要在云平台开通即用,按消息使用量收费。例如阿里云
MaaS( Messaging as a Service)提供的是一站式消息平台,提供业界最主流的开源消息队列托管服务,包括
Kaka、 RabbitMQ、 RocketS等;也支持多种主流标准协议规范,包括AMQP、MQTT、JMS、 STOMP等,
以满足微服务、loT、EDA、 Streaming等各种细分领域的需求。
事件驱动架构:由于|oT、云计算技术的快速发展,事件驱动架构在未来将会被越来越多的企业采纳,通过事
39The cloud-nctive Architecture White Paper
件的抽象、异步化,来提供业务解耦、加快业务迭代。在过去事件驱动架构往往是通过消息中间件来实现,事件用
息来传递。进入云计算时代,云厂商提供更加贴近业务的封装,比如 Azure提供了事件网格,把所有云资源的
些运维操作内置事件,用户可以自行编写事件处理程序实现运维自动化;AWS则把所有云资源的操作事件都用
SNS的 Topic来承载,通过消息做事件分发,用户类似实现 WebHook来处理事件。由于事件是异步触发的,天
然适合 Serverless,所以现在很多云厂商都采用自身的 Serverless服务来运行事件负载,包括阿里云 Function
Compute、 Azure Function、 AWS Lambda都和事件处理集成。今年阿里云也发布了最新产品 EventBridge
作为事件驱动和无服务器架构的核心承载产品。
0阿里巴巴云原生架构设计
AcNA( Alibaba Cloud Native Architecting)架构设计方法B
阿里巴巴为大量各行各业的企业客户提供基于阿里云服务的解决方案和最佳实践,帮助企业基于阿里
云特别是云原生技术完成数字化转型,并积累了大量的方案和经验教训。阿里巴巴将企业的核心关注点、
企业组织与|T文化、工程实施能力等方面与架构技术等进行结合,形成了阿里巴巴独有的云原生架构设
计方法ACNA( Alibaba Cloud Native Architecting)。
ACNA是
「4+1」的架构设计流程,「4」代表架构设计的关键视角,包括企业战略视角、业
务发展视角、组织能力视角和云原生技术架构视角;「1」表示云原生架构的架构持续演进闭环。4个架
构视角和一个闭环的关系如下图所示
架构持续演进闭环
企业战略视角
业务发展视角
服务化弹性无服务器可观
韧性自动化
能力
能力化程度测性
能力
水平
组织能力视角
架构持续演进闭环
ACNA架构设计方法The cloud-nctive Architecture White Paper
ACNA除了是一个架构设计方法,也包含了对云原生架构的评估体系、成熟度衡量体系、行业应用
最佳实践、技术和产品体系、架构原则、实施指导等。
2企业战略视角
任何架构都必须服务于企业战略,云原生架构也不例外。云原生架构和以往的架构升级不同,云原玍
架构不仅是一个技术升级,更是一个对企业核心业务生产流程(即通过软件开发和运营构建数字化业务)
的重构,如果打一个比方的话,就像工业时代用更自动化的流水线替换手工作坊一样深刻。
企业必须规划清楚战略中业务战略与「T战略之间的关系,即「战略只是服务好业务战略进行必要
的技术支撑呢,还是「T战略本身也是业务战略的一部分。通常高科技公司本身会对云计算有更高的需
求,比如通过大量使用云厂商提供的A技术为用户提供智能化的用户体验,也会使用|oT和音视频技
术为用户建立更广泛和生动的连接。实际上我们发现,在数字化转型的今天,越来越多的企业认为T战
略应该在企业战略中扮演技术赋能业务创新的重要角色,CTO( Chief Technology Officer)、C|O
Chief Information Officer) CDO( Chief Data Officer) CISO( Chief Information Security
Officer)等岗位的设计也从一个层面表明了这些技术在企业战略中的位置。
3业务发展视角
B
阿里巴巴总结为企业提供云服务和咨询的过程,发现数字化业务对技术架构的主要诉求是业务连续性、
业务快速上线、成本以及科技赋能业务创新。业务连续性诉求主要包括了数字化业务必须能够持续为用
户提供服务,不能因为软硬件故障或者bug导致业务不可用,也能够抵御黑客攻击、数据中心不可用
自然灾害等意外事故。此外,当业务规模快速增长时,不能因为软硬件资源来不及购买或者部署而导致
不能拓展新用户
市场瞬息万变,数字化业务由于比传统实体业务更灵活可变而被要求更快的推向市场能力,包括新业
务快速构建的能力、现有业务快速更新的能力。这些能力诉求被云原生架构深刻理解并在产品、工具、
流程等层面得到不同程度的处理,需要注意的是这个诉求同时对组织结构带来了新的要求,也可能要求
应用进行彻底的重构(比如微服务化)。
云计算作为新的技术必须为企业释放成本红利,帮助企业从原来的 CAPEX模式转变为OPEX模式,
不用事先购买大批软硬件资源,而是用多少付多少;同时大量采用云原生架构也会降低企业开发和运维
成本,有数据展示通过采用容器平台技术就降低了30%以上的运维支出。阿里云
传统模式下如果要使用高科技技术赋能业务,有一个冗长的选型、POC、试点和推广的过程,而大
量使用云厂商和三方的云服务,由于这些云服务具备更快的连接和试错成本,且在不同技术的集成上具
备统一平台和统一技术依赖的优势,从而可以让业务更快速的应用新技术进行创新
4组织能力视角
云原生架构涉及到的架构升级对企业中的开发、测试、运维等人员都带来了巨大的影晌,技术架构的
升级和实现需要企业中相关的组织匹配,特别是架构持续演进需要有类似“架构治理委员会”这样的组织,
不断评估、检查架构设计与执行之间的偏差
此外前面提到云原生服务中重要的架构原则就是服务化(包括微服务、小服务等),这个领域典型的
个原则就是“康威定律”,要求企业中让技术架构与企业沟通架构保持一致,否则会出现畸形的服务
化架构实现。
5云原生技术架构视角
1)服务化能力
用微服务或者小服务构建业务,分离大块业务中具备不同业务迭代周期的模块,并让业务以标准化AP|等方式
进行集成和编排;服务间采用事件驱动的方式集成;减小相互依赖;通过可度量建设不断提升服务的SLA能力
2)弹性能力
自动根据业务峰值、资源负载扩充或者收缩系统的规模
3)无服务器化程度
在业务中尽量使用云服务而不是自己持有三方服务,特别是自己运维开源软件的情况;并让应用的设计尽量变成
无状态的模式,把有状态部分保存到云服务中;尽量采用FaS、容器/应用无服务器的云服务
4)可观测性
「T设施需要被持续治理,任何「T设施中的软硬件发生错误后能够被快速修复,从而不会让这样的错误对业务带
来影响,这就需要系统有全面的可观测性,从传统的日志方式、监控、APM到链路跟踪、服务QoS度量
43The cloud-nctive Architecture White Paper
5)韧性能力
除了包括服务化中常用的熔断、限流、降级、自动重试、反压等特性外,还包括高可用、容灾、异步化的特性
6自动化水平
关注整个开发、测试和运维三个过程的敏捷,推荐使用容器技术自动化软件构建过程、使用OAM标准化软件交
付过程、使用laC( Infrastructure as Code)/ GitOps等自动化CCD流水线和运维过程
7)安全能力
关注业务的数字化安全,在利用云服务加固业务运行环境的同时,采用安全软件生命周期开发,使应用符合
1sO27001、 PCIDSS、等级保护等安全要求。这个维度作为基础维度,要求在架构评测中关注但是并不参与评分。
6架构持续演进闭环
B
云原生架构演进是一个不断迭代的过程,每一次迭代都是从企业战略、业务诉求到架构设计与实施的一个完整闭
环,整体关系如下图
企业战略视角
业务发展视角
01
02.
识别业务痛点
和架构债务
确定架构迭代
目标
08.
迭代验收和
03
复盘
评估架构风险
架构持续
演进闭环
07.
04
架构风险控制
选取云原生
技术
06
05
架构评审和
设计评审
制定迭代计划
架构持续演进闭环阿里云
其中,
关键输入:企业战略视角、业务发展视角;
关键过程:识别业务痛点和架构债务、确定架构迭代目标、评估架构风险、选取云原生技术、制定迭代计划、架
构评审和设计评审、架构风险控制、迭代验收和复盘。
7云原生架构成熟度模型
B
由于云原生架构包含了6个关键架构维度(简写为 SESORA, Service+ Elasticity+ Serverless+
Observability+ Resilience+ Automation),因此我们先定义关键维度的成熟度级别
指标维度
ACNA-1
ACNA-2
ACNA-3
ACNA-4
(0分)
(1分)
(2分)
(3分)
服务化能力
无
部分服务化&缺乏治理全部服务化&有治理体系Mesh化的服务体系
(Service)
(单体应用)
自持技术,初步服务化)
(自持技术,初步服务化)
(云技术,治理最佳实践)
(日 asticity)
(固定容量)
(监控+人工扩缩容)
监控+代码伸缩,百节点规模)(基于流量等多策略,万节点规模)
无服务器化程度未采用Baas无状态计算委托给云
有状态存储委托给云
全无服务器方式运行
(Serverless
(计算、网络、大数据等)
数据库、文件、对象存储等)
( Serverless/FaaS运行全部代码)
(Observability)
(日志分析、应用级监控、APM)(链路级 Tracing、 Metrics度量)(用观测大数据提升业务体验)
韧性能力
十分钟级切流
分钟级切流
秒级切流、业务无感
(主备HA、集群HA、冷备容灾)(熔断、限流、降级、多活容灾等)( Serverless、 Service Mesh等)
(Automation)
(基于容器做C/CD)
(提升软件交付自动化)
(自动化软件交付和运维)
云原生架构成熟度模型:关键指标维度
由于云原生架构包含了6个关键架构维度(简写为 SESORA, Service+ Elasticity+ Serverless+
Observability+ Resilience+ Automation),因此我们先定义关键维度的成熟度级别
云原生架构
成熟度
零级
基础级
发展级
成熟级
完全传统架构
大于等于16分
級别和定义
未使用云计算或者
小于等于10分
大于10且小于16分
云的技术能力)
且无ACNA-1级
且无ACNA2级
第一步
第二步
根据 SESORA对6个未读分别评分并汇总
根据分值分段获得评级结果
云原生架构成熟度模型
5阿里巴巴云原生产品介绍
云原生产品家族
阿里巴巴云原生产品家族包括容器产品家族、微服务产品家族、 Serverless产品家族、 Service
Mesh产品家族、消息产品、云原生数据库家族、云原生大数据产品家族等。
业务流量
阿里巴巴云原生产品家族
源代码
微服务网关(CsB)
Event Center( Event Bridge
统一控制台
微服务平台(EDAS)微服务引擎(MSE)分布式配置(ACM)分布式事务(GTs)
T服务治理
容器镜像
Serverless
Service Mesh基础设施
SLA管理
引擎
安全、限流、降级、熔断、灰度、流量控制、协议转换
AlOPS
(FC、SAE)
SLA度量、故障注入、模拟流量
(EDAS、ACM、ARMS、AHAS)
应用元数据
二进制包
容器平台
ACK/ASK/ACR
MQ(MQ系列)数据库( PolarDB)大数据( AnalyticDB)
其他BaaS
公有云
私有
混合云
laas
laaS
laaS
阿里巴巴云原生产品家族
2容器产品家族
阿里云容器服务自从2016年5月正式推出,历经4年时间,服务了全球上万家企业客户。阿里云容
器服务产品家族可以在公共云、边缘计算和专有云环境提供企业容器平台。阿里云容器产品以容器服务
Kubernetes版(ACK)和 Serverless Kubernetes(AsK)为核心,它们构建在阿里云基础设施
46阿里云
微服务
有状态应用
大数据、智能应用
创新应用
Apache Dubbo Spring Cloud
MysQ
RocketMQ
Apache Flink
Tensorflow
区块链
安全治理
可观测性
单性成本优化
多云混合云
异构算力调度
自动化、智能化
(日志、监控、追踪、告譬)
运维体系
公共云
边缘计算
专有云
ACK(Edge
企业版、敏捷PaaS版
阿里云容器产品大图
之上,计算、存储、网络、安全等,并为企业客户提供标准化接口、优化的能力和简化的用户体验。通
过的 CNCF Kubernetes一致性兼容认证的ACK为企业提供了一系列业务所需的必备能力,如安全治理
端到端可观测性、多云混合云等
镜像服务(ACR)作为企业云原生应用资产管理的核心,企业可以借之高效管理 Docker镜像
Helm Chart等应用资产,并与CCD工具结合,组成完整 DevSecOps流程。
3微服务产品家族
EDAS(企业分布式应用服务)是一个面向微服务应用的应用全生命周期PaaS平台,产品全面支
持HSF、Dubo、 Spring Cloud技术体系,提供ECS集群和K8s集群的应用开发、部署、监控、运
维等全栈式解决方案。
MsE(微服务引擎)是一个面向业界主流开源微服务框架 Spring cloud、Dubo的微服务平台,
包含治理中心、托管注册/配置中心,一站式的解决方案帮助用户提升微服务的开发效率和线上稳定性。
ACM(应用配置管理),是一款应用配置中心产品,实现在微服务、 Devops、大数据等场景下的
分布式配置服务,保证配置的安全合规。
CSB Micro Gateway(微服务网关服务)针对微服务架构下AP|开放的特点,提供能与微服务环
境的治理策略无缝衔接的网关服务,实现高效的微服务AP开放。The cloud-nctive Architecture White Paper
GTS(全局事务服务)用于实现分布式环境下特别是微服务架构下的高性能事务一致性,可以与多
种数据源、微服务框架配合使用,实现分布式数据库事务、多库事务、消息事务、服务链路级事务及各
种组合。
ARMS(应用实时监控服务)是一款应用性能管理产品,包含前端监控、应用监控和 Prometheus
监控三大子产品,涵盖了浏览器、小程序、APP、分布式应用和容器环境等性能管理,实现全栈式性能
监控和端到端全链路追踪诊断。
链路追踪( Tracing Analysis)为分布式应用的开发者提供了完整的调用链路还原、调用请求量统计、
链路拓扑、应用依赖分析等工具,能够帮助开发者快速分析和诊断分布式应用架构下的性能瓶颈,提高
微服务时代下的开发诊断效率。
PTs( Performance Testing Service)是一款云化测试工具,提供性能测试、AP|调试和监测
等多种能力,紧密结合监控、流控等产品提供一站式高可用能力,高效检验和管理业务性能。
4 Serverless产品家族
FC(函数计算)是一个事件驱动的全托管 Serverless计算服务,用户无需管理服务器等基础设施,
只需编写代码并上传,函数计算会准备好计算资源,并以弹性、可靠的方式运行业务代码。
SAE( Serverless应用引擎)实现了 Serverless架构+微服务架构的完美融合,真正按需使
用、按量计费,节省闲置计算资源,同时免去laaS运维,有效提升开发运维效率;SAE支持 Spring
coud、 Dubbo和HSF等流行的微服务架构。
Serverless工作流是一个用来协调多个分布式任务执行的全托管 Serverless云服务,致力于简化
开发和运行业务流程所需要的任务协调、状态管理以及错误处理等繁琐工作,让用户聚焦业务逻辑开发。
用户可以用顺序、分支、并行等方式来编排分布式仼务,服务会按照设定好的顺序可靠地协调任务执行,
跟踪每个任务的状态转换,并在必要时执行用户定义的重试逻辑,以确保工作流顺利完成
5 Service Mesh产品家族
托管服务网格(AsM)提供全托管的微服务应用流量管理平台,兼容sto的同时,支持多个
Kubernetes集群中应用的统一流量管理,为容器和虚拟机中应用服务提供一致的通信、安全和可观测
能力。
8阿里云
AHAS(应用高可用服务)是专注于提高应用及业务高可用的工具平台,目前主要提供应用架构探
测感知,故障注入式高可用能力评测和流控降级高可用防护三大核心能力,通过各自的工具模块可以快
速低成本的在营销活动场景、业务核心场景全面提升业务稳定性和韧性
6消息产品家族
B
消息队列 RocketMQ版是阿里云基于 Apache RocketMQ构建的低延迟、高并发、高可用、高可
靠的分布式消息中间件。该产品最初由阿里巴巴自研并捐赠给 Apache基金会,服务于阿里集团13年,
覆盖全集团所有业务,支撑干万级并发、万亿级数据洪峰,历年刷新全球最大的交易消息流转记录。
消息队列 Kafka版是阿里云基于 Apache Kafka构建的高吞吐量、高可扩展性的分布式消息队列服
务,广泛用于日志收集、监控数据聚合、流式数据处理、在线和离线分析等,是大数据生态中不可或缺
的产品之一,阿里云提供全托管服务,用户无需部署运维,更专业、更可靠、更安全。
消息队列AMQP版由阿里云基于AMQP标准协议自研,完全兼容 RabbitMQ开源生态以及多语
言客户端,打造分布式、高吞吐、低延迟、高可扩展的云消息服努。
微消息队列MQTT版是专为移动互联网(M)、物联网(o领域设计的消息产品,覆盖互动直播
金融支付、智能餐饮、即时聊天、移动Apps、智能设备、车联网等多种应用场景;通过对MQTT
Web socket等协议的全面支持,连接端和云之间的双向通信,实现C2C、C2B、B2C等业务场景之
间的消息通信,可支撑千万级设备与消息并发,真正做到万物互联。
阿里云消息服务MNS是一种高效、可靠、安全、便捷、可弹性扩展的分布式消息服务,能够帮助应
用开发者在他们应用的分布式组件上自由的传递数据、通知消息,构建松耦合系统。
事件总线 EventBridge是阿里云提供的一款无服务器事件总线服务,支持阿里云服务、自定义应用、
SaaS应用以标准化、中心化的方式接入,并能够以标准化的 CloudEvents1.0协议在这些应用之间路
由事件,帮助用户轻松构建松耦合、分布式的事件驱动架构。
7云原生数据库产品家族
PolarDB是阿里巴巴自主研发的下一代关系型分布式云原生数据库,目前兼容三种数据库引擎
MySQL、 PostgreSQL、高度兼容 Oracle语法;计算能力最高可扩展至1000核以上,存储容量最高
9The cloud-nctive Architecture White Paper
可达100T。 PolarDB经过阿里巴巴双十一活动的最佳实践,让用户既享受到开源的灵活性与价格,又
享受到商业数据库的高性能和安全性。
PolarDB-X(原DRDS升级版)是由阿里巴巴自主研发的云原生分布式数据库,融合分布式SQL
引擎DRDS与分布式自研存储XDB,专注解决海量数据存储、超高并发吞吐、大表瓶颈以及复杂计算
效率等数据库瓶颈难题,历经各届天猫双11及阿里云各行业客户业务的考验,助力企业加速完成业务数
字化转型。
8云原生大数据产品家族
云原生数据仓库 AnalyticDB MySQL版(简称ADB,原分析型数据库 MySQL版)是一种支持
高并发低延时查询的新一代云原生数据仓库,全面兼容 MySQL协议以及SQL:2003语法标准,可以对
海量数据进行即时的多维分析透视和业务探索,快速枃建企业云上数据仓库。产品规格按需可选,基础
版成本最低,适合B查询应用;集群版提供高并发数据实时写入和查询能力,适用于高性能应用;弹性
模式版本存储廉价按量计费,适用于10TB以上数据上云场景
云原生数据仓库 AnalyticDB PostgreSQL版,支持标准SQL2003,兼容 PostgreSQL
Greenplum,高度兼容 Oracle语法生态;具有存储计算分离,在线弹性平滑扩容的特点;既支持仼意
维度在线分析探索,也支持高性能离线数据处理;是面向互联网,金融,证券,保险,银行,数字政务
新零售等行业有竞争力的数据仓库方案。
50各个行业面临的挑战及解决方案
云原生解决方案
随着云计算的普及与云原生的广泛应用,越来越多的从业者、决策者清晰地认识到「云原生化将成为
企业技术创新的关键要素,也是完成企业数字化转型的最短路径」。因此,具有前瞻思维的互联网企业
从应用诞生之初就扎根于云端,谨慎稳重的新零售、政府、金融、医疗等领域的企业与机构也逐渐将业
务应用迁移上云,深度使用云原生技术与云原生架构。面对架构设计、开发方式到部署运维等不同业务
场景,基于云原生架构的应用通常针对云的技术特性进行技术生命周期设计,最大限度利用云平台的弹性、
分布式、自助、按需等产品优势。借助以下几个典型实践案例,我们来看看企业如何使用云原生架构解
决交付周期长、资源利用率低等实际业务问题。
案例一:申通快递核心业务系统云原生化上云案例
1)背景和挑战
作为发展最为迅猛的物流企业之一,申通快递一直积极探索技术创新赋能商业増长之路,以期达到降本提效目的。
目前,申通快递日订单处理量已达干万量级,亿级别物流轨迹处理量,每天产生数据已达到TB级别,使用1300+
个计算节点来实时处理业务。
过往申通快递的核心业务应用运行在DC机房,原有DC系统帮助申通安稳度过早期业务快速发展期。但伴随
着业务体量指数级增长,业务形式愈发多元化。原有系统暴露出不少问题,传统|OE架构、各系统架构的不规范、
稳定性、研发效率都限制了业务高速发展的可能。软件交付周期过长,大促保障对资源的特殊要求难实现、系统稳定
性难以保障等业务问题逐渐暴露。
在与阿里云进行多次需求沟通与技术验证后,申通最终确定阿里云为唯一合作伙伴,采用云原生技术和架构实现
核心业务搬迁上阿里云。2019年开始将业务逐步从DC迁移至阿里云。目前,核心业务系统已经在阿里云上完成流
量承接,为申通提供稳定而高效的计算能力。
2)云原生解决方
51The cloud-nctive Architecture White Paper
申通核心业务系统原架构基于∨ mware+ Oracle数据库进行搭建。随着搬迁上阿里云,架构全面转型为基于
Kubernetes的云原生架构体系。其中,引入云原生数据库并完成应用基于容器的微服务改造是整个应用服务架构重
构的关键点。
引入云原生数据库
通过引入OLTP跟OLAP型数据库,将在线数据与离线分析逻輯拆分到两种数据库中,改变此前完全依赖
Oracle数据库的现状。满足在处理历史数据查询场景下 Oracle数据库所无法支持的实际业务需求
应用容器化
伴随着容器化技术的引进,通过应用容器化有效解决了环境不一致的问题,确保应用在开发、测试、生产环
境的一致性。与虚拟机相比,容器化提供了效率与速度的双重提升,让应用更适合微服务场景,有效提升产研效率。
微服务改造
由于过往很多业务是基于 Oracle的存储过程及触发器完成的,系统间的服务依赖也需要 Oracle数据库
OGG同步完成。这样带来的问题就是系统维护难度高且稳定性差。通过引入 Kubernetes的服务发现,组建
微服务解决方案,将业务按业务域进行拆分,让整个系统更易于维护。
综合考虑申通实际业务需求与技术特征,最终选择了「阿里云ACK+神龙+云数据库」的云原生解决方案,从
而实现核心应用迁移上阿里云。
阿里云DNS
Linux-Bind
阿里云DNS
PrivateZone
流量入
F5-外部VP
F5-内部VP
SLB-外部P
SLB-内部VP
外部:sto- express.cn内部: stosystem. com
外部:sto.cn
内部: stosystem. com
接入层
Nginx
N
生产-2套 Nginx接入
生产-3套 ngres
外网
内网
内/外
内/外办公网
NET
Java
中
中间件
Redis
Oracle
云上中间件
云Reds
OLAP/OLTP
基础设施
VMWare虚拟化平台
ACK
物理机
神龙服务器
上云混合态架构
申通核心业务上云架构示意图
52阿里云
2.1架构阐述
基础设施,全部计算资源取自阿里云的神龙裸金属服务器。相较于一般云服务器(ECS), Kubernetes搭配
神龙服务器能够获得更优性能及更合理的资源利用率。且云上资源按需取量,对于拥有大促活动等短期大流量业务场
景的申通而言极为重要。相较于线下自建机房、常备机器,云上资源随取随用。在大促活动结束后,云上资源使用完
毕后即可释放,管理与采购成本更低,相应效率。
流量接入,阿里云提供两套流量接入,一套是面向公网请求,另外一套是服务内部调用。域名解析采用云DNS
及 PrivateZone。借助 Kubernetes的 Ingress能力实现统一的域名转发,以节省公网SLB的数量,提高运维管
理效率。
2.2平台层
基于 Kubernetes打造的云原生PaS平台优势明显突出。
打通 DevOps闭环,统一测试,集成,预发、生产环境
天生资源隔离,机器资源利用率高
流量接入可实现精细化管理;
集成了日志、链路诊断、 Metrics平台
统一 ApiServer接口和扩展,天生支持多云跟混合云部署。
23应用服务层
每个应用都在 Kubernetes上面创建单独的一个 Namespace,应用跟应用之间实现资源隔离。通过定义各个
应用的配置Yam模板,当应用在部署时直接编辑其中的镜像版本即可快速完成版本升级,当需要回滚时直接在本地
启动历史版本的镜像快速回滚。
2.4运维管理
线上 Kubernetes集群采用阿里云托管版容器服务,免去了运维 Master节点的工作,只需要制定 Worker节
点上线及下线流程即可。同时业务系统均通过阿里云的PaaS平台完成业务日志搜索,按照业务需求投交扩容任务,
系统自动完成扩容操作,降低了直接操作 Kubernetes集群带来的业务风险。
3)应用效益
成本方面:使用公有云作为计算平台,可以让企业不必因为业务突发增长需求,而一次性投入大量资金成本用于
采购服务器及扩充机柜。在公共云上可以做到随用随付,对于一些创新业务想做技术调研十分便捷。用完即释放
按量付费。另外云产品都免运维自行托管在云端,有效节省人工运维成本,让企业更专注于核心业务。
53The cloud-nctive Architecture White po
稳定性方面:首先,云上产品提供至少5个9以上的SLA服务确保系统稳定,而自建系统稳定性相去甚远。其
次,部分开源软件可能存在功能bug,造成故障隐患。最后,在数据安全方面云上数据可以轻松实现异地备份,阿
里云数据存储体系下的归档存储产品具备高可靠、低成本、安全性、存储无限等特点,让企业数据更安全。
效率方面:借助与云产品深度集成,研发人员可以完成一站式研发、运维工作。从业务需求立项到拉取分支开发,
再到测试环境功能回归验证,最终部署到预发验证及上线,整个持续集成流程耗时可缩短至分钟级。排查问题方面
研发人员直接选择所负责的应用,并通过集成的SLS日志控制台快速检索程序的异常日志进行问题定位,免去了登
录机器查日志的麻烦。
赋能业务:阿里云提供超过300余种的云上组件,组件涵盖计算、A、大数据、|OT等等诸多领域。研发人员
开箱即用,有效节省业务创新带来的技术成本
2案例二:完美日记电商业务案例
1)背景和挑战
作为美妆届的璀璨新星,完美日记( Perfect Diary)上线不到两年即成为天猫彩妆销冠,截至2020年4月
品牌SKU超过700个,全网用户粉丝数量超过2500万,月曝光量10亿+。
伴随着公司业务高速发展,技术运维对于面临着非常严峻的挑战。伴随着“双11”电商大促、“双12购物节、
小程序、网红直播带货呈现爆发式增长趋势,如何确保微商城系统稳定顺畅地运行成为完美日记面对的首要难题。其
中,比较突出几个挑战包含以下几点
系统开发迭代快,线上问题较多,定位问题耗时较长
频繁大促,系统稳定性保障压力很大,第三方接口和一些慢SQL存在导致严重线上故障的风险
压测与系统容量评估工作相对频繁,缺乏常态化机制支撑;
系统大促所需资源与日常资源相差较大,需要频繁扩缩容。
2云原生解决方案
完美日记与阿里云一起针对所面临问题以及未来业务规划进行了深度沟通与研讨。通过阿里云原生应用稳定性解
决方案以解决业务问题。引入阿里云容器服务ACK、 Spring Cloud Alibaba、PTs、AHAS、链路追踪等配套产品,
对应用进行容器化改造部署,优化配套的测试、容量评估、扩所容等研发环节,提升产研效率。
5阿里云
Redis云数据库
APM类全链路监控
品品品
ARMS
容器弹性伸缩
品品品
容器
限流,熔断,降级,系统保护
容器
AHAS
性能测试服务
监控,报警
MQ消息队列
云监控
DB弹性扩容
日志类数据采集、消费、投递
云盘
IAI NAS
云日志
及查询分析功能
完美日记核心应用架构示意图
方案的关键点是
通过容器化部署,利用阿里云容器服务的快速弹性应对大促时的资源快速扩容。
提前接入链路追踪产品,用于对分布式环境下复杂的服务调用进行跟踪,对异常服务进行定位,帮助客户在
测试和生产中快速定位问题并修复,降低对业务的影响。
使用阿里云性能测试服务(PTS)进行压测,利用秒级流量拉起、真实地理位置流量等功能,以最真实的互
联网流量进行压测,确保业务上线后的稳定运营。
采集压测数据,解析系统强弱依赖关系、关键瓶颈点,对关键业务接口、关键第三方调用、数据库慢调用、
系统整体负载等进行限流保护。
配合阿里云服务团队,在大促前进行ECs/RDs/安全等产品扩容、链路梳理、缓存/连接池预热、监控大屏
制作、后端资源保障演练等,帮助大促平稳进行
3)应用收益
高可用:利用应用高可用服务产品(AHAS)的限流降级和系统防护功能,对系统关键资源进行防护,并对整体
系统水位进行兜底;确保大促平稳进行,确保顺畅的用户体验。
容量评估:利用性能测试服务(PTS)和业务实时监控(ARMS)对系统单机能力及整体容量进行评估,对单
机及整体所能承载的业务极限量进行提前研判,以确保未来对业务大促需求可以做出合理的资源规划和成本预测
大促保障机制:通过与阿里云服务团队的进行多次配合演练,建立大促保障标准流程及应急机制,达到大促保障
常态化。
客户声音
使用ACK容器服务可以帮助我们快速拉起测试环境,利用PTS即时高并发流量压测确认系统水位,结合
ARMS监控,诊断压测过程中的性能瓶颈,最后通过AHAS对突发流量和意外场景进行实时限流降级,加上阿里云
团队保驾护航,保证了我们每一次大促活动的系统稳定性和可用性,同时利用ACK容器快速弹性扩缩容,节約服务
器成本50%以上。
完美日记技术中台负责人
55The cloud-nctive Architecture White Paper
3案例三:特步业务中台案例(零售、公共云)
背景和挑战
成立于2001年的特步,作为中国领先的体育用品企业
勹店数6230家。2016年,特步启动集团第三次
战略升级,打造以消费者体验为核心的“3+”(互联网+、体育+和产品+)的战略目标,积极拥抱云计算、大数据
等新技术,实现业务引领和技术创新,支撑企业战略变革的稳步推进。在集团战略的促使下,阿里云中间件团队受邀
对特步信息化进行了深度调研,挖掘阻碍特步战略落地的些许挑战
商业套件导致无法满足特步业务多元化发展要求,例如多品牌拆分重组所涉及的相关业务流程以及组织调整。
对特步而言,传统应用系统都是紧耦合,业务的拆分重组意味着必须重新实施部署相关系统
历史包袱严重,内部烟囱系统林立。通过调研,阿里云发现特步烟囱系统多达六十三套,仅供应商就有
三十余家。面对线上线下业务整合涉及到的销售、物流、生产、采购、订货会、设计等不同环节及场景:想
要实现全渠道整合,需要将几十套系统全部打通。
高库存、高缺货问题一直是服装行业的死结,特步同样被这些问题困扰着。系统割裂导致数据无法实时在线
并受限于传统单体 SQLServer数据库并发限制,6000多家门店数据只能采用T+1方式回传绐总部,直接
影响库存高效协同周转。
建设成本浪费比较严重,传统商业套件带来了“烟囱式”系统的弊端,导致很多功能重复建设、重复数据
模型以及不必要的重复维护工作。
2)云原生解决方案
阿里云根据特步业务转型战略需求,为之量身打造了基于云原生架构的全渠道业务中台解决方案,将不同渠道通
用功能在云端合并、标准化、共享,衍生出全局共享的商品中心、渠道中心、库存中心、订单中心、营销中心、用户
中心、结算中心。无论哪个业务线、哪个渠道、哪个新产品诞生或调整,「T组织都能根据业务需求,基于共享服务
中心现有模块快速响应,打破低效的“烟囱式”应用建设方式。全渠道业务中台遵循互联网架构原则,规划线上线下
松耦合云平台架构,不仅彻底摆脱传统「拖业务后腿的顽疾并实现灵活支撑业务快速创新,将全渠道数据融通整合
在共享服务中心平台上,为数据化决策、精准营销、统一用户体验奠定了良好的产品与数据基础,让特步真正走上了
“互联网+”的快车道。
2017年1月特步与阿里云启动全渠道中台建设,耗时6个月完成包括需求调研、中台设计、研发实施、测试验
证等在内的交付部署,历经4个月实现全国42家分公司、6000+门店全部上线成功。以下是特步全渠道业务中台
总体规划示意图,
56阿里云
xm口
CRM
4②带
门店零售POS分销订货B2B天猫淘宝
官方商城特跑族APP特购商城
CRM分销商城B2B2C会员商城
WMS
智慧导购私人定制
渠道分销管理
门店零售运营
门店运营
全渠道运营
采购管理
消售管理
要货管理促销管理
门店管理
库存管理渠道管理
库存管理
业绩管理
导购管理
货权转移
调拨管理发货管理
绩效考核
渠道中心
库存中心
数据中心
业务中台
基础服务
商品服务
出入库服务
报表服务
组织服务
类目服务
盘点服务
充计服务
门店服务
促销服务
库存可视
标签服务
SAP
PLM
MES
HR/OA
下面是基于云原生中间件的技术架构示意图
应
OMS
DRP
CRM
库存平衡
全渠道运营平台
库存中心
结算中心
主数据
中
商品中心
会员中心
营销中心
EDAS
MQ
ARMS
PTS
CSB
阿里云原生中间件产品
基础设施
阿里云|aaS服务
后台系统
库存平衡
架构的关键点
应用侧:新技术架构全面承载面向不同业务部门的相关应用,包括门店POS、电商OMS、分销商管理供销
存DRP、会员客户管理CRM。此外,在全渠道管理方面也会有一些智能分析应用,比如库存平衡,同时可
以通过全渠道运营平台来简化全渠道的一些配置管理。所有涉及企业通用业务能力比如商品、订单等,可以
直接调用共享中心的能力,让应用“更轻薄”。
57The cloud-nctive Architecture White Paper
共享中心:全渠道管理涉及到参与商品品类、订单寻源、共享库存、结算规则等业务场景,也涉及与全渠道
相关的会员信息与营销活动等。这些通用业务能力全部沉淀到共享中心,向不同业务部门输出实时/在线/统
/复用的能力。直接将特步所有订单/商品/会员等信息融合、沉淀到一起,从根本上消除数据孤岛。
技术层:为了满足弹性、高可用、高性能等需求,通过 Kubernetes/EDAS/MQ/ARMS/PTs等云原生中间
件产品,目前特步核心交易链路并发可支撑10wps且支持无线扩客提升并发能力。采用阿里历经多年双11
考验的技术平台,稳定性/效率都得到了高规格保障,让开发人员能够更加专注在业务逻辑实现,再无后顾之忧。
基础设施:底层的计算、存储、网络等laS层资源。
后台系统:客户内部的后台系统,比如SAP、生产系统、HROA等
3)应用收益
全渠道业务中台为特步核心战略升级带来了明显的变化,逐步实现了|T驱动业务创新。
经过中台改造后,POS系统从离线升级为在线化。包括收银、库存、会员、营销在内的POS系统核心业务全
部由业务中台统一提供服务,从弱管控转变为集团强管控,集团与消费者之间真正建立起连接,为消费者精细化管理
奠定了坚实的基础。
中台的出现,实现了前端渠道的全局库存共享,库存业务由库存中心实时处理。借助全局库存可视化,交易订单
状态信息在全渠道实时流转,总部可直接根据实时经营数据对线下店铺进行销售指导,实现快速跨店商品挑拨。中台
上线后,售罄率提升8%,缺货率降低12%,周转率提升20%,做到赋能一线业务。
信息化驱动业务创新,通过共享服务中心将不同渠道类似功能在云端合并共享,打破低效的“烟囱式”应用
建设方式,吸收互联网DDD领域驱动设计原则,设计线上线下松耦合云平台架构,不仅彻底摆脱了传统∏T拖业务
后腿的顽疾并灵活支撑业务快速创新。全渠道数据融通整合在共享服务中心平台上,沉淀和打造出特步的核心数据资
产,培养出企业中最稀缺的“精通业务,懂技术”创新人才,使之在企业业务创新、市场竞争中发挥核心作用。截止
2019年初,业务部门对部门认可度持续上升,目前全渠道业务支撑系统几乎全部自主搭建,80%前台应用已经
全部运行在中台之上,真正实现技术驱动企业业务创新。
4中国联通号卡业务云化案例(传统业务,专有云)
1)背景和挑战
直以来,作为国内三大运营商之一的中国联通始终以新理念新模式促进信息基础设施升级。推动网络资源优化
演进升级,持续提升网络竞争力。中国联通BSS核心系统随着用户规模增长,个性化业务复杂性日趋提升、竞争压
力不断加大,以CBSS1.0为核心的集中业务系统,在系统架构、支撑能力、业务运营等方面无法满足一体化运营发
展要求的问题日益突出,主要体现在
58阿里云
难以满足个性化业务模式和新业务场景需求,客户与一线营业人员体感较差。
日常运营面临的新问题数量、种类多,待优化提升的需求压力大
传统系统架构制肘了开放性、扩展性、体验性、稳定性均的提升,支撑能力与支撑模式无法满足集团冮集中
一体化运营的业务需要。
外围对接系统众多。作为全国唯一的号卡资源管理方和服务提供方,号卡资源集中共享系统需要与包括
cBSS、商城网厅、省分BSS、北六ESS、总部CRM、沃易购等在内的四十余种外围系统对接。因为要向
全国所有销售系统提供号卡资源服务,对号卡资源集中共享系统提出了非常严苛的性能要求,进而也提岀了
更高规格的扩展性要求。
面向全渠道,号卡资源集中共享系统除了要向目前己存自有渠道(营业厅、电子渠道)、社会渠道提供高效
稳定的服务外,也要考虑未来面向互联网渠道等新兴渠道开放服务能力。这都对号卡资源集中共享系统的服
务化、能力开放能力提出了很高的要求
高性能挑战,满足全国要求的性能要求估算:面向未来的互联网营销模式以及物联网等新业务领域可能带来
的更高要求。传统架构存在无法横向扩展、同步调用流程长的问题。引入分布式架构成为迫在眉睫的需求。
高可用挑战,号卡资源作为中国联通的核心资源的同时,号卡资源管理同样是销售流程的关键环节,号卡资
源管理系统直接关系到销售业务是否可正常开展全国集中的号卡资源管理系统、直接关系到全国销售业务是
否能正常开展,引入分布式架构的同时必须确保高可用。
2)云原生解决方案
中国联通与阿里云结合阿里云原生PaaS、阿里飞天操作系统、阿里云原生数据库以及中国联通天宫平台,共同
研发运营商级专有云平台“天宫云”,支撑中国联通核心业务应用。合作过程中,阿里云从互联网架构,业务中台架构,
云原生技术等方面出发,基于阿里云业务中台,互联网技术上的最佳实践,云原生完整技术产品的整体架构方案。阿
里云原生PaaS平台为业务能力层和核心能力层及能力开放管理平台提供包括:分布式服务框架、分布式消息服务
分布式数据服务、分布式监控等云原生技术服务、管理业务能力、技术组件、云原生运维管理服务在内的基本技术支持。
全面云化的系统:基于“平台+应用”的三层分布式架构模式,采取高内聚、低耦合、易扩展、服务化的设计原则
由去中心化的服务框架为应用提供服务,由能力开放平台提供能力集成和能力开放。
服务化设计:充分发挥平台线性扩展能力和服务化设计优势,纵向分多个中心、中心分多个模块,横向分层设计,
从而实现业务需求的灵活响应和快速支撑。
下面是中国联通号卡应用的技术架构示意图。
59The cloud-nctive Architecture White Paper
SLB软负载均衡
监控运维
己CsB能力开放平台
协议转换
基础监控
EDAS分布式服务
注册订购
协议转换
分布式链路跟踪
号码查询
靓号费用查询成卡占用
靓号减免审批
号码入库
卡入库
减免结果生效
MQ分布式消息服务
日志分析
异步事务
号码中心服务卡中心服务操作员中心服务报表中心服务日志中心服务
通知服务
备份恢复
DRDs分布式关系型数据库服务+RDS数据库
●Me分布式缓存oss分布式文件
业务参数
成卡文件
安全防护
会话信息
业务数据
下面是中国联通号卡应用的部署架构示意图。
负载均衡
负载均衡
WEB应用
APP应用
能力开放平台
接入应用
EDAS
EDAS
EDAS
虚拟机
虚拟机
虚拟机
HSF服务调用
号码中心
卡中心
操作员中心
报表中心
日志中心
业务服务
EE EDAS
亲EDAs
的EDAs
EDAS
亲EDAS
虚拟机
虚拟机
虚拟机
虚拟机
O机
平台服务
部署架构的优势
共享平台服务:平台组件服务单独部署,自身分布式设计,全局应用、服务共享使用
分层分中心部署:应用、业务服务分层、分中心、分区部署,高度灵活、稳定、可扩展。
60阿里云
3)应用收益
彻底解决号码资源在多省、多渠道、多系统共同管理问题,过去号码状态同步问题多,一号多卖现象严重,有效
降低业务冲突
解决销售过程中由于传统架构瓶颈造成的各类业务问题,开卡业务效率提升了10倍,单日选号访问量提升了3倍,
需求响应时间缩短了50%,上层业务效率显著提升。
支撑业务快速创新,比如:支撑微信红点业务推广,由推广前每天访问量1000万+,到推广一周后快速上升到1.1
亿+,完全满足业务部门销售策略支持需求
实现集中管控,资源合理分配,彻底解决数据分散、号码利用率低问题。
5 Timing App的 Serverless实践案例
1)背景和挑战
作为广受好评的学习应用, Timing App专注于帮助社区用户提升学习凝聚力,达成学习目标。目前已有超过
700万人通过 Timing进行高效学习。与传统在线学习应用不同, Timing app提供了 Timing自习室、图书馆学习、
视频打卡、学习日记、契约群、学习服务等多类具有社交性质的在线教育服务,帮助用户找到自己的学习节奏,找到
坚持学习的一万种理由。 Timing业务本身具有潮汐特性,用户访问主要集中在晚间和节假日。受疫情影响,春节期
间峰值流量暴增4倍,公司面临较大的运维成本压力。在用户、流量爆发式增长背景下, Timing App不得不直面以
下四大痛点:
系统稳定性差。原有PHP单应用架构系统无法做到线性快速扩容,在业务高峰时段,系统问题频繁发生,严
重影响用户体验。
产品迭代缓慢。随着业务的高速发展,原有单体架构对于产品的迭代力不从心,没法快速响应研发需求。
资源使用浪费。由于业务具有非常强的流量潮夕特征,需要按照业务高峰阶段进行资源保有配置,造成资源
的浪费。
技术成本昂贵。以前的团队除了技术负责人及少数团队新成员外,基本缺乏微服务架构实战经验。想要实现
微服务改造,急需能够快速上手的平台支撑,需要最大限度降低底层laS,容器以及常用微服务套件的学习
成本。
2云原生解决方案
61The cloud-nctive Architecture White po
阿里云应用引擎 Serverless(SAE),基于 Serverless架构,屏蔽了底层laS运维和K8s细节,区别
于Fas形态的 Serverless产品,用户无需修改编程模型,零代码改造就能直接使用。同时,完美结合 Spring
Cloud /dubao等微服务架构,提供应用发布、管理和服务治理等应用全生命周期的服务,完美贴合 Timing的技术
需求:极限弹性伸缩,应用生命周期灵活管理,完美支持主流微服务架构。
下图的方案架构示意图。
Spring cloud
用户中心
群组中心
资源中心
调度中心
图书馆中心
单体
应用
认证中心
学习中心
支付中心
自习中心
XX中心
PHP应用
配置中心
注册中心
多发布策略
服务安全
微服务化
SAE
应用生命周期管理
极致弹性伸缩管理
实时应用监控
优雅上下线
日志管理
负载均衡
流量可控制
ECS
平台 Kubernetes集群
屏蔽的底层资源
平台资源池
方案的关键优势是
。利用弹性伸缩,应对不确定突发流量。提供秒级自动弹性&定时弹性能力,帮助应用轻松应对大促峰值流量,
保证SLA的同时也节省机器保有成本。多适用于互联网、游戏、在线教育行业。
应用环境随需灵活启停,节约成本。提供了一键启停开发测试环境的能力,即开即用,节省成本,方便运维
适用于对成本敏感、云上有多套环境但部分环境闲置率较高的企业型客户(不限行业)。
中小企业快速构建云上微服务应用。帮助用户屏蔽底层aaS购买和运维细节、底层K8s细节,低门槛部署
微服务应用。适用于初创型/上升期的公司(不限行业),业务增长很快,对增长有较高预期,但人员配置
跟不上
整体技术架构更为清晰,每个服务相互独立且职责明确。加之阿里云应用引擎 Serverless(SAE)加持,
让客户只关注在业务层,做好产品。
3)应用收益
Timing App在较短时间内和阿里云共同梳理业务结构,将之前的PHP单体应用转型为了微服务架构,包括用
户中心、群组中心、资源中心、调度中心、图书馆中心、认证中心、学习中心、支付中心、自习中心等九大业务模块。
方案实施后, Timing App实现了降本增效,护航系统稳定性,为提升用户体验,增加用户粘性作岀了重要贡献。
62阿里云
降成本:大幅节省自建微服务架枃的云服务器成本。基于秒级弹性能力,无需长期保有固定资源,按需自动弹、
按分钟计费,极大提升了资源利用率。
提效率:屏蔽了底层|aaS购买、底层 Kubernetes细节和运维的烦恼,低门槛部署 Dubbo/Spring Cloud等
微服务应用,支撑新业务快速上线。此外,还提供了QPS、RT、接口调用量、错误数等实时监控功能,帮助研发
人员快速定位问题,提升诊断效率,让企业聚焦于业务本身。
业务稳定:基于阿里云应用引擎 Serverless(SAE)的定时弹性能力和基于监控指标弹性( CPU/Memory等)
无须容量规划,秒级弹性,便可轻松应对流量暴増,保障SLA。
63云原生架构未来发展趋势
容器技术发展趋势
在云端: Serverless技术渐融入主流
动态、混合、分布式的云环境将成为新常态
极致弹性,免运维,最安全,高效开发
Serverless
分布式云
统一技术找:统一应用界面,统一管理界面
与云深度融合,复杂性下沉,无限弹性
Serverless容器
从多云/混合云
公共云服务能力将延伸到边缘计算和DC
新的应用开发、交付范形开始形成
函数计算
分布式云
云原生架构推进无边界云计算,促成云边端应用
一体协调
无处不在的计算催生新代容器实现
云原生操作系统开始浮现
针对计算场景优化、安全、轻、高效
定义开放标准,向下封装资源,向上克描应用
基于 MicroM的安全容器
新一代的计算单元
云原生操作系统
计算、存储、网络、安全的云原生进化
基于 WebAssembly的可移植、轻量容器
多种工作负载、海量计算任务,多种异构算力的
s虚拟化创新,如 cgroup v2提升隔离性
高效调度和编
标准化、自动化、可移植、安全可控的应用交付、
管理体系
(1)趋势一:无处不在的计算催生新一代容器实现
随着互联网的发展到万物智联,5G、AoT等新技术的涌现,随处可见的计算需求已经成为现实。针对不同计算
场景,容器运行时会有不同需求。 Katacontainer、 Firecracker、 vIsor、 Unikernel等新的容器运行时技术层出
不穷,分别解决安全隔离性、执行效率和通用性三个不同维度的要求。OCl( Open Container Initiative)标准的出现,
使不同技术采用一致的方式进行容器生命周期管理,进一步促进了容器引擎技术的持续创新。其中,我们可以预见以
下几个细分方向的未来趋势
64阿里云
基于 MicroM的安全容器占比将逐渐增加,提供更强的安全隔离能力。虚拟化和容器技术的融合,已成为未来
重要趋势。在公共云上,阿里云容器服务已经提供了对基于 Kata Container的阿里云的袋鼠容器引擎支持,可以运
行不可信的工作负载,实现安全的多租隔离。
基于软硬一体设计的机密计算容器开始展露头角。比如阿里云安全、系统软件、容器服务团队以及蚂蚁金服可信
原生团队共同推出了面向机密计算场景的开源容器运行时技术栈 inclavare- containers,支持基于 Intel sGX机密
计算技术的机密容器实现,如蚂蚁金服的○ccum、开源社区的 Graphene等 Libary OS。它极大降低了机密计算
的技术门槛,简化了可信应用的开发、交付和管理。
WebAssembly作为新一代可移植、轻量化、应用虚拟机,在loT,边缘计算,区块链等场景会有广泛的应用前景
WASMNWAS将会成为一个跨平台容器实现技术。近期Sooo推出的 WebAssembly Hub就将WASM应用通
过OC|镜像标准进行统一管理和分发,从而更好地应用在 Isto服务网格生态中。
2)趋势二:云原生操作系统开始浮现
Kubernetes已经成为云时代的操作系统。对比 Linux与 Kubernetes的概念模型,他们都是定义了开放的、
标准化的访问接口;向下封装资源,向上支撑应用。
应用
云原生应用
操作系统( Operating System)
云原生操作系统( Cloud Native OS)
工具与开发库( Utilities and Libraries)
软件包
拓展服务
管理
镜像
Shell
GCC/Java
标准类库
配置管理
um等
服务网格 Isto A计算 Kubeflow无服务器 Native应用管理OAM
仓库
AP与系统服务( API and System Services)
Kubernetes Resources
POSIX
系统服务
Window管理器
工作负载
存储卷
服务发现/路由
内核( OS Kernel)
Kubernetes core
安全
System Calls
服务
Kubernetes Core
服务
资源调
进程管理
安全模型
O管理
资源调度
Pod管理
安全模型
CRD/Controlle
设备驱动
存储CS,网络CN, Device Plugin,云供应商CCM等
硬件
公共云、专有云、边缘计算
它们都提供了对底层计算、存储、网络、异枃计算设备的资源抽象和安全访问模型,可以根据应用需求进行资源
调度和编排。 Linux的计算调度单元是进程,调度范围限制在一台计算节点。而 Kubernetes的调度单位是Pod
可以在分布式集群中进行资源调度,甚至跨越不同的云环境。
65The cloud-nctive Architecture White Paper
Redis Mysal Kafka
TIDB
Elastic Tensor
keeper
Search Flow
Spark Flink
KU
ubernetes
Compute
Network
Storage
Security
Public Cloud
Edge Computing
Private Cloud
从无状态应用,到企业核心应用,到数据智能应用
过往 Kubernetes上主要运行着无状态的Web应用。随着技术演进和社区发展,越来越多有状态应用和大数
据A应用负载逐渐迁移到 Kubernetes上。Fink、 Spark等开源社区以及 Cloudera、 Databricks等商业公司都
开始加大对 Kubernetes的支持力度。
统一技术栈提升资源利用率:多种计算负载在 Kubernetes集群统一调度,可以有效提升资源利用率。
Gartner预测“未来3年,70%A任务运行在容器和 Serverless上。”A模型训练和大数据计算类工作负载需
要 Kubernetes提供更低调度延迟、更大并发调度吞吐和更高异构资源利用率。阿里云在和 Kubernetes上游社区
共同合作,在 Scheduler v2 framework上,通过扩展机制增强 Kubernetes调度器的规模、效率和能力,具备更
好的兼容性,可以更好的支撑多种工作负载的统一调度。
统一技能栈降低人力成本: Kubernetes可以在DC、云端、边缘等不同场景进行统一部署和交付。云原生提
倡的 DevOps文化和工具集可以有效提升技术迭代速度并降低人力成本
加速数据服务的云原生化:由于计算存储分离具备巨大的灵活性和成本优势,数据服务的云原生化也逐渐成为
趋势。容器和 Serverless的弹性可以简化对计算任务的容量规划。结合分布式缓存加速(比如Aux或阿里
Jindofs)和调度优化,大大提升数据计算类和A任务的计算效率。
3)趋势三: Serverless容器技术逐渐成为市场主流
Serverless和容器技术也开始融合得到了快速的发展。通过 Serverless容器,一方面根本性解决
Kubernetes自身复杂性问题,让用户无需受困于 Kubernetes集群容量规划、安全维护、故障诊断等运维工作
方面进一步释放云计算能力,将安全、可用性、可伸缩性等需求下沉到基础设施实现。
4)趋势四:动态、混合、分布式的云环境将成为新常态
云已是大势所趋,但对于企业客户而言,有些业务出于对数据主权、安全隐私的考量,会采用混合云架构。
66阿里云
些企业为了满足安全合规、成本优化、提升地域覆盖性和避免云厂商锁定等需求,会选择多个云厂商。混合云/多云
架构已成为企业上云新常态。 Gartner指出,"到2021,超过75%的大中型组织将采用多云或者混合∏战略。
此外边缘计算将成为企业云战略的重要组成部分,为应用提供更低网络延迟,更高网络带宽和更低网络成本。我
们需要有能力将智能决策、实时处理能力从云延展到边缘和loT设备端。
随着云平台已经成为企业数字化转型的创新平台,一个变化随之产生—云正在靠近它们。在分布式云中,公有
云的服务能力可以位于不同的物理位置,而公共云平台提供者会负责服务的运维、治理、更新和演变。
然而不同环境的基础设施能力、安全架构的差异会造成企业∏架构和运维体系的割裂,加大云战略实施复杂性
增加运维成本。在云原生时代,以 Kubernetes为代表的云原生技术屏蔽了基础设施差异性,推动了以应用为中心
的混合云/分布式云架构的到来。从而更好地支持不同环境下应用统一生命周期管理和统一资源调度。 Kubernetes
已经成为企业多云管理的事实基础
阿里云容器服务ACK去年9月份发布了混合云20架构,提供了完备的混合云 Kubernetes管理能力。
ASM-统一流量治理
曰日志服务
跨集群路由
流量管理
地域亲和性
故障转移
全中心
APM
发布策略
弹性策略
全链路监控
全局服务发现
prometheus
ASM-统一流量治理
安全治理
可观测性
弹性调度
应用发布
ASM Sidecar(可选)
ASM Sidecar(可选)
ASM Sidecar(可选)
接入 Agent
ACK接入 Agent
ACK公有云K8s集群
ACK敏捷版K8s集群
其他云K8s集群
混合云2.0架构
ACK提供了统一集群管理能力,除了可以管理阿里云 Kubernetes集群之外,还可以纳管用户在|DC的自有
Kubernetes集群和其他云的 Kubernetes集群。利用统一的控制平面实现多个集群的统一的安全治理、可观测性、
应用管理、备份恢复等能力。比如利用日志服务、托管 Prometheus服务,可以无侵入的方式帮助用户对线上、线
下集群有一个统一的可观测性大盘。利用云安全中心,AHAS可以帮助客户在混合云的整体架构中发现并解决安全
和稳定性风险。The cloud-nctive Architecture White Paper
托管服务网格ASM提供统一的服务治理能力,结合阿里云云企业网CEN、智能接入网关SAG提供的多地域、
混合云网络能力,可以实现服务就近访问,故障转移,灰度发布等功能。
ACK也提供了统一的应用交付能力,通过 Stops方式可以将应用安全、一致、稳定地发布在多个不同的云环
境中。配合网格ASM提供的流量管理能力,可以支持云容灾、异地多活等应用场景,提升业务连续性。
2基于云原生的新一代应用编程界面
Kubenetes已经成为了云原生的操作系统,而容器成为了操作系统调度的基本单元,同时定义了应用交付的标
准。但是对于应用开发者来说,这些还远没有深入到应用的架构,改变应用的编程界面。但是这种变革已经在悄然发
生了,而且有不断加速之势。
Sidecar架构彻底改变了应用的运维架构。由于 Sidecar架构支持在运行时隔离应用容器与其他容器,因此
原本在虚拟机时代和业务进程部署在一起的大量运维及管控工具,都被剥离到独立的容器里进行统一管理。
对于应用来说,仅仅是按需声明使用运维能力,能力的实现成为云平台的职责。
应用生命周期全面托管。在容器技术基础上,应用进一步描述清晰自身状态(例如通过 Liveness Probe)
描述自身的弹性指标以及通过 Service mesh和 Serverless技术将流量托管给云平台。云平台将能够全面
管理应用的生命周期,包括服务的上下线、版本升级、完善的流量调配、容量管理等,并保障业务稳定性。
用声明式配置方式使用云服务。云原生应用的核心特点之一就是大量依赖云服务(包括数据库、缓存、消息等)
构建,以实现快速交付。而这些服务的配置实际上是应用自身的资产,为了能够让应用无缝地运行在混合云
的场景,应用逐渐开始以基础设施即代码的方式使用云服务。正因如此,相关的产品如 Terraform、 Pulumi
越来越受到应用开发者的拥抱。
语言无关的分布式编程框架成为一种服务。为了解决分布式带来的技术挑战,传统中间件需要在客户端SDK
编写大量的逻辑管理分布式的状态。今天,还是基于 Sidecar架构,我们看到很多项目在把这些内客下沉
到Sidecar中,并通过语言无关的AP|(基于gppc/http)提供给应用。这一变化进一步简化应用代码
的逻辑和应用研发的职责,例如配置的绑定,身份的认证和鉴权都可以在 Sidecar被统一处理。在这个方面
dapr.io和 cloudstate.o是先行者。
综上,包括生命周期管理、运维管理、配置范围和扩展和管理、以及语言无关的编程框架,一起构成了崭新的应
用与云之间的编程界面。这一变革的核心逻辑还是把应用中和业务无关的逻辑和职责,剥离到云服务,并在这个过程
中形成标准,让应用开发者能够在专有云、公有云、或者混合云的场景中,都能有一致的研发运维体验。
68〔阿里云
Serverless发展趋势
近年来, Serverless一直在高速发展,呈现出越来越大的影响力。在这样的趋势下,主流云服务商也在不断丰
富云产品体系,提供更便捷的开发工具,更高效的应用交付流水线,更完善的可观测性,更丰富的产品间集成。
1)趋势一: Serverless将无处不在
任何足够复杂的技术方案都可能被实现为全托管、 Serverless化的后端服务。不只是云产品,也包括来自合作
伙伴和三方的服务,云及其生态的能力将通过AP|+ Serverless来体现。事实上,对于任何以AP作为功能透出
方式的平台型产品或组织,例如钉钉、微信、滴滴等等, Serverless都将是其平台战略中最重要的部分。
2)趋势二: Serverless将通过事件驱动的方式连接云及其生态中的一切
通过事件驱动和云服务连接, Serverless能力也会扩展到整个云的生态。无论是用户自己的应用还是合作伙伴
的服务,无论是 on-premise环境还是公有云,所有的事件都能以 Serverless的方式处理。云服务及其生态将更
紧密地连接在一些,成为用户构建弹性、高可用应用的基石。
3)趋势三: Serverless计算将持续提高计算密度,实现最佳的性能功耗比和性能价棓比
虚拟机和容器是两种取向不同的虚拟化技术,前者安全性强、开销大,后者则相反。 Serverless计算平台一方
面要求兼得最高的安全性和最小的资源开销,另一方面要保持对原有程序执行方式的兼容,比如支持任意二进制文件,
这使得适用于特定语言VM的方案不可行。以 AWS Fire Cracker为例,其通过对设备模型的裁剪和 kernel加载流
程的优化,实现百毫秒的启动速度和极小的内存开销,一台裸金属实例可以支持数以千计的实例运行。结合应用负载
感知的资源调度算法;虚拟化技术有望在保持稳定性能的前提下,将超卖率提升一个数量级。
当 Serverless计算的规模与影响力变得越来越大,在应用框架、语言、硬件等层面上根据 Serverless负载特
点进行端对端优化就变得非常有意义。新的Java虚拟机技术大幅提高了Java应用启动速度,非易失性內存帮助实
例更快被唤醒;CP∪硬件与操作系统协作对髙密环境下性能扰动实现精细隔离,所有新技术正在创造崭新的计算环
实现最佳性能功耗比和性能价格比的另一个重要方向是支持异构硬件。长期以来,κ86处理器的性能越来越难
以提升。而在A|等对算力要求极高的场景,GPU、FPGA、TPU( Tensor Processing Units)等架构处理器的
计算效率更具优势。随着异构硬件虛拟化、资源池化、异枃资源调度和应用框架支持的成熟,异构硬件的算力也能通
过 Serverless的方式释放,大幅降低用户使用门槛。
69阿里云