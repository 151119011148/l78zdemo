译者介绍
薛命灯
毕业于厦门大学软件学院,十余年软件
开发和架构经验, InfoQ高级社区编
辑。译有《硅谷革命》《生产微服务》
等书。微信公众号 CodeDeepTURING圖灵程序设计M
Kafka权威指南
Kafka: The definitive Guide
[美] Neha Narkhede Gwen Shapira Todd Palino著
薛命灯译
Beijing· Boston. Farnham. Sebastopol· Tokyo
OREILLY G
O'Reilly Media,nc授权人民邮电出版社出版
人民邮电出版社
北京图书在版编目(CIP)数据
Kafka权威指南/(美)妮哈·纳克海德
( Neha Narkhede),(美)格温·沙皮拉( Gwen Shapira),
(美)托德·帕利诺( Todd palin)著;薛命灯译.
北京:人民邮电出版社,2018.1
(图灵程序设计丛书)
ISBN978-7-115-47327-1
Ⅰ.①K…Ⅱ.①妮…②格…③托…④薛…Ⅲ.①
分布式操作系统一指南Ⅳ.①TP316.4-62
中国版本图书馆CIP数据核字(2017)第290682号
内容提要
本书是关于 Kafka的全面教程,主要内容包括: Kafka相对于其他消息队列系统的优点,
主要是它如何完美匹配大数据平台开发;详解 Kafka内部设计;用Kaka构建应用的最佳实践;
理解在生产中部署Kaka的最佳方式;如何确保Kaka集群的安全。
本书适合Java开发人员、大数据平台开发人员以及对分布式系统感兴趣的读者阅读。
◆著
[] Neha Narkhede Gwen Shapira Todd Palino
译
薛命灯
责任编辑朱巍
执行编辑张海艳
责任印制彭志环
◆人民邮电出版社出版发行北京市丰台区成寿寺路11号
邮编100164电子邮件315@ptpress.com.cn
网址http://www.ptpress.com.cn
北京鑫正大印刷有限公司印刷
◆开本:800×10001/16
印张:14.5
字数:343千字
2018年1月第1版
印数:1-4000册
2018年1月北京第1次印刷
著作权合同登记号图字:01-2017-6476号
定价:69.00元
读者服务热线:(010)51095186转600印装质量热线:(01081055316
反盗版热线:(010)81055315
广告经营许可证:京东工商广登字20170147号版权声明
o 2017 by Neha Narkhede, Gwen Shapira, Todd Palin
Simplified Chinese Edition, jointly published by O'Reilly Media, Inc and Posts Telecom Press,
2018. Authorized translation of the English edition, 2017 O Reilly Media, Inc, the owner of all
rights to publish and sell the same
All rights reserved including the rights of reproduction in whole or in part in any form
英文原版由 O'Reilly media,Inc.出版,2017。
简体中文版由人民邮电出版社出版,2018。英文原版的翻译得到 O'Reilly Media,inc.的授
权。此简体中文版的出版和销售得到出版权和销售权的所有者— OReilly media,Inc.的
许可。
版权所有,未得书面许可,本书的任何部分和全部不得以任何形式重制。O Reilly Media,nc介绍
O'Reilly media通过图书、杂志、在线服务、调查研究和会议等方式传播创新知识
自1978年开始, OReilly一直都是前沿发展的见证者和推动者。超级极客们正在开创
着未来,而我们关注真正重要的技术趋势—通过放大那些“细微的信号”来刺激社
会对新科技的应用。作为技术社区中活跃的参与者, O'Reilly的发展充满了对创新的
倡导、创造和发扬光大。
Oˇ Reilly为软件开发人员带来革命性的“动物书”;创建第一个商业网站(GNN);组
织了影响深远的开放源代码峰会,以至于开源软件运动以此命名;创立了Make杂志,
从而成为DIY革命的主要先锋;公司一如既往地通过多种形式缔结信息与人的纽带。
oˆ Reilly的会议和峰会集聚了众多超级极客和高瞻远瞩的商业领袖,共同描绘出开创
新产业的革命性思想。作为技术人士获取信息的选择, OReilly现在还将先锋专家的
知识传递给普通的计算机用户。无论是通过图书出版、在线服务或者面授课程,每
项 O'Reilly的产品都反映了公司不可动摇的理念—信息是激发创新的力量
业界评论
“ O'Reilly radar博客有口皆碑。”
— Wired
OReilly凭借一系列(真希望当初我也想到了)非凡想法建立了数百万美元的业务。”
-Business 2.0
O Reilly Conference是聚集关键思想领袖的绝对典范。”
CRN
“一本 O'Reilly的书就代表一个有用、有前途、需要学习的主题。
Irish Times
Tim是位特立独行的商人,他不光放眼于最长远、最广阔的视野,并且切实地按照
Yogi Berra的建议去做了:‘如果你在路上遇到岔路口,走小路(岔路)。’回顾过去,
rim似乎毎一次都选择了小路,而且有几次都是一闪即逝的机会,尽管大路也不错。”
Linux journal目录
序
. XI
前言
第1章初识 Kafka
1.1发布与订阅消息系统
1.1.1如何开始…
1.1.2独立的队列系统
12 Kafka登场
1.2.1消息和批次…
444
1.2.2模式
1.2.3主题和分区
1.24生产者和消费者
1.2.5 broker和集群
1.2.6多集群
1.3为什么选择Kaka
56788
13.1多个生产者…
1.3.2多个消费者
1.33基于磁盘的数据存储
1.34伸缩性
1.3.5高性能
14数据生态系统
15起源故事
1.5.1 LinkedIn的问题……………
1.5.2Kaka的诞生……
121.5.3走向开源……
12
1.54命名
13
1.6开始Kaka之旅…
13
第2章安装 Kafka……
……14
21要事先行…
2.1.1选择操作系统…
2.1.2安装Java…
21.3安装 Zookeeper………
15
22安装 Kafka Broker…
2.3 broker配置…
23.1常规配置
2.3.2主题的默认配置
24硬件的选择
24.1磁盘吞吐量…
23
2.4.2磁盘容量……
243内存…
…23
2.4.4网络…
………………24
2.4.5CPU………
24
2.5云端的 Kafka
24
2.6Kaka集群
…24
26.1需要多少个 broker…
2.6.2 broker配置
…………25
2.6.3操作系统调优
26
27生产环境的注意事项
28
2.7.1垃圾回收器选项
………………28
27.2数据中心布局
27.3共享 Zookeeper…
29
28总结
第3章Kaka生产者向Kaka写入数据…
31
3.1生产者概览
32创建Kaka生产者…
33发送消息到Kaka…
……………4
33.1同步发送消息
35
33.2异步发送消息
……35
34生产者的配置………
…………36
3.5序列化器
39
35.1自定义序列化器
39
3.5.2使用Avro序列化
目录3.5.3在Kaka里使用Avro
3.6分区
45
3.7旧版的生产者API………………
46
3.8总结
………47
第4章Kaka消费者—从Kaka读取数据
……48
4.1 Kafka Consumer概念
4.1.1消费者和消费者群组…
…48
4.1.2消费者群组和分区再均衡
51
4.2创建 Kafka消费者…
…52
4.3订阅主题…
…53
44轮询
53
4.5消费者的配置
4.6提交和偏移量
46.1自动提交
4.6.2提交当前偏移量…
46.3异步提交
4.64同步和异步组合提交
61
4.6.5提交特定的偏移量
4.7再均衡监听器
62
4.8从特定偏移量处开始处理记录
49如何退出
…66
4.10反序列化器
衡,
4.11独立消费者
为什么以及怎样使用没有群组的消费者
71
4.12旧版的消费者API…
4.13总结
第5章深入 Kafka
73
51集群成员关系
73
52控制器…
74
53复制…
…74
54处理请求
541生产请求……
54.2获取请求…………
54.3其他请求
,,
…80
5物理存储…………
81
5.51分区分配
552文件管理
5.5.3文件格式……
83
554索引
目录|ⅶi555清理…
55.6清理的工作原理………
84
5.5.7被删除的事件
…………86
558何时会清理主题
………86
59总结
86
第6章可靠的数据传递
87
6.1可靠性保证
6.2复制…
……88
6.3 broker配置
89
6.3.1复制系数
89
63.2不完全的首领选举
……………90
633最少同步副本……
91
64在可靠的系统里使用生产者
6.41发送确认……
92
64.2配置生产者的重试参数…
64.3额外的错误处理……
…94
65在可靠的系统里使用消费者
94
6.5.1消费者的可靠性配置
…95
6.52显式提交偏移量
…95
66验证系统可靠性…
……97
6.6.1配置验证
66.2应用程序验证
………98
6.6.3在生产环境监控可靠性…
.7总结
第7章构建数据管道
…101
7.1构建数据管道时需要考虑的问题
……102
7.1.1及时性
…102
7.12可靠性
…102
713高吞吐量和动态吞吐量
7.1.4数据格式
7.1.5转换
104
7.1.6安全性
104
7.1.7故障处理能力
7.1.8耦合性和灵活性……
72如何在 Connect API和客户端AP之间作出选择
…105
7.3 Kafka Connect
…106
7.3.1运行 Connect
106
7.3.2连接器示例——文件数据源和文件数据池
………107
目录7.3.3连接器示例—从 MySQL到 ElasticSearch
7.3.4深入理解 Connect………
114
7.4 Connect之外的选择…
116
7.4.1用于其他数据存储的摄入框架
116
74.2基于图形界面的ETL工具
117
7.4.3流式处理框架
,,,,,,来中中,非
117
7.5总结
117
第8章跨集群数据镜像……
118
8.1跨集群镜像的使用场景
…………118
82多集群架构
119
82.1跨数据中心通信的一些现实情况
119
822Hub和 Spoke架构
120
82.3双活架构
121
824主备架构
8,2.5延展集群
127
83 Kafka的 MirrorMaker……
128
83.1如何配置
129
8.3,2在生产环境部署 MirrorMaker
130
8.3.3 MirrorMaker调优
132
84其他跨集群镜像方案
134
841优步的 rEplicator…
……………134
8.4.2 Confluent ig Replicator
…135
8.5总结
…135
第9章管理 Kafka………
……………136
91主题操作
136
91.1创建主题…
137
9.1.2增加分区……
138
9.1.3删除主题
……138
9.1.4列出集群里的所有主题
………139
91.5列出主题详细信息
…139
92消费者群组
……140
92.1列出并描述群组…
92.2删除群组……
…142
92.3偏移量管理
…142
93动态配置变更
143
93.1覆盖主题的默认配置
93.2覆盖客户端的默认配置
145
9.33列出被覆盖的配置
……………145
目录ⅸx9.3.4移除被覆盖的配置
……46
94分区管理……
146
94.1首选的首领选举…………
……………………146
942修改分区副本
147
943修改复制系数
5
944转储日志片段…
94.5副本验证
………152
9.5消费和生产
153
9.5.1控制台消费者
153
952控制台生产者
155
96客户端ACL
157
97不安全的操作………
………………157
97.1移动集群控制器
157
9.7.2取消分区重分配
…157
97.3移除待删除的主题…
158
974手动删除主题
98总结
159
第10章监控 Kafka
160
10.1度量指标基础……………
中,非
160
10.1.1度量指标在哪里
………………………60
10.1.2内部或外部度量……
…………………………………………………………161
10.1.3应用程序健康检测
……161
10.1.4度量指标的覆盖面
………………161
10.2 broker的度量指标
162
10.21非同步分区
162
10.2.2 broker度量指标
166
10.23主题和分区的度量指标
173
10.2.4Java虚拟机监控…
174
10.2.5操作系统监控…
175
10.2.6日志
………176
10.3客户端监控
177
10.3.1生产者度量指标
177
10.32消费者度量指标
…179
0.3.3配额
181
10.4延时监控…………
182
10.5端到端监控
……183
10.6总结
…183
目录第11章流式处理
184
1.1什么是流式处理…
112流式处理的一些概念
112.1时间
187
11.2.2状态………
112.3流和表的二元性
188
11.2.4时间窗口
189
113流式处理的设计模式
………190
113.1单个事件处理…
191
11.3.2使用本地状态……
191
113.3多阶段处理和重分区
193
1134使用外部查找——流和表的连接
11.3.5流与流的连接
195
113.6乱序的事件
……195
11.3.7重新处理……
196
114 Streams示例
……197
114.1字数统计
………197
114.2股票市场统计
……199
11.3填充点击事件流
………201
1.5 Katka streams的架构概览
.,9,非,中,;,,
…………202
11.5.1构建拓扑
202
11.5.2对拓扑进行伸缩
203
1153从故障中存活下来……
…205
116流式处理使用场景…
205
1.7如何选择流式处理框架
206
11.8总结
208
附录A在其他操作系统上安装 Kafka
209
作者介绍
214
封面介绍
目录|xi序
这是一个激动人心的时刻,成千上万的企业在使用Kaka,三分之一多的世界500强公司
也在其中。Kaka是成长最快的开源项目之一,它的生态系统也在蓬勃发展。 Kafka正在成
为管理和处理流式数据的利器。
Kafka从何而来?我们为什么要开发Kaka?Kaka到底是什么?
Kafka最初是 LinkedIn的一个内部基础设施系统。我们发现,虽然有很多数据库和系统可
以用来存储数据,但在我们的架构里,刚好缺一个可以帮助处理持续数据流的组件。在开
发Kaka之前,我们实验了各种现成的解决方案,从消息系统到日志聚合系统,再到ETL
工具,它们都无法满足我们的需求。
最后,我们决定从头开发一个系统。我们不想只是开发一个能够存储数据的系统,比如
传统的关系型数据库、键值存储引擎、搜索引擎或缓存系统,我们希望能够把数据看成
是持续变化和不断增长的流,并基于这样的想法构建出一个数据系统;事实上,是一个
数据架构。
这个想法实现后比我们最初预想的适用性更广。Kaka一开始被用在社交网络的实时应用
和数据流当中,而现在已经成为下一代数据架构的基础。大型零售商正在基于持续数捃流
改造他们的基础业务流程,汽车公司正在从互联网汽车那里收集和处理实时数据流,银行
也在重新思考基于 Kafka改造他们的基础流程和系统。
那么Kaka在这当中充当了怎样的角色?它与现有的系统有什么区别?
我们认为Kaka是一个流平台:在这个平台上可以发布和订阅数据流,并把它们保存起
来、进行处理,这就是构建 Kafka的初衷。以这种方式来看待数据确实与人们习惯的想法
有所不同,但它确实在构建应用和架构方面表现出了强大的抽象能力。 Kafka经常会被拿
来与现有的技术作比较:企业级消息系统、大数据系统(如 Hadoop)和数据集成或ETL
工具。这里的每一项比较都有一定的道理,但也有失偏颇。
Katka有点像消息系统,允许发布和订阅消息流。从这点来看,它类似于 ActiveMQ、
RabbitMQ或IM的 MQSeries等产品。尽管看上去有些相似,但Kaka与这些传统的消
息系统仍然存在很多重要的不同点,这些差异使它完全不同于消息系统。首先,作为个现代的分布式系统,Kaka以集群的方式运行,可以自由伸缩,处理公司的所有应用程
序。Kaka集群并不是一组独立运行的 broker,而是一个可以灵活伸缩的中心平台,可以
处理整个公司所有的数据流。其次, Kafka可以按照你的要求存储数据,保存多久都可以。
作为数据连接层, Kafka提供了数据传递保证—可复制、持久化,保留多长时间完全可
以由你来决定。最后,流式处理将数据处理的层次提升到了新高度。消息系统只会传递
消息,而 Kafka的流式处理能力让你只用很少的代码就能够动态地处理派生流和数据集。
Kaka的这些独到之处足以让你刮目相看,它不只是“另一个消息队列”
从另一个角度来看 Kafka,我们会把它看成实时版的 Hadoop这也是我们设计和构建
Kaka的原始动机之一。 Hadoop可以存储和定期处理大量的数据文件,而Kaka可以存储
和持续处理大型的数据流。从技术角度来看,它们有着惊人的相似之处,很多人将新兴的
流式处理看成批处理的超集。它们之间的最大不同体现在持续的低延迟处理和批处理之间
的差异上。 Hadoop和大数据主要应用在数据分析上,而Kaka因其低延迟的特点更适合用
在核心的业务应用上。业务事件时刻在发生,Kaka能够及时对这些事件作出响应,基于
Kaka构建的服务直接为业务运营提供支撑,提升用户体验。
afka与ETL工具或其他数据集成工具之间也可以进行一番比较。 Kafka和这些工具都擅
长移动数据,但我想它们最大的不同在于Kaka颠覆了传统的思维。Kaka并非只是把数
据从一个系统拆解出来再塞进另一个系统,它其实是一个面向实时数据流的平台。也就是
说,它不仅可以将现有的应用程序和数据系统连接起来,它还能用于加强这些触发相同数
据流的应用。我们认为这种以数据流为中心的架构是非常重要的。在某种程度上说,这些
数据流是现代数字科技公司的核心,与他们的现金流一·样重要。
将上述的三个领域聚合在一起,将所有的数据流整合到一起,流平台因此变得极具吸
引力。
当然,除了这些不同点之外,对于那些习惯了开发请求与响应风格应用和关系型数据库的
人来说,要学会基于持续数据流构建应用程序也着实是一个巨大的思维转变。借助这本书
来学习Kaka再好不过了,从内部架构到API,都是由对Kaka最了解的人亲手呈现的。
我希望你们能够像我一样喜欢这本书!
Jay Kreps, Confluent联合创始人兼CEO
x序前言
给予一个技术书籍作者最好的赞赏莫过于这句话“如果在一开始接触这门技术时能看
到这本书就好了”。在开始写这本书的时候,我们就是以这句话作为写作目标。我们开发
Kafka,在生产环境运行 Kafka,帮助很多公司构建基于Kaka的系统,帮助他们管理数据
管道,积累了很多经验,但也困惑:“应该把哪些东西分享给Kaka新用户,让他们从新
手变成专家?”这本书就是我们日常工作最好的写照:运行 Kafka并帮助其他人更好地使
用 Kafka。
我们相信,书中提供的这些内容能够帮助Kaka用户在生产环境运行Kaka以及基于
Kafka构建健壮的高性能应用程序。我们列举了一些非常流行的应用场景:用于事件驱动
微服务系统的消息总线、流式应用和大规模数据管道。这本书通俗易懂,能够帮助毎一个
Kaka用户在任意的架构或应用场景里使用好Kaka。书中介绍了如何安装和配置Kaka、
如何使用 Kafka APl、Kaka的设计原则和可靠性保证,以及 Kafka的一些架构细节,如复
制协议、控制器和存储层。我们相信, Kafka的设计原理和内部架构不仅会成为分布式系
统构建者的兴趣所在,对于那些在生产环境部署Kaka或使用Kaka构建应用程序的人来
说也是非常有用的。越是了解 Kafka,就越是能够更好地作出权衡
在软件工程里,条条道路通罗马,每一个问题都有多种解决方案。Kaka为专家级别的用
户提供了巨大的灵活性,而新手则需要克服陡峭的学习曲线才能成为专家。 Kafka通常会
告诉你如何使用某个功能特性,但不会告诉你为什么要用它或者为什么不该用它。我们会
尽可能地解释我们的设计决策和权衡背后的缘由,以及用户在哪些情况下应该或不应该使
用 Kafka提供的特性。
读者对象
这本书是为使用 Kafka API开发应用程序的工程师和在生产环境安装、配置、调优、监控
Kaka的运维工程师(也可以叫作SRE、运维人员或系统管理员)而写的。我们也考虑到
了数据架构师和数据工程师,他们负责设计和构建整个组织的数据基础架构。某些章节
(特别是第3章、第4章和第11章)主要面向Java开发人员,并假设读者已经熟悉基本的
Jaⅵa语言编程,比如异常处理和并发编程。其他章节(特别是第2章、第8章、第9章和第10章)则假设读者在 Linux的运行、存储和网络配置方面有一定的经验。本书的其余部
分则讨论了一般性的软件架构,不要求读者具备特定的知识。
另一类可能对本书感兴趣的人是那些经理或架构师,他们不直接使用Kaka,但会与使用
Kaka的工程师打交道。他们有必要了解Kaka所能提供的保证机制,以及他们的同事在
构建基于 Kafka的系统时所作出的权衡。这本书可以成为企业管理人员的利器,确保他们
的工程师在Kaka方面训练有素,让他们的团队了解他们本该知道的知识。
排版约定
本书使用了下列排版约定。
黑体
表示新术语或重点强调的内容。
等宽字体( constant width)
表示程序片段,以及正文中出现的变量、函数名、数据库、数据类型、环境变量、语句
和关键字等。
加粗等宽字体( constant width bold)
表示应该由用户输入的命令或其他文本。
等宽斜体( constant width italic)
表示应该由用户输入的值或根据上下文确定的值替换的文本。
该图标表示提示或建议。
该图标表示一般注记。
该图标表示警告或警示。
使用代码示例
本书是要帮你完成工作的。一般来说,如果本书提供了示例代码,你可以把它用在你的程
序或文档中。除非你使用了很大一部分代码,否则无需联系我们获得许可。比如,用本书
Xⅵi|前言的几个代码片段写一个程序就无需获得许可,销售或分发 O'Reilly图书的示例光盘则需要
获得许可;引用本书中的示例代码回答问题无需获得许可,将书中大量的代码放到你的产
品文档中则需要获得许可。
我们很希望但并不强制要求你在引用本书内容时加上引用说明。引用说明一般包括书名、
作者、出版社和ISBN。例如“Kaka权威指南,作者 Neha Narkhede、 Gwen Shapira和
Todd palin( OReilly),版权归 Neha Narkhede、 Gwen Shapira和 Todd palin所有,978
1-4919-3616-0”
如果你觉得自己对示例代码的用法超出了上述许可的范围,欢迎你通过 permissions@
oreilly.com与我们联系。
O Reilly Safari
SAfari
Safari(原来叫 Safari Books online)是面向企业、政府、教育从
业者和个人的会员制培训和参考咨询平台。
我们向会员开放成千上万本图书以及培训视频、学习路线、交互式教程和专业视频。这
些资源来自250多家出版机构,其中包括 O'Reilly media、 Harvard Business review、
Prentice hall Professional、 Addison- Wesley Professional、 Microsoft press、Sams、Que、
Peachpit Press, Adobe、 Focal Press、 Cisco press、 John Wiley&sons、 Syngress, Morgan
Kaufmann、 IBM Redbooks、 Packt、 Adobe press、 FT Press、 Apress、 Manning、New
Riders、 McGraw-Hill. Jones& Bartlet和 Course Technology。
更多信息,请访问htp:/reilly.com/safari
联系我们
请把对本书的评价和问题发给出版社。
美国:
O Reilly media Inc
1005 Gravenstein Highway North
Sebastopol, CA 95472
中国:
北京市西城区西直门南大街2号成铭大厦C座807室(100035
奥莱利技术咨询(北京)有限公司
OReilly的每一本书都有专属网页,你可以在那儿找到本书的相关信息,包括勘误表、示
例代码以及其他信息。本书的网站地址是htt: reilly/2tVmYjk。
对于本书的评论和技术性问题,请发送电子邮件到:bookquestions@oreilly.com
要了解更多 O'Reilly图书、培训课程、会议和新闻的信息,请访问以下网站
http://www.oreilly.com
前言|xⅶi我们在 Facebook的地址如下:htp: acebook. com/oreilly
请关注我们的Twitter动态:htp:twitter.com/yoreillymedia
我们的YouTube视频地址如下:htp:/vww.youtube.com/oreillymedia
致谢
我们想感谢众多为Kaka和它的生态系统做出贡献的人。如果没有他们艰辛的工作,就不
会有这本书的问世。特别感谢 Jay Kreps、 Neha Narkhede和 Jun rao,以及他们在 LinkedIn
的同事和领导,他们创造了 Kafka,并把它捐献给了 Apache软件基金会。
很多人在早前为本书提供了很多有价值的反馈,我们非常感激他们为此付出的时间,也
很钦佩他们的专业能力,这些人包括: Apurva Mehta、 Arseniy Tashoyan、 Dylan Scott
Ewen Cheslack- Postava、 Grant henke、 Ismael Juma、 James Cheng、 Jason Gustafson、Jeff
Holloman、 Joel Koshy、 Jonathan Seidman、 Matthias sax、 Michael noll, Paolo castagna。我
们还想感谢众多在网站上留下评论和反馈的读者。
很多审稿人提供了有价值的意见,极大改进了本书的质量。书中的遗留错误理应由我们作
者负责。
我们要感谢 O'Reilly编辑 Shannon cutt的鼓励、耐心和深谋远虑。对于一个作者来说,与
Oˇ Reilly一起合作是一段非凡的经历——他们所提供的支持,从工具到签名售书,都是无
可匹敌的。我们感谢每一个参与本书相关工作的人,很感激他们愿意与我们一起工作
另外,我们也想感谢我们的领导和同事,感谢他们在我们写作这本书的过程中给予的帮助
和鼓励。
Gwen要感谢她的丈夫 Omer Shapira,在她写书的几个月时间里,他一直给予她支持和耐
心。还有她的父亲 Lior Shapira,让她学会了如何在困难面前不轻言放弃,尽管这种生活哲
学总是让她麻烦不断。
Tod要感谢他的妻子 Marcy和女儿Bell及Kaye,她们一直在背后默默地支持他。因为
有了她们的支持,他才有更多的时间写作,才能厘清思路,坚持到最后。
电子书
扫描如下二维码,即可购买本书电子版。
xvⅷi前言第1章
初识 Kafka
数据为企业的发展提供动力。我们从数据中获取信息,对它们进行分析处理,然后生成更
多的数据。每个应用程序都会产生数据,包括日志消息、度量指标、用户活动记录、响应
消息等。数据的点点滴滴都在暗示一些重要的事情,比如下一步行动的方向。我们把数据
从源头移动到可以对它们进行分析处理的地方,然后把得到的结果应用到实际场景中,这
样才能够确切地知道这些数据要告诉我们什么。例如,我们每天在 Amazon网站上浏览感
兴趣的商品,浏览信息被转化成商品推荐,并在稍后展示给我们。
这个过程完成得越快,组织的反应就越敏捷。花费越少的精力在数据移动上,就越能专注
于核心业务。这就是为什么在一个以数据为驱动的企业里,数据管道会成为关键性组件。
如何移动数据,几乎变得与数据本身一样重要。
每一次科学家们发生分歧,都是因为掌握的数据不够充分。所以我们可以先就获
取哪一类数据达成一致。只要获取了数据,问题也就迎刃而解了。要么我是对
的,要么你是对的,要么我们都是错的。然后我们继续研究。
Neil de Grasse Tyson
1.1发布与订阅消息系统
在正式讨论 Apache Kafka(以下简称Kaka)之前,先来了解发布与订阅消息系统的概念
并认识这个系统的重要性。数据(消息)的发送者(发布者)不会直接把消息发送给接收
者,这是发布与订阅消息系统的一个特点。发布者以某种方式对消息进行分类,接收者
(订阅者)订阅它们,以便接收特定类型的消息。发布与订阅系统一般会有一个 broker,也
就是发布消息的中心点。1.1.1如何开始
发布与订阅消息系统的大部分应用场景都是从一个简单的消息队列或一个进程间通道开始
的。例如,你的应用程序需要往别处发送监控信息,可以直接在你的应用程序和另一个可
以在仪表盘上显示度量指标的应用程序之间建立连接,然后通过这个连接推送度量指标
如图1-1所示
前端
服务器
服务器
应用程序
应用程序
度量指标
度量指标
度量指标
服务器
图1-1:单个直连的度量指标发布者
这是刚接触监控系统时简单问题的应对方案。过了不久,你需要分析更长时间片段的度量
指标,而此时的仪表盘程序满足不了需求,于是,你启动了一个新的服务来接收度量指
标。该服务把度量指标保存起来,然后进行分析。与此同时,你修改了原来的应用程序,
把度量指标同时发送到两个仪表盘系统上。现在,你又多了3个可以生成度量指标的应用
程序,它们都与这两个服务直接相连。而你的同事认为最好可以对这些服务进行轮询以便
获得告警功能,于是你为毎一个应用程序增加了一个服务器,用于提供度量指标。再过一
阵子,有更多的应用程序出于各自的目的,都从这些服务器获取度量指标。这时的架构看
起来就像图1-2所示的那样,节点间的连接一团糟。
端|数据库数据库1聊天
后端
服务器服务器服务器副本
服务器购物车服务器
度量
数据库
用户界面
指标分析
活动监控
监控器
图1-2:多个直连的度量指标发布者
这时,技术债务开始凸显出来,于是你决定偿还掉一些。你创建了一个独立的应用程序,
用于接收来自其他应用程序的度量指标,并为其他系统提供了一个查询服务器。这样,之
前架构的复杂度被降低到图1-3所示的那样。那么恭喜你,你已经创建了一个基于发布与
订阅的消息系统。
2|第1章前端
数据库数据库
聊天
后端
服务器服务器服务器副本服务器
购物车服务器
发布与订阅
度量指标
度量
指标分析活动监控
数据库
用户界面
监控器
图13:度量指标发布与订阅系统
1.1.2独立的队列系统
在你跟度量指标打得不可开交的时候,你的一个同事也正在跟日志消息奋战。还有另一个
同事正在跟踪网站用户的行为,为负责机器学习开发的同事提供信息,同时为管理团队生
成报告。你和同事们使用相同的方式创建这些系统,解耦信息的发布者和订阅者。图1-4
所示的架构包含了3个独立的发布与订阅系统。
前端前端数据库数据库聊天
后端
服务器服务器服务器
副本
服务器购物车服务器
发布与订阅
发布与
发布与
度量指标
订阅日志
订阅跟踪
用户界面指标分析活动监控数据库日志搜索安全分析离线处理
度量
监控器
图14:多个发布与订阅系统
这种方式比直接使用点对点的连接(图1-2)要好得多,但这里有太多重复的地方。你的
公司因此要为数据队列维护多个系统,每个系统又有各自的缺陷和不足。而且,接下来可
能会有更多的场景需要用到消息系统。此时,你真正需要的是一个单一的集中式系统,它
可以用来发布通用类型的数据,其规模可以随着公司业务的增长而增长
初识Kaka|31.2 Kafka登场
Kaka就是为了解决上述问题而设计的一款基于发布与订阅的消息系统。它一般被称为
“分布式提交日志”或者“分布式流平台”。文件系统或数据库提交日志用来提供所有事务
的持久记录,通过重放这些日志可以重建系统的状态。同样地, Kafka的数据是按照一定
顺序持久化保存的,可以按需读取。此外, Kafka的数据分布在整个系统里,具备数据故
障保护和性能伸缩能力
1.2.1消息和批次
Kafka的数据单元被称为消息。如果你在使用Kaka之前已经有数据库使用经验,那么可
以把消息看成是数据库里的一个“数据行”或一条“记录”。消息由字节数组组成,所以
对于Kaka来说,消息里的数据没有特别的格式或含义。消息可以有一个可选的元数据,
也就是键。键也是一个字节数组,与消息一样,对于 Kafka来说也没有特殊的含义。当消
息以一种可控的方式写入不同的分区时,会用到键。最简单的例子就是为键生成一个一致
性散列值,然后使用散列值对主题分区数进行取模,为消息选取分区。这样可以保证具有
相同键的消息总是被写到相同的分区上。第3章将详细介绍键的用法。
为了提高效率,消息被分批次写入Kaka。批次就是一组消息,这些消息属于同一个主题
和分区。如果每一个消息都单独穿行于网络,会导致大量的网络开销,把消息分成批次传
输可以减少网络开销。不过,这要在时间延迟和吞吐量之间作出权衡:批次越大,单位时
间内处理的消息就越多,单个消息的传输时间就越长。批次数据会被压缩,这样可以提升
数据的传输和存储能力,但要做更多的计算处理。
1.2.2模式
对于Kaka来说,消息不过是晦涩难懂的字节数组,所以有人建议用一些额外的结构来
定义消息内容,让它们更易于理解。根据应用程序的需求,消息模式( schema)有许多
可用的选项。像JSON和XML这些简单的系统,不仅易用,而且可读性好。不过,它们
缺乏强类型处理能力,不同版本之间的兼容性也不是很好。 Kafka的许多开发者喜欢使用
pache avro,它最初是为 Hadoop开发的一款序列化框架。Avro提供了一种紧凑的序列化
格式,模式和消息体是分开的,当模式发生变化时,不需要重新生成代码;它还支持强类
型和模式进化,其版本既向前兼容,也向后兼容。
数据格式的一致性对于 Kafka来说很重要,它消除了消息读写操作之间的耦合性。如果读
写操作紧密地耦合在一起,消息订阅者需要升级应用程序才能同时处理新旧两种数据格
式。在消息订阅者升级了之后,消息发布者才能跟着升级,以便使用新的数据格式。新的
应用程序如果需要使用数据,就要与消息发布者发生耦合,导致开发者需要做很多繁杂
的工作。定义良好的模式,并把它们存放在公共仓库,可以方便我们理解Kaka的消息结
构。第3章将详细讨论模式和序列化。
4|第1章1.2.3主题和分区
Kaka的消息通过主题进行分类。主题就好比数据库的表,或者文件系统里的文件夹。主
题可以被分为若干个分区,一个分区就是一个提交日志。消息以追加的方式写入分区,然
后以先入先出的顺序读取。要注意,由于一个主题一般包含几个分区,因此无法在整个主
题范围内保证消息的顺序,但可以保证消息在单个分区内的顺序。图1-5所示的主题有4
个分区,消息被追加写入毎个分区的尾部。 Kafka通过分区来实现数据冗余和伸缩性。分
区可以分布在不同的服务器上,也就是说,一个主题可以横跨多个服务器,以此来提供比
单个服务器更强大的性能。
主题“ opicName
分区0
消息写入
分区20
1234516789101m
图15:包含多个分区的主题表示
我们通常会使用流这个词来描述Kaka这类系统的数据。很多时候,人们把一个主题的数
据看成一个流,不管它有多少个分区。流是一组从生产者移动到消费者的数据。当我们讨
论流式处理时,一般都是这样描述消息的。 Kafka streams、 Apache Samza和Stom这些框
架以实时的方式处理消息,也就是所谓的流式处理。我们可以将流式处理与离线处理进行
比较,比如 Hadoop就是被设计用于在稍后某个时刻处理大量的数据。第11章将会介绍流
式处理。
1.2.4生产者和消费者
Kafka的客户端就是Kaka系统的用户,它们被分为两种基本类型:生产者和消费者。除
此之外,还有其他高级客户端APⅠ用于数据集成的 Kafka Connect Apl和用于流式处理
的 Kafka streams。这些高级客户端API使用生产者和消费者作为内部组件,提供了高级的
功能。
生产者创建消息。在其他发布与订阅系统中,生产者可能被称为发布者或写入者。一般情
况下,一个消息会被发布到一个特定的主题上。生产者在默认情况下把消息均衡地分布到
主题的所有分区上,而并不关心特定消息会被写到哪个分区。不过,在某些情况下,生产
者会把消息直接写到指定的分区。这通常是通过消息键和分区器来实现的,分区器为键生
成一个散列值,并将其映射到指定的分区上。这样可以保证包含同一个键的消息会被写到
同一个分区上。生产者也可以使用自定义的分区器,根据不同的业务规则将消息映射到分
区。第3章将详细介绍生产者。
初识 Kafka|5消费者读取消息。在其他发布与订阅系统中,消费者可能被称为订阅者或读者。消费者订
阅一个或多个主题,并按照消息生成的顺序读取它们。消费者通过检査消息的偏移量来区
分已经读取过的消息。偏移量是另一种元数据,它是一个不断递增的整数值,在创建消息
时,Kaka会把它添加到消息里。在给定的分区里,每个消息的偏移量都是唯一的。消费
者把每个分区最后读取的消息偏移量保存在 Zookeeper或 Kafka上,如果消费者关闭或重
启,它的读取状态不会丢失。
消费者是消费者群组的一部分,也就是说,会有一个或多个消费者共同读取一个主题。群
组保证毎个分区只能被一个消费者使用。图1-6所示的群组中,有3个消费者同时读取一
个主题。其中的两个消费者各自读取一个分区,另外一个消费者读取其他两个分区。消费
者与分区之间的映射通常被称为消费者对分区的所有权关系。
通过这种方式,消费者可以消费包含大量消息的主题。而且,如果一个消费者失效,群组
里的其他消费者可以接管失效消费者的工作。第4章将详细介绍消费者和消费者群组。
主题“ topicName”
消费者
分区0
回回
洋组
消费者0
o23456I
消费者
分区2
D12345678910
消费者2
区[21345|6819m1节
图1-6:消费者群组从主题读取消息
1.2.5 broker和集群
个独立的 Kafka服务器被称为 broker。 broker接收来自生产者的消息,为消息设置偏移
量,并提交消息到磁盘保存。 broker为消费者提供服务,对读取分区的请求作出响应,返
回已经提交到磁盘上的消息。根据特定的硬件及其性能特征,单个 broker可以轻松处理数
千个分区以及每秒百万级的消息量。
broker是集群的组成部分。每个集群都有一个 broker同时充当了集群控制器的角色(自动
从集群的活跃成员中选举岀来)。控制器负责管理工作,包括将分区分配给 broker和监控
broker。在集群中,一个分区从属于一个 broker,该 broker被称为分区的首领。一个分区
可以分配给多个 broker,这个时候会发生分区复制(见图1-7)。这种复制机制为分区提供
了消息冗余,如果有一个 broker失效,其他 broker可以接管领导权。不过,相关的消费者
和生产者都要重新连接到新的首领。第6章将详细介绍集群的操作,包括分区复制。
第1章Ka1a集群
Broker
主题A
主题A
的分区0
的分区1
消息A/O
首领
来自A0的消息
生产者
复制A/0
复制A
消费者
消息A/I
Broker
来自A1的消息
主题A
主题A
的分区0的分区1
图1-7:集群里的分区复制
保留消息(在一定期限内)是 Kafka的一个重要特性。 Kafka broker默认的消息保留策略
是这样的:要么保留一段时间(比如7天),要么保留到消息达到一定大小的字节数(比
如lGB)。当消息数量达到这些上限时,旧消息就会过期并被删除,所以在任何时刻,可
用消息的总量都不会超过配置参数所指定的大小。主题可以配置自己的保留策略,可以将
消息保留到不再使用它们为止。例如,用于跟踪用户活动的数据可能需要保留几天,而应
用程序的度量指标可能只需要保留几个小时。可以通过配置把主题当作紧凑型日志,只有
最后一个带有特定键的消息会被保留下来。这种情况对于变更日志类型的数据来说比较适
用,因为人们只关心最后时刻发生的那个变更。
1.2.6多集群
随着 Kafka部署数量的增加,基于以下几点原因,最好使用多个集群。
数据类型分离
安全需求隔离
多数据中心(灾难恢复)
如果使用多个数据中心,就需要在它们之间复制消息。这样,在线应用程序才可以访问到
多个站点的用户活动信息。例如,如果一个用户修改了他们的资料信息,不管从哪个数据
中心都应该能看到这些改动。或者多个站点的监控数据可以被聚集到一个部署了分析程序
和告警系统的中心位置。不过,Kaka的消息复制机制只能在单个集群里进行,不能在多
个集群之间进行。
Kafka提供了一个叫作 MirrorMaker的工具,可以用它来实现集群间的消息复制。
MirrorMaker的核心组件包含了一个生产者和一个消费者,两者之间通过一个队列相连。
初识 Kafka|7消费者从一个集群读取消息,生产者把消息发送到另一个集群上。图1-8展示了一个使
用 MirrorMaker的例子,两个“本地”集群的消息被聚集到一个“聚合”集群上,然后将
该集群复制到其他数据中心。不过,这种方式在创建复杂的数据管道方面显得有点力不从
心。第7章将详细讨论这些案例。
数据中心A
数据中心B
数据中心C
生产者(消费者
生产者】(消费者
消费者】(消费者
Kaka本地集群
Kaka本地集群
费
消费
Kafka聚合集群
MirrorMaker
MirrorMaker
「生产
「生产
生产
<ana聚合集群
Kaa聚合集群逍费
MirrorMaker
图1-8:多数据中心架构
1.3为什么选择Kaka
基于发布与订阅的消息系统那么多,为什么Kaka会是一个更好的选择呢?
1.3.1多个生产者
Kaka可以无缝地支持多个生产者,不管客户端在使用单个主题还是多个主题。所以它很
适合用来从多个前端系统收集数据,并以统一的格式对外提供数据。例如,一个包含了多
个微服务的网站,可以为页面视图创建一个单独的主题,所有服务都以相同的消息格式向
该主题写入数据。消费者应用程序会获得统一的页面视图,而无需协调来自不同生产者的
数据流。
1.32多个消费者
除了支持多个生产者外, Kafka也支持多个消费者从一个单独的消息流上读取数据,而且
消费者之间互不影响。这与其他队列系统不同,其他队列系统的消息一旦被一个客户端读
取,其他客户端就无法再读取它。另外,多个消费者可以组成一个群组,它们共享一个消
8|第1章息流,并保证整个群组对每个给定的消息只处理一次。
1.3.3基于磁盘的数据存储
Kafka不仅支持多个消费者,还允许消费者非实时地读取消息,这要归功于 Kafka的数据
保留特性。消息被提交到磁盘,根据设置的保留规则进行保存。毎个主题可以设置单独的
保留规则,以便满足不同消费者的需求,各个主题可以保留不同数量的消息。消费者可能
会因为处理速度慢或突发的流量高峰导致无法及时读取消息,而持久化数据可以保证数据
不会丢失。消费者可以在进行应用程序维护时离线一小段时间,而无需担心消息丢失或堵
塞在生产者端。消费者可以被关闭,但消息会继续保留在Kaka里。消费者可以从上次中
断的地方继续处理消息。
1.34伸缩性
为了能够轻松处理大量数据, Kafka从一开始就被设计成一个具有灵活伸缩性的系统。用
户在开发阶段可以先使用单个 broker,再扩展到包含3个 broker的小型开发集群,然后随
着数据量不断增长,部署到生产环境的集群可能包含上百个 broker。对在线集群进行扩展
丝毫不影响整体系统的可用性。也就是说,一个包含多个 broker的集群,即使个别 broker
失效,仍然可以持续地为客户提供服务。要提髙集群的容错能力,需要配置较高的复制系
数。第6章将讨论关于复制的更多细节。
1.3.5高性能
上面提到的所有特性,让Kaka成为了一个高性能的发布与订阅消息系统。通过横向扩展
生产者、消费者和 broker, Kafka可以轻松处理巨大的消息流。在处理大量数据的同时,
它还能保证亚秒级的消息延迟。
1.4数据生态系统
已经有很多应用程序加入到了数据处理的大军中。我们定义了输入和应用程序,负责生成
数据或者把数据引入系统。我们定义了输出,它们可以是度量指标、报告或者其他类型的
数据。我们创建了一些循环,使用一些组件从系统读取数据,对读取的数据进行处理,然
后把它们导到数据基础设施上,以备不时之需。数据类型可以多种多样,每一种数据类型
可以有不同的内容、大小和用途。
Kaka为数据生态系统带来了循环系统,如图1-9所示。它在基础设施的各个组件之间传
递消息,为所有客户端提供一致的接口。当与提供消息模式的系统集成时,生产者和消费
者之间不再有紧密的耦合,也不需要在它们之间建立任何类型的直连。我们可以根据业务
需要添加或移除组件,因为生产者不再关心谁在使用数据,也不关心有多少个消费者
初识 Kafka9在线
应用程序
流处理
离线处理
Apache Solr,
Hadoop
OpenTSDB
Storm, Flink
Kafka
指标
日志
交易数据
物联网
数据
图1-9:大数据生态系统
使用场景
1.活动跟踪
Kaka最初的使用场景是跟踪用户的活动。网站用户与前端应用程序发生交互,前端应用
程序生成用户活动相关的消息。这些消息可以是一些静态的信息,比如页面访问次数和点
击量,也可以是一些复杂的操作,比如添加用户资料。这些消息被发布到一个或多个主题
上,由后端应用程序负责读取。这样,我们就可以生成报告,为机器学习系统提供数据,
更新搜索结果,或者实现其他更多的功能。
2.传递消息
Kafka的另一个基本用途是传递消息。应用程序向用户发送通知(比如邮件)就是通过传
递消息来实现的。这些应用程序组件可以生成消息,而不需要关心消息的格式,也不需要
关心消息是如何被发送的。一个公共应用程序会读取这些消息,对它们进行处理:
格式化消息(也就是所谓的装饰);
将多个消息放在同一个通知里发送;
根据用户配置的首选项来发送数据
使用公共组件的好处在于,不需要在多个应用程序上开发重复的功能,而且可以在公共组
件上做一些有趣的转换,比如把多个消息聚合成一个单独的通知,而这些工作是无法在其
他地方完成的。
3.度量指标和日志记录
Kafka也可以用于收集应用程序和系统度量指标以及日志。Kaka支持多个生产者的特性在
这个时候就可以派上用场。应用程序定期把度量指标发布到 Kafka主题上,监控系统或告
警系统读取这些消息。Kaka也可以用在像 Hadoop这样的离线系统上,进行较长时间片段
10
第1章的数据分析,比如年度增长走势预测。日志消息也可以被发布到 Kafka主题上,然后被路
由到专门的日志搜索系统(比如 Elasticsearch)或安全分析应用程序。更改目标系统(比
如日志存储系统)不会影响到前端应用或聚合方法,这是Kaka的另一个优点。
4.提交日志
Kafka的基本概念来源于提交日志,所以使用Kaka作为提交日志是件顺理成章的事。我
们可以把数据库的更新发布到Kaka上,应用程序通过监控事件流来接收数据库的实时更
新。这种变更日志流也可以用于把数据库的更新复制到远程系统上,或者合并多个应用程
序的更新到一个单独的数据库视图上。数据持久化为变更日志提供了缓冲区,也就是说
如果消费者应用程序发生故障,可以通过重放这些日志来恢复系统状态。另外,紧凑型日
志主题只为每个键保留一个变更数据,所以可以长时间使用,不需要担心消息过期问题。
5.流处理
流处理是又一个能提供多种类型应用程序的领域。可以说,它们提供的功能与 Hadoop里
的map和 I reduce有点类似,只不过它们操作的是实时数据流,而 Hadoop则处理更长时间
片段的数据,可能是几个小时或者几天, Hadoop会对这些数据进行批处理。通过使用流
式处理框架,用户可以编写小型应用程序来操作Kaka消息,比如计算度量指标,为其他
应用程序有效地处理消息分区,或者对来自多个数据源的消息进行转换。第11章将通过
其他案例介绍流处理。
1.5起源故事
Kafka是为了解决 LinkedIn数据管道问题应运而生的。它的设计目的是提供一个高性能的
消息系统,可以处理多种数据类型,并能够实时提供纯净且结构化的用户活动数据和系统
度量指标。
数据为我们所做的每一件事提供了动力。
-Jeff Weiner, LinkedIn CEO
1.5.1 LinkedIn的问题
本章开头提到过, LinkedIn有一个数据收集系统和应用程序指标,它使用自定义的收集器
和一些开源工具来保存和展示内部数据。除了跟踪CPU使用率和应用性能这些一般性指
标外, LinkedIn还有一个比较复杂的用户请求跟踪功能。它使用了监控系统,可以跟踪单
个用户的请求是如何在内部应用间传播的。不过监控系统存在很多不足。它使用的是轮询
拉取度量指标的方式,指标之间的时间间隔较长,而且没有自助服务能力。它使用起来不
太方便,很多简单的任务需要人工介入才能完成,而且一致性较差,同一个度量指标的名
字在不同系统里的叫法不一样。
与此同时,我们还创建了另一个用于收集用户活动信息的系统。这是一个HTTP服务,前
端的服务器会定期连接进来,在上面发布一些消息(XML格式)。这些消息文件被转移到
线下进行解析和校对。同样,这个系统也存在很多不足。XML文件的格式无法保持一致,
初识 Kafka|1而且解析XML文件非常耗费计算资源。要想更改所创建的活动类型,需要在前端应用和
离线处理程序之间做大量的协调工作。即使是这样,在更改数据结构时,仍然经常出现系
统崩溃现象。而且批处理时间以小时计算,无法用它完成实时的任务。
监控和用户活动跟踪无法使用同一个后端服务。监控服务太过笨重,数据格式不适用于活
动跟踪,而且无法在活动跟踪中使用轮询拉取模型。另一方面,把跟踪服务用在度量指标
上也过于脆弱,批处理模型不适用于实时的监控和告警。不过,好在数据间存在很多共
性,信息(比如特定类型的用户活动对应用程序性能的影响)之间的关联度还是很高的。
特定类型用户活动数量的下降说明相关应用程序存在问题,不过批处理的长时间延迟意味
着无法对这类问题作出及时的反馈。
最开始,我们调硏了一些现成的开源解决方案,希望能够找到一个系统,可以实时访问
数据,并通过横向扩展来处理大量的消息。我们使用 ActiveMQ创建了一个原型系统,但
它当时还无法满足横向扩展的需求。 LinkedIn不得不使用这种脆弱的解决方案,虽然
ActiveMQ有很多缺陷会导致 broker暂停服务。客户端的连接因此被阻塞,处理用户请求
的能力也受到影响。于是我们最后决定构建自己的基础设施。
1.52 Kafka的诞生
LinkedIn的开发团队由 Jay Kreps领导。 Jay Kreps是 LinkedIn的首席工程师,之前负责分
布式键值存储系统 Voldemort的开发。初建团队成员还包括 Neha Narkhede,不久之后,
Jun rao也加入了进来。他们一起着手创建一个消息系统,可以同时满足上述的两种需求
并且可以在未来进行横向扩展。他们的主要目标如下
使用推送和拉取模型解耦生产者和消费者;
为消息传递系统中的消息提供数据持久化,以便支持多个消费者;
通过系统优化实现高吞吐量;
·系统可以随着数据流的增长进行横向扩展。
最后我们看到的这个发布与订阅消息系统具有典型的消息系统接口,但从存储层来看,它
更像是一个日志聚合系统。 Kafka使用Avro作为消息序列化框架,每天高效地处理数十亿
级别的度量指标和用户活动跟踪信息。 LinkedIn已经拥有超过万亿级别的消息使用量(截
止到2015年8月),而且每天仍然需要处理超过千万亿字节的数据。
1.53走向开源
2010年底, Kafka作为开源项目在 GitHub上发布。201l年7月,因为倍受开源社区的关
注,它成为 Apache软件基金会的孵化器项目。2012年10月, Kafka从孵化器项目毕业。
从那时起,来自 LinkedIn内部的开发团队一直为 Kafka提供大力支持,而且吸引了大批
来自 Linked外部的贡献者和参与者。现在, Kafka被很多组织用在一些大型的数据管道
上。2014年秋天, Jay Kreps、 Neha Narkhede和JRao离开 LinkedIn,创办了 Confluent
Confluent是一个致力于为企业开发提供支持、为Kaka提供培训的公司。这两家公司连同
来自开源社区持续增长的贡献力量,一直在开发和维护 Kafka,让 Kafka成为大数据管道
的不二之选。
12|第1章1.54命名
关于 Kafka的历史,人们经常会问到的一个问题就是, Katka这个名字是怎么想出来的,
以及这个名字和这个项目之间有着怎样的联系。对于这个问题, Jay Kreps解释如下:
我想既然Kaka是为了写数据而产生的,那么用作家的名字来命名会显得更有意
义。我在大学时期上过很多文学课程,很喜欢 Franz Katka。况且,对于开源项目
来说,这个名字听起来很酷。因此,名字和应用本身基本没有太多联系
1.6开始 Kafka之旅
现在我们对Kaka已经有了一个大体的了解,还知道了一些常见的术语,接下来可以开始
使用 Kafka来创建数据管道了。在下一章,我们将探究如何安装和配置 Kafka,还会讨论
如何选择合适的硬件来运行Kaka,以及把Kaka应用到生产环境需要注意的事项
初识 Kafka13第2章
安装 Kafka
这一章将介绍如何安装和运行Kaka,包括如何设置 Zookeeper( Kafka使用 Zookeeper保
存 Broker的元数据),还会介绍Kaka的基本配置,以及如何为Kaka选择合适的硬件,
最后介绍如何在一个集群中安装多个 Kafka broker,以及把 Kafka应用到生产环境需要注
意的事项。
2.1要事先行
在使用 Katka之前需要先做一些事情,接下来介绍怎样做。
2.1.1选择操作系统
Kaka是使用Java开发的应用程序,所以它可以运行在 Windows、 MacoS和 Linux等多
种操作系统上。本章将着重介绍如何在 Linux上安装和使用 Katka,因为把Kaka安装在
Linux系统上是最为常见的。即使只是把Kaka作为一般性用途,仍然推荐使用 Linux系
统。关于如何在 Windows和 MacOS上安装Kaka,请参考附录A
2.1.2安装Java
在安装 Zookeeper和 Kafka之前,需要先安装Java环境。这里推荐安装Java8,可以使用
系统自带的安装包,也可以直接从 Java. com网站下载。虽然运行 Zookeeper和Kaka只需
要Java运行时版本,但也可以安装完整的JDK,以备不时之需。假设JDK8 update51已
经安装在 sr/java/jdk1.8.051目录下,其他软件的安装都是基于这个前提进行的。
14213安装 Zookeeper
Kaka使用 Zookeeper保存集群的元数据信息和消费者信息。 Kafka发行版自带了
Zookeeper,可以直接从脚本启动,不过安装一个完整版的 Zookeeper也并不费劲。
生产者
Kafka Broker
消贵者
broker和主题
消费者元数据
元数据
分区偏移量
Zookeeper
图2-1: Kafka和 Zookeeper
Zookeeper的3.4.6稳定版已经在Kaka上做过全面测试,可以从 apache.org下载该版本的
Zookeeperhttp://bit.ly/2sdwsgj
1.单机服务
下面的例子演示了如何使用基本的配置安装 Zookeeper,安装目录为/ usr/local/ zookeeper,
数据目录为/ar/ lib/zookeeper
tar -zxf zookeeper-3.4.6. tar. gz
mv zookeeper-3.4.6 /usr/local/zookeeper
mkdir -p/ var/lib/zookeeper
cat >/usr/local/zookeeper/conf/zoo. cfg < EOF
tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
EOF
export JAVA_HOME=/usr/java/jdk180-51
#/usr/local/zookeeper/bin/zkServer sh start
JMX enabled by default
Using config: / usr/local/zookeeper /bin/./conf/zoo. cfg
Starting zookeeper
STARTED
现在可以连到 Zookeeper端口上,通过发送四字命令srvr来验证 Zookeeper是否安装正确。
telnet loca lhost 2181
Trying 1
Connected to loca lhost
Escape character is Al
srvr
Zookeeper version: 3. 4.6-1569965, built on 02 /20/201409: 09 GMT
Latency min/avg/max: 0/0/0
Received: 1
Sent: 0
安装 Kafka|15Connections: 1
Outstanding: o
Zxid: OxO
Mode: standalone
ode count: 4
Connection closed by foreign host
#
2. Zookeeper群组( Ensemble)
Zookeeper集群被称为群组。 Zookeeper使用的是一致性协议,所以建议每个群组里应该包
含奇数个节点(比如3个、5个等),因为只有当群组里的大多数节点(也就是法定人数)
处于可用状态, Zookeeper才能处理外部的请求。也就是说,如果你有一个包含3个节点
的群组,那么它允许一个节点失效。如果群组包含5个节点,那么它允许2个节点失效。
群组节点个数的选择
假设有一个包含5个节点的群组,如果要对群组做一些包括更换节点在内的
配置更改,需要依次重启每一个节点。如果你的群组无法容忍多个节点失
效,那么在进行群组维护时就会存在风险。不过,也不建议一个群组包含超
过7个节点,因为 Zookeeper使用了一致性协议,节点过多会降低整个群组
的性能。
群组需要有一些公共配置,上面列出了所有服务器的清单,并且每个服务器还要在数据
目录中创建一个myid文件,用于指明自己的ID。如果群组里服务器的机器名是zoo1
example.com、z002,example.com、z003.example.com,那么配置文件可能是这样的:
tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
initLimit=20
syncLimit=5
server 1=zoo1. example. com: 2888: 3888
server 2=z002. example. com: 2888: 3888
server
nple.com:2888:3888
在这个配置中, intimi表示用于在从节点与主节点之间建立初始化连接的时间上限,
syncLimit表示允许从节点与主节点处于不同步状态的时间上限。这两个值都是 tickTime的
倍数,所以 initLimit是20*2000ms,也就是40s。配置里还列出了群组中所有服务器的地
址。服务器地址遵循 server,X- hostname: peer Port: Leader Port格式,各个参数说明如下:
服务器的ID,它必须是一个整数,不过不一定要从0开始,也不要求是连续的;
hostname
服务器的机器名或IP地址;
pierpont
用于节点间通信的TCP端口;
16第2章leaderPort
用于首领选举的TCP端口。
客户端只需要通过 client Port就能连接到群组,而群组节点间的通信则需要同时用到这3个
端口( peer Port、 leader Port、 client Port)。
除了公共的配置文件外,每个服务器都必须在 data dir目录中创建一个叫作myid的文件,
文件里要包含服务器ID,这个ID要与配置文件里配置的DD保持一致。完成这些步骤后,
就可以启动服务器,让它们彼此间进行通信了。
22安装 Kafka broker
配置好Java和 Zookeeper之后,接下来就可以安装Kaka了。可以从htp:/ kafka. apache
org/ Downloads. html下载最新版本的Kaka。截至本书写作时,Kaka的版本是090.1,对
应的 Scala版本是2.10。
下面的例子将 Kafka安裝在/ asr/local/kaka目录下,使用之前配置好的 Zookeeper,并把消
息日志保存在/ mp/kafka-logs目录下
tar -zxf kafka 2. 11-0.9.0.1 tgz
my kafka.11-0.9.0.1 /usr/local/ kafka
mkdir /tmp/ kafka-logs
export JAVA_ HOME=/usr /java/jdk18051
#/usr/local/kafka/bin/kafka-server-start sh -daemon
/usr/local/kafka/config/server properties
#
旦 Kafka创建完毕,就可以对这个集群做一些简单的操作来验证它是否安装正确,比如
创建一个测试主题,发布一些消息,然后读取它们。
创建并验证主题:
#/usr/local/kafka/bin/kafka-topics sh --create--zookeeper loca thost: 2181
replication-factor 1--partitions 1--topic test
Created topic"test
#/usr/local/kafka/bin/kafka-topics sh --zookeeper loca lhost: 2181
describe -topic test
Topic: test Partition Count: 1 Replication Factor 1 Config
Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0
往测试主题上发布消息:
#/usr/local/kafka/bin/ kafka-console-producer sh --broker-list
localhost: 9092 --topic test
Test Message 1
Test Message 2
从测试主题上读取消息:
安装 Kafka|17#/usr/local/kafka/bin/kafka-console- consumer. sh--zookeeper
loca lhost: 2181 -topic test --from-beginning
Test message 1
Test Message 2
Consumed 2 messages
2.3 broker配置
Kafka发行包里自带的配置样本可以用来安装单机服务,但并不能满足大多数安装场景的
要求。 Kafka有很多配置选项,涉及安装和调优的方方面面。不过大多数调优选项可以使
用默认配置,除非你对调优有特别的要求。
2.3.1常规配置
有一些配置选项,在单机安装时可以直接使用默认值,但在部署到其他环境时要格外小
这些参数是单个服务器最基本的配置,它们中的大部分需要经过修改后才能用在集
群里。
1 broker. id
每个 broker都需要有一个标识符,使用 broker. id来表示。它的默认值是0,也可以被设置
成其他任意整数。这个值在整个 Kafka集群里必须是唯一的。这个值可以任意选定,如果
出于维护的需要,可以在服务器节点间交换使用这些ID。建议把它们设置成与机器名具有
相关性的整数,这样在进行维护时,将ID号映射到机器名就没那么麻烦了。例如,如果
机器名包含唯一性的数字(比如hostl.example.com、host2example.com),那么用这些数字
来设置 broker. id就再好不过了。
port
如果使用配置样本来启动 Kafka,它会监听9092端口。修改port配置参数可以把它设置
成其他任意可用的端口。要注意,如果使用1024以下的端口,需要使用root权限启动
Kaka,不过不建议这么做。
3. zookeeper. connect
用于保存 broker元数据的 Zookeeper地址是通过 zookeeper. connect来指定的。
Localhost:2181表示这个 Zookeeper是运行在本地的2181端口上。该配置参数是用冒号分
隔的一组 hostname:port/path列表,每一部分的含义如下
hostname是 Zookeeper服务器的机器名或P地址;
·port是 Zookeeper的客户端连接端口;
·φpath是可选的 Zookeeper路径,作为 Kafka集群的 chroot环境。如果不指定,默认使用
根路径。
如果指定的 chroot路径不存在, broker会在启动的时候创建它。
18|第2章为什么使用 chroot路径
在 Kafka集群里使用 chroot路径是一种最佳实践。 Zookeeper群组可以共享
给其他应用程序,即使还有其他Kaka集群存在,也不会产生冲突。最好是
在配置文件里指定一组 Zookeeper服务器,用分号把它们隔开。一旦有一个
Zookeeper服务器宕机, broker可以连接到 Zookeeper群组的另一个节点上
4. log. dirs
Kaka把所有消息都保存在磁盘上,存放这些日志片段的目录是通过 log dirs指定的。它是
一组用逗号分隔的本地文件系统路径。如果指定了多个路径,那么 broker会根据“最少使
用”原则,把同一个分区的日志片段保存到同一个路径下。要注意, broker会往拥有最少
数目分区的路径新增分区,而不是往拥有最小磁盘空间的路径新增分区。
5. num recovery. threads per data. dir
对于如下3种情况,Kaka会使用可配置的线程池来处理日志片段
服务器正常启动,用于打开每个分区的日志片段;
服务器崩溃后重启,用于检查和截短每个分区的日志片段;
服务器正常关闭,用于关闭日志片段。
默认情况下,每个日志目录只使用一个线程。因为这些线程只是在服务器启动和关闭时会
用到,所以完全可以设置大量的线程来达到并行操作的目的。特别是对于包含大量分区的
服务器来说,一旦发生崩溃,在进行恢复时使用并行操作可能会省下数小时的时间。设置
此参数时需要注意,所配置的数字对应的是 log dirs指定的单个日志目录。也就是说,如
果nun. recovery. threads.per,data.dr被设为8,并且Log.dr指定了3个路径,那么总
共需要24个线程。
6. auto create topics. enable
默认情况下,Kaka会在如下几种情形下自动创建主题
当一个生产者开始往主题写入消息时
当一个消费者开始从主题读取消息时;
当任意一个客户端向主题发送元数据请求时。
很多时候,这些行为都是非预期的。而且,根据Kaka协议,如果一个主题不先被创建,
根本无法知道它是否已经存在。如果显式地创建主题,不管是手动创建还是通过其他配置
系统来创建,都可以把auto. create. topics. enable设为 false。
2.3.2主题的默认配置
Kafka为新创建的主题提供了很多默认配置参数。可以通过管理工具(将在第9章介绍)
为毎个主题单独配置一部分参数,比如分区个数和数据保留策略。服务器提供的默认配置
可以作为基准,它们适用于大部分主题。
安装Kka|19使用主题配置覆盖( override)
之前的Kaka版本允许主题覆盖服务器的默认配置,包括Log. retention
hours per topic, log retention bytes per topic
和tog. seg ment. bytes
per. topic这几个参数。新版本不再支持这些参数,而且如果要对参数进行
覆盖,需要使用管理工具。
1. num partitions
num partitions参数指定了新创建的主题将包含多少个分区。如果启用了主题自动创建功
能(该功能默认是启用的),主题分区的个数就是该参数指定的值。该参数的默认值是1
要注意,我们可以增加主题分区的个数,但不能减少分区的个数。所以,如果要让一个主
题的分区个数少于num. partitions指定的值,需要手动创建该主题(将在第9章讨论)。
第1章里已经提到, Kafka集群通过分区对主题进行横向扩展,所以当有新的 broker加入
集群时,可以通过分区个数来实现集群的负载均衡。当然,这并不是说,在存在多个主题
的情况下(它们分布在多个 broker上),为了能让分区分布到所有 broker上,主题分区的
个数必须要大于 broker的个数。不过,拥有大量消息的主题如果要进行负载分散,就需要
大量的分区
如何选定分区数量
为主题选定分区数量并不是一件可有可无的事情,在进行数量选择时,需要
考虑如下几个因素。
主题需要达到多大的吞吐量?例如,是希望每秒钟写入100KB还是1GB?
从单个分区读取数据的最大吞吐量是多少?每个分区一般都会有一个消费
者,如果你知道消费者将数据写入数据库的速度不会超过每秒50MB,那
么你也该知道,从一个分区读取数据的吞吐量不需要超过每秒50MB。
可以通过类似的方法估算生产者向单个分区写入数据的吞吐量,不过生产
者的速度一般比消费者快得多,所以最好为生产者多估算一些吞吐量。
每个 broker包含的分区个数、可用的磁盘空间和网络带宽。
如果消息是按照不同的键来写入分区的,那么为已有的主题新增分区就会
很困难。
单个 broker对分区个数是有限制的,因为分区越多,占用的内存越多,完
成首领选举需要的时间也越长
很显然,综合考虑以上几个因素,你需要很多分区,但不能太多。如果你估算出主题的吞
吐量和消费者吞吐量,可以用主题吞吐量除以消费者吞吐量算出分区的个数。也就是说,
如果毎秒钟要从主题上写入和读取1GB的数据,并且每个消费者每秒钟可以处理50MB
的数据,那么至少需要20个分区。这样就可以让20个消费者同时读取这些分区,从而达
到每秒钟1GB的吞吐量。
20|第2章如果不知道这些信息,那么根据经验,把分区的大小限制在25GB以内可以得到比较理想
的效果。
2. log. retention.ms
Kafka通常根据时间来决定数据可以被保留多久。默认使用Log. retention. hours参数来配
置时间,默认值为168小时,也就是一周。除此以外,还有其他两个参数 Log retention
minutes和1og, retention,ms。这3个参数的作用是一样的,都是决定消息多久以后会被删
除,不过还是推荐使用1 og retention. ms。如果指定了不止一个参数,Kaka会优先使用
具有最小值的那个参数。
根据时间保留数据和最后修改时间
根据时间保留数据是通过检查磁盘上日志片段文件的最后修改时间来实现
的。一般来说,最后修改时间指的就是日志片段的关闭时间,也就是文件里
最后一个消息的时间戳。不过,如果使用管理工具在服务器间移动分区,最
后修改时间就不准确了。时间误差可能导致这些分区过多地保留数据。在第
9章讨论分区移动时会提到更多这方面的内容。
3. log. retention bytes
另一种方式是通过保留的消息字节数来判断消息是否过期。它的值通过参数Log
retention bytes
来指定,作用在每一个分区上。也就是说,如果有一个包含8个分区的主
题,并且og. retention. bytes被设为lGB,那么这个主题最多可以保留8GB的数据。所
以,当主题的分区个数增加时,整个主题可以保留的数据也随之增加。
根据字节大小和时间保留数据
如果同时指定了1 og retention bytes和1 og retention. ms(或者另一个时
间参数),只要任意一个条件得到满足,消息就会被删除。例如,假设og.
retention.ns设置为8640000(也就是1天), Log retention bytes设置
为100000就是1GB),如果消息字节总数在不到一天的时间就超
过了1GB,那么多出来的部分就会被删除。相反,如果消息字节总数小于
lGB,那么一天之后这些消息也会被删除,尽管分区的数据总量小于1GB。
4. log. segment bytes
以上的设置都作用在日志片段上,而不是作用在单个消息上。当消息到达 broker时,它
们被追加到分区的当前日志片段上。当日志片段大小达到Log. segment bytes指定的上限
(默认是1GB)时,当前日志片段就会被关闭,一个新的日志片段被打开。如果一个日志
片段被关闭,就开始等待过期。这个参数的值越小,就会越频繁地关闭和分配新文件,从
而降低磁盘写入的整体效率。
如果主题的消息量不大,那么如何调整这个参数的大小就变得尤为重要。例如,如果一个
主题每天只接收100MB的消息,而Log. segment bytes使用默认设置,那么需要10天时
安装Kaka|21间才能填满一个日志片段。因为在日志片段被关闭之前消息是不会过期的,所以如果Log.
retention. ms被设为604800000(也就是1周),那么日志片段最多需要17天才会过期。
这是因为关闭日志片段需要10天的时间,而根据配置的过期时间,还需要再保留7天时
间(要等到日志片段里的最后一个消息过期才能被删除)。
使用时间戳获取偏移量
日志片段的大小会影响使用时间戳获取偏移量。在使用时间戳获取日志偏移
A量时, Kafka会检查分区里最后修改时间大于指定时间戳的日志片段(已经
被关闭的),该日志片段的前一个文件的最后修改时间小于指定时间戳。然
后,Kaka返回该日志片段(也就是文件名)开头的偏移量。对于使用时间
戳获取偏移量的操作来说,日志片段越小,结果越准确。
5. log. segment. ms
另一个可以控制日志片段关闭时间的参数是109.se9ment,ms,它指定了多长时间之后日
志片段会被关闭。就像Log. retention. bytes和Lo
og retention. ms这两个参数一样,Log
segment bytes和Log. retention,ms这两个参数之间也不存在互斥问题。日志片段会在大
小或时间达到上限时被关闭,就看哪个条件先得到满足。默认情况下,Log. segment,ns没
有设定值,所以只根据大小来关闭日志片段。
基于时间的日志片段对磁盘性能的影响
在使用基于时间的日志片段时,要着重考虑并行关闭多个日志片段对磁盘性
能的影响。如果多个分区的日志片段永远不能达到大小的上限,就会发生这
种情况,因为 broker在启动之后就开始计算日志片段的过期时间,对于那些
数据量小的分区来说,日志片段的关闭操作总是同时发生。
6. message. max bytes
broker通过设置 message.max. bytes参数来限制单个消息的大小,默认值是1000000,也
就是1MB。如果生产者尝试发送的消息超过这个大小,不仅消息不会被接收,还会收到
broker返回的错误信息。跟其他与字节相关的配置参数一样,该参数指的是压缩后的消息
大小,也就是说,只要压缩后的消息小于 message. max bytes指定的值,消息的实际大小
可以远大于这个值
这个值对性能有显著的影响。值越大,那么负责处理网络连接和请求的线程就需要花越多
的时间来处理这些请求。它还会增加磁盘写入块的大小,从而影响IO吞吐量。
在服务端和客户端之间协调消息大小的配置
消费者客户端设置的 fetch. message.max. bytes必须与服务器端设置的消息
L大小进行协调。如果这个值比mes.x.byes小,那么消费者就无法读
取比较大的消息,导致出现消费者被阻塞的情况。在为集群里的 broker配置
replica. fetch. max bytes参数时,也遵循同样的原则。
22|第2章2.4硬件的选择
为Kaka选择合适的硬件更像是一门艺术。Kaka本身对硬件没有特别的要求,它可以运
行在任何系统上。不过,如果比较关注性能,那么就需要考虑几个会影响整体性能的因
素:磁盘吞吐量和容量、内存、网络和CPU。在确定了性能关注点之后,就可以在预算范
围内选择最优化的硬件配置。
2.4.1磁盘吞吐量
生产者客户端的性能直接受到服务器端磁盘吞吐量的影响。生产者生成的消息必须被提交
到服务器保存,大多数客户端在发送消息之后会一直等待,直到至少有一个服务器确认消
息已经成功提交为止。也就是说,磁盘写入速度越快,生成消息的延迟就越低。
在考虑硬盘类型对磁盘吞吐量的影响时,是选择传统的机械硬盘(HDD)还是固态硬盘
(SSD),我们可以很容易地作岀决定。固态硬盘的査找和访问速度都很快,提供了最好的
性能。机械硬盘更便宜,单块硬盘容量也更大。在同一个服务器上使用多个机械硬盘,可
以设置多个数据目录,或者把它们设置成磁盘阵列,这样可以提升机械硬盘的性能。其他
方面的因素,比如磁盘特定的技术(串行连接存储技术或SATA),或者磁盘控制器的质
量,都会影响吞吐量。
2.4.2磁盘容量
磁盘容量是另一个值得讨论的话题。需要多大的磁盘容量取决于需要保留的消息数量。如
果服务器每天会收到1TB消息,并且保留7天,那么就需要7TB的存储空间,而且还要
为其他文件提供至少10%的额外空间。除此之外,还需要提供缓冲区,用于应付消息流量
的增长和波动。
在决定扩展 Kafka集群规模时,存储容量是一个需要考虑的因素。通过让主题拥有多个分
区,集群的总流量可以被均衡到整个集群,而且如果单个 broker无法支撑全部容量,可以
让其他 broker提供可用的容量。存储容量的选择同时受到集群复制策略的影响(将在第6
章讨论更多的细节)。
243内存
除了磁盘性能外,服务器端可用的内存容量是影响客户端性能的主要因素。磁盘性能影响
生产者,而内存影响消费者。消费者一般从分区尾部读取消息,如果有生产者存在,就紧
跟在生产者后面。在这种情况下,消费者读取的消息会直接存放在系统的页面缓存里,这
比从磁盘上重新读取要快得多。
运行Kaka的JM不需要太大的内存,剩余的系统内存可以用作页面缓存,或者用来缓
存正在使用中的日志片段。这也就是为什么不建议把 Kafka同其他重要的应用程序部署
在一起的原因,它们需要共享页面缓存,最终会降低 Kafka消费者的性能。
安装 Kafka24.4网络
网络吞吐量决定了 Kafka能够处理的最大数据流量。它和磁盘存储是制约 Kafka扩展规模
的主要因素。 Kafka支持多个消费者,造成流入和流出的网络流量不平衡,从而让情况变
得更加复杂。对于给定的主题,一个生产者可能每秒钟写入1MB数据,但可能同时有多
个消费者瓜分网络流量。其他的操作,如集群复制(在第6章介绍)和镜像(在第8章介
绍)也会占用网络流量。如果网络接口出现饱和,那么集群的复制出现延时就在所难免,
从而让集群不堪一击。
2.4.5cPU
与磁盘和内存相比, Kafka对计算处理能力的要求相对较低,不过它在一定程度上还是
会影响整体的性能。客户端为了优化网络和磁盘空间,会对消息进行压缩。服务器需要
对消息进行批量解压,设置偏移量,然后重新进行批量压缩,再保存到磁盘上。这就是
Kafka对计算处理能力有所要求的地方。不过不管怎样,这都不应该成为选择硬件的主
要考虑因素。
2.5云端的 Kafka
Kafka一般被安装在云端,比如亚马逊网络服务( Amazon web services,AWS)。AWS提
供了很多不同配置的实例,我们要根据Kaka的性能优先级来选择合适的实例。可以先从
要保留数据的大小开始考虑,然后考虑生产者方面的性能。如果要求低延迟,那么就需要
专门为IO优化过的使用固态硬盘的实例,否则,使用配备了临时存储的实例就可以了。
选好存储类型之后,再选择CPU和内存就容易得多。
实际上,如果使用AWS,一般会选择m4实例或r3实例。m4实例允许较长时间地保留数
据,不过磁盘吞吐量会小一些,因为它使用的是弹性块存储。r3实例使用固态硬盘,具有
较高的吞吐量,但保留的数据量会有所限制。如果想两者兼顾,那么需要升级成i实例或
d2实例,不过它们的成本要高得多。
2.6 Kafka集群
单个Kaka服务器足以满足本地开发或POC要求,不过集群也有它的强大之处。使用集
群最大的好处是可以跨服务器进行负载均衡,再则就是可以使用复制功能来避免因单点故
障造成的数据丢失。在维护 Kafka或底层系统时,使用集群可以确保为客户端提供高可用
性。本节只是介绍如何配置 Kafka集群,第6章将介绍更多关于数据复制的内容。
24|第2章Kaka集群
Broker 1
主题A
分区0
生产者
Broker 2
主题A
消费者
分区
Broker 3
生产者
主题B
分区0
图22:一个简单的 Kafka集群
2.6.1需要多少个 broker
个Kaka集群需要多少个 broker取决于以下几个因素。首先,需要多少磁盘空间来保
留数据,以及单个 broker有多少空间可用。如果整个集群需要保留10TB的数据,每个
broker可以存储2TB,那么至少需要5个 broker。如果启用了数据复制,那么至少还需要
倍的空间,不过这要取决于配置的复制系数是多少(将在第6章介绍)。也就是说,如
果启用了数据复制,那么这个集群至少需要10个 broker
第二个要考虑的因素是集群处理请求的能力。这通常与网络接口处理客户端流量的能力有
关,特别是当有多个消费者存在或者在数据保留期间流量发生波动(比如高峰时段的流量
爆发)时。如果单个 broker的网络接口在高峰时段可以达到80%的使用量,并且有两个
消费者,那么消费者就无法保持峰值,除非有两个 broker。如果集群启用了复制功能,则
要把这个额外的消费者考虑在内。因磁盘吞吐量低和系统内存不足造成的性能问题,也可
以通过扩展多个 broker来解决。
2.62 broker配置
要把一个 broker加入到集群里,只需要修改两个配置参数。首先,所有 broker都必须配
置相同的 zookeeper. connect,该参数指定了用于保存元数据的 Zookeeper群组和路径。
其次,每个 broker都必须为 broker. id参数设置唯一的值。如果两个 broker使用相同的
broker,讠d,那么第二个 broker就无法启动。在运行集群时,还可以配置其他一些参数,特
别是那些用于控制数据复制的参数,这些将在后续的章节介绍。
安装Kaka|252.6.3操作系统调优
大部分 Linux发行版默认的内核调优参数配置已经能够满足大多数应用程序的运行需求,
不过还是可以通过调整一些参数来进一步提升Kaka的性能。这些参数主要与虚拟内存
网络子系统和用来存储日志片段的磁盘挂载点有关。这些参数一般配置在/ /etc/sysctl. conf
文件里,不过在对内核参数进行调整时,最好参考操作系统的文档。
1.虚拟内存
般来说, Linux的虚拟内存会根据系统的工作负荷进行自动调整。我们可以对交换分区
的处理方式和内存脏页进行调整,从而让Kaka更好地处理工作负载。
对于大多数依赖吞吐量的应用程序来说,要尽量避免内存交换。内存页和磁盘之间的交换
对 Kafka各方面的性能都有重大影响。Kaka大量地使用系统页面缓存,如果虚拟内存被
交换到磁盘,说明已经没有多余内存可以分配给页面缓存了。
种避免内存交换的方法是不设置任何交换分区。内存交换不是必需的,不过它确实能够
在系统发生灾难性错误时提供一些帮助。进行内存交换可以防止操作系统由于内存不足而
突然终止进程。基于上述原因,建议把vn. sappiness i参数的值设置得小一点,比如1。该
参数指明了虚拟机的子系统将如何使用交换分区,而不是只把内存页从页面缓存里移除。
要优先考虑减小页面缓存,而不是进行内存交换。
为什么不把 Vn. Sappiness设为零
先前,人们建议尽量把 Vm. swaptness设为0,它意味着“除非发生内存溢
出,否则不要进行内存交换”。直到 Linux内核3.5-rcl版本发布,这个值的
意义才发生了变化。这个变化被移植到其他的发行版上,包括 Red hat企业
版内核26.32-303。在发生变化之后,0意味着“在任何情况下都不要发生交
换”。所以现在建议把这个值设为1。
脏页会被冲刷到磁盘上,调整内核对脏页的处理方式可以让我们从中获益。Kaka依赖IO性
能为生产者提供快速的响应。这就是为什么日志片段一般要保存在快速磁盘上,不管是单个
快速磁盘(如SSD)还是具有 NVRAM缓存的磁盘子系统(如RAD)。这样一来,在后台刷
新进程将脏页写入磁盘之前,可以减少脏页的数量,这个可以通过将 vm.dirty background
rato设为小于10的值来实现。该值指的是系统内存的百分比,大部分情况下设为5就可以
了。它不应该被设为0,因为那样会促使内核频繁地刷新页面,从而降低内核为底层设备的
磁盘写入提供缓冲的能力。
通过设置w. dirty ratio参数可以增加被内核进程刷新到磁盘之前的脏页数量,可以将它
设为大于20的值(这也是系统内存的百分比)。这个值可设置的范围很广,60-80是个比
较合理的区间。不过调整这个参数会带来一些风险,包括未刷新磁盘操作的数量和同步刷
新引起的长时间Io等待。如果该参数设置了较高的值,建议启用 Kafka的复制功能,避
免因系统崩溃造成数据丢失。
为了给这些参数设置合适的值,最好是在Kaka集群运行期间检查脏页的数量,不管是在
生存环境还是模拟环境。可以在/proc/ vmstat文件里查看当前脏页数量。
26|第2章cat /proc/vmstat egrep "dirty writeback
rty 3875
nr writeback 29
nr writeback temp 0
2.磁盘
除了选择合适的磁盘硬件设备和使用RAID外,文件系统是影响性能的另一个重要因素。
有很多种文件系统可供选择,不过对于本地文件系统来说,EXT4(第四代可扩展文件系
统)和XFS最为常见。近来,XFS成为很多 Linux发行版默认的文件系统,因为它只需
要做少量调优就可以承担大部分的工作负荷,比EXT4具有更好的表现。EXT4也可以做
得很好,但需要做更多的调优,存在较大的风险。其中就包括设置更长的提交间隔(默认
是5),以便降低刷新的频率。EXT4还引入了块分配延迟,一旦系统崩溃,更容易造成数
据丢失和文件系统毁坏。XFS也使用了分配延迟算法,不过比EXT4的要安全些。XFS为
Kafka提供了更好的性能,除了由文件系统提供的自动调优之外,无需额外的调优。批量
磁盘写入具有更高的效率,可以提升整体的IO吞吐量。
不管使用哪一种文件系统来存储日志片段,最好要对挂载点的 noatime参数进行合理的设
置。文件元数据包含3个时间戳:创建时间(cime)、最后修改时间(mime)以及最后访
可时间( atime)。默认情况下,每次文件被读取后都会更新 atime,这会导致大量的磁盘写
操作,而且 atime属性的用处不大,除非某些应用程序想要知道某个文件在最近一次修改
后有没有被访问过(这种情况可以使用 realtime)。Kaka用不到该属性,所以完全可以把
它禁用掉。为挂载点设置 noatime参数可以防止更新aime,但不会影响 ctime和 mtime
3.网络
默认情况下,系统內核没有针对快速的大流量网络传输进行优化,所以对于应用程序来
说,一般需要对 Linux系统的网络栈进行调优,以实现对大流量的支持。实际上,调整
Kafka的网络配置与调整其他大部分Web服务器和网络应用程序的网络配置是一样的。
首先可以对分配给 socket读写缓冲区的内存大小作出调整,这样可以显著提升网络的传
输性能。 socket读写缓冲区对应的参数分别是net.core. wmem default和 net core,rnem
default,合理的值是131072(也就是128KB)。读写缓冲区最大值对应的参数分别是
net.core.Wmen_max和net,core,rmen_max,合理的值是2097152(也就是2MB)。要注
意,最大值并不意味着每个 socket一定要有这么大的缓冲空间,只是说在必要的情况下
才会达到这个值。
除了设置 socket外,还需要设置 TCP socket的读写缓冲区,它们的参数分别是 net. Lpv4
tcp wmem和net.ip4.tcp_rnen。这些参数的值由3个整数组成,它们使用空格分隔,分!
表示最小值、默认值和最大值。最大值不能大于net.core.wne_nax和net.core.rnen_nax
指定的大小。例如,“4096655362048000”表示最小值是4KB、默认值是64KB、最大值
是2MB。根据 Kafka服务器接收流量的实际情况,可能需要设置更高的最大值,为网络连
接提供更大的缓冲空间
还有其他一些有用的网络参数。例如,把net.ipv4.tcp_ window scaling设为1,启用TCP
时间窗扩展,可以提升客户端传输数据的效率,传输的数据可以在服务器端进行缓冲。把
安装 Kafka|27net.ipy4.tcp_ max_syn back log设为比默认值1024更大的值,可以接受更多的并发连接。
把net. corenetdev_max_ back log设为比默认值1000更大的值,有助于应对网络流量的爆
发,特别是在使用千兆网络的情况下,允许更多的数据包排队等待内核处理。
2.7生产环境的注意事项
当你准备把Kaka从测试环境部署到生产环境时,需要注意一些事项,以便创建更可靠的
消息服务。
2.7.1垃圾回收器选项
为应用程序调整Java垃圾回收参数就像是一门艺术,我们需要知道应用程序是如何使用内
存的,还需要大量的观察和试错。幸运的是,Java7为我们带来了G1垃圾回收器,让这
种状况有所改观。在应用程序的整个生命周期,G1会自动根据工作负载情况进行自我调
节,而且它的停顿时间是恒定的。它可以轻松地处理大块的堆内存,把堆内存分为若干小
块的区域,每次停顿时并不会对整个堆空间进行回收。
正常情况下,G1只需要很少的配置就能完成这些工作。以下是G1的两个调整参数。
MaxGCPauseMilLi.
该参数指定每次垃圾回收默认的停顿时间。该值不是固定的,G1可以根据需要使用更
长的时间。它的默认值是200ms。也就是说,G1会决定垃圾回收的频率以及每一轮需
要回收多少个区域,这样算下来,毎一轮垃圾回收大概需要200ms的时间。
InitiatingHeapOccupancyPercent
该参数指定了在G1启动新一轮垃圾回收之前可以使用的堆内存百分比,默认值是45。
也就是说,在堆内存的使用率达到45%之前,G1不会启动垃圾回收。这个百分比包括
新生代和老年代的内存。
Kafka对堆内存的使用率非常高,容易产生垃圾对象,所以可以把这些值设得小一些。如
果一台服务器有64GB内存,并且使用5GB堆内存来运行Kaka,那么可以参考以下的配
置: MaxGCPauseMillis可以设为20ms; InitiatingHeapOccupancy Percent可以设为35,这
样可以让垃圾回收比默认的要早一些启动。
Kaka的启动脚本并没有启用Gl回收器,而是使用了 Parallel New和CMs( Concurrent
Mark- Sweep,并发标记和清除)垃圾回收器。不过它可以通过环境变量来修改。本章前面
的内容使用 start命令来修改它:
export JAVA_HOME=/usr/java/jdk180-51
t export KAFKA JVM PERFORMANCE OPTS="-server -XX: +UseG1GC
XX: MaxGCPauseMillis=20-XX: InitiatingHeapOccupancy Percent=35
XX: +DisableExplicitGc -Djava. awt. headless=true"
#f /usr/local/kafka/bin/kafka-server-start sh-daemon
/usr/local/kafka/ config/server properties
28|第2章27.2数据中心布局
在开发阶段,人们并不会太关心 Kafka服务器在数据中心所处的物理位置,因为即使集群
在短时间内出现局部或完全不可用,也不会造成太大影响。但是,在生产环境,服务不可
用意味着金钱的损失,具体表现为无法为用户提供服务或者不知道用户正在做什么。这个
时候,使用 Kafka集群的复制功能就变得尤为重要(请参考第6章),而服务器在数据中
心所处的物理位置也变得重要起来。如果在部署Kaka之前没有考虑好这个问题,那么在
后续的维护过程中,移动服务器需要耗费更高的成本。
在为 broker增加新的分区时, broker并无法获知机架的信息。也就是说,两个 broker有
可能是在同一个机架上,或者在同一个可用区域里(如果运行在像AWS这样的的云服务
上),所以,在为分区添加副本的时候,这些副本很可能被分配给同一个机架上的 broker,
它们使用相同的电源和网络连接。如果该机架出了问题,这些分区就会离线,客户端就无
法访问到它们。更糟糕的是,如果发生不完整的主节点选举,那么在恢复时就有可能丢失
数据(第6章将介绍更多细节)。
所以,最好把集群的 broker安装在不同的机架上,至少不要让它们共享可能出现单点故障
的基础设施,比如电源和网络。也就是说,部署服务器需要至少两个电源连接(两个不同
的回路)和两个网络交换器(保证可以进行无缝的故障切换)。除了这些以外,最好还要
把 broker安放在不同的机架上。因为随着时间的推移,机架也需要进行维护,而这会导致
机器离线(比如移动机器或者重新连接电源)。
2.7.3共享 Zookeeper
Kafka使用 Zookeeper来保存 broker、主题和分区的元数据信息。对于一个包含多个节点的
Zookeeper群组来说,Kaa集群的这些流量并不算多,那些写操作只是用于构造消费者群
组或集群本身。实际上,在很多部署环境里,会让多个 Kafka集群共享一个 Zookeeper群
组(每个集群使用一个 chroot路径)。
Kafka消费者和 Zookeeper
在Kaka0.9.0.0版本之前,除了 broker之外,消费者也会使用 Zookeeper来
保存一些信息,比如消费者群组的信息、主题信息、消费分区的偏移量(在
消费者群组里发生失效转移时会用到)。到了0.9.0.0版本,Kaka引入了
个新的消费者接口,允许 broker直接维护这些信息。这个新的消费者接口将
在第4章介绍。
不过,消费者和 Zookeeper之间还是有一个值得注意的地方,消费者可以选择将偏移量提
交到 Zookeeper或Kaka,还可以选择提交偏移量的时间间隔。如果消费者将偏移量提交
到 Zookeeper,那么在每个提交时间点上,消费者将会为每一个消费的分区往 Zookeeper写
入一次偏移量。合理的提交间隔是1分钟,因为这刚好是消费者群组的某个消费者发生失
效时能够读取到重复消息的时间。值得注意的是,这些提交对于 Zookeeper来说流量不算
小,特别是当集群里有多个消费者的时候。如果 Zookeeper群组无法处理太大的流量,就
安装 Kafka|29有必要使用长一点的提交时间间隔。不过不管怎样,还是建议使用最新版本的 Kafka,让
消费者把偏移量提交到 Kafka服务器上,消除对 Zookeeper的依赖。
虽然多个Kaka集群可以共享一个 Zookeeper群组,但如果有可能的话,不建议
把 Zookeeper共享给其他应用程序。Kaka对 Zookeeper的延迟和超时比较敏感,与
Zookeeper群组之间的一个通信异常就可能导致Kka服务器出现无法预测的行为。这样
很容易让多个 broker同时离线,如果它们与 Zookeeper之间断开连接,也会导致分区离
线。这也会给集群控制器带来压力,在服务器离线一段时间之后,当控制器尝试关闭一个
服务器时,会表现出一些细小的错误。其他的应用程序因重度使用或进行不恰当的操作给
Zookeeper群组带来压力,所以最好让它们使用自己的 Zookeeper群组。
2.8总结
在这一章,我们学习了如何运行Kaka,同时也讨论了如何为Kaka选择合适的硬件,以
及在生产环境中使用Kaka需要注意的事项。有了Kaka集群之后,接下来要介绍基本
的客户端应用程序。后面两章将介绍如何创建客户端,并用它们向 Kafka生产消息(第3
章)以及从 Kafka读取这些消息(第4章)。
30|第2章第3章
Kafka生产者—向Kaka写入数据
不管是把 Kafka作为消息队列、消息总线还是数据存储平台来使用,总是需要有一个可以
往Kaka写入数据的生产者和一个可以从 Kafka读取数据的消费者,或者一个兼具两种角
色的应用程序。
例如,在一个信用卡事务处理系统里,有一个客户端应用程序,它可能是一个在线商店,
每当有支付行为发生时,它负责把事务发送到 Kafka上。另一个应用程序根据规则引擎检
查这个事务,决定是批准还是拒绝。批准或拒绝的响应消息被写回Kaka,然后发送给发
起事务的在线商店。第三个应用程序从Kaka上读取事务和审核状态,把它们保存到数据
库,随后分析师可以对这些结果进行分析,或许还能借此改进规则引擎。
开发者们可以使用Kaka内置的客户端API开发Kaka应用程序。
在这一章,我们将从Kaka生产者的设计和组件讲起,学习如何使用 Kafka生产者。我们
将演示如何创建 KafkaProducer和 Producer Records对象、如何将记录发送给Kaka,以及
如何处理从 Kafka返回的错误,然后介绍用于控制生产者行为的重要配置选项,最后深入
探讨如何使用不同的分区方法和序列化器,以及如何自定义序列化器和分区器。
在第4章,我们将会介绍Kaka的消费者客户端,以及如何从Kaka读取消息
第三方客户端
除了内置的客户端外, Kafka还提供了二进制连接协议,也就是说,我们直
接向Kaka网络端口发送适当的字节序列,就可以实现从Kaka读取消息或
往Kaka写入消息。还有很多用其他语言实现的Kaka客户端,比如C++
Python、Go语言等,它们都实现了 Kafka的连接协议,使得 Kafka不仅仅
局限于在Java里使用。这些客户端不属于Kaka项目,不过 Kafka项目wiki
上提供了一个清单,列出了所有可用的客户端。连接协议和第三方客户端超
出了本章的讨论范围。3.1生产者概览
一个应用程序在很多情况下需要往Kaka写入消息:记录用户的活动(用于审计和分析)、
记录度量指标、保存日志消息、记录智能家电的信息、与其他应用程序进行异步通信、缓
冲即将写入到数据库的数据,等等。
多样的使用场景意味着多样的需求:是否每个消息都很重要?是否允许丢失一小部分消
息?偶尔出现重复消息是否可以接受?是否有严格的延迟和吞吐量要求?
在之前提到的信用卡事务处理系统里,消息丢失或消息重复是不允许的,可以接受的延迟
最大为500ms,对吞吐量要求较高——我们希望每秒钟可以处理一百万个消息、。
保存网站的点击信息是另一种使用场景。在这个场景里,允许丢失少量的消息或出现少量
的消息重复,延迟可以高一些,只要不影响用户体验就行。换句话说,只要用户点击链接
后可以马上加载页面,那么我们并不介意消息要在几秒钟之后才能到达Kaka服务器。吞
吐量则取决于网站用户使用网站的频度。
不同的使用场景对生产者API的使用和配置会有直接的影响。
尽管生产者API使用起来很简单,但消息的发送过程还是有点复杂的。图3-1展示了向
Kafka发送消息的主要步骤。
ProducerRecord
Topi
[Partition]
Value
Send 0
如果成功,
如果不能重试,
序列化器
返回元数据
抛出异常
分区器
主题
主题B
重试》是
分区0
分区1
批次
批次0
是
批次1
批次1
失败?
批次2
批次2
Kafka Broker
图31:Kaka生产者组件图
2第3章我们从创建一个 Producer Record对象开始, Producer record对象需要包含目标主题和要发
送的内容。我们还可以指定键或分区。在发送 Producer record对象时,生产者要先把键和
值对象序列化成字节数组,这样它们才能够在网络上传输。
接下来,数据被传给分区器。如果之前在 Producer record对象里指定了分区,那么分区器
就不会再做任何事情,直接把指定的分区返回。如果没有指定分区,那么分区器会根据
Producer Record对象的键来选择一个分区。选好分区以后,生产者就知道该往哪个主题和
分区发送这条记录了。紧接着,这条记录被添加到一个记录批次里,这个批次里的所有消
息会被发送到相同的主题和分区上。有一个独立的线程负责把这些记录批次发送到相应的
broker
上
服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka,就返回一个
RecordMetadata对象,它包含了主题和分区信息,以及记录在分区里的偏移量。如果写入
失败,则会返回一个错误。生产者在收到错误之后会尝试重新发送消息,几次之后如果还
是失败,就返回错误信息。
3.2创建 Kafka生产者
要往Kaka写入消息,首先要创建一个生产者对象,并设置一些属性。 Kafka生产者有3
个必选的属性。
bootstrap servers
该属性指定 broker的地址清单,地址的格式为host:port。清单里不需要包含所有的
broker地址,生产者会从给定的 broker里查找到其他 broker的信息。不过建议至少要
提供两个 broker的信息,一旦其中一个宕机,生产者仍然能够连接到集群上。
ey serializer
broker希望接收到的消息的键和值都是字节数组。生产者接口允许使用参数化类型,因
此可以把Java对象作为键和值发送给 broker。这样的代码具有良好的可读性,不过生
产者需要知道如何把这些Java对象转换成字节数组。key. serializer必须被设置为
个实现了 org. apache. kafka. common serialization. Serializer接口的类,生产者会使
用这个类把键对象序列化成字节数组。Kaka客户端默认提供了 ByteArray Serializer
(这个只做很少的事情)、 StringSerializer和 nteger Serializer,因此,如果你只
使用常见的几种Java对象类型,那么就没必要实现自己的序列化器。要注意,key
serializer是必须设置的,就算你打算只发送值内容。
value. serializer
与key. serializer一样, value. serializer指定的类会将值序列化。如果键和值都是字
符串,可以使用与key. serializer一样的序列化器。如果键是整数类型而值是字符串,
那么需要使用不同的序列化器。
下面的代码片段演示了如何创建一个新的生产者,这里只指定了必要的属性,其他使用默
认设置。
private Properties kafkaProps new Properties();(1
Kaka生产者—向 Kafka写入数据33kafkaProps. put( "bootstrap servers",broker 1: 9092, broker 2: 9092");
"org. apache. kafka. common serialization String Serializer );2
kafkaProps. put("value. serializer
org. apache. kafka. common. serialization. String Serializer");
producer new KafkaProducer<String, String>(kafkaProps ):3
①新建一个 Properties对象。
②因为我们打算把键和值定义成字符串类型,所以使用内置的 StringSerializer。
③在这里我们创建了一个新的生产者对象,并为键和值设置了恰当的类型,然后把
Properties对象传给它。
这个接口很简单,通过配置生产者的不同属性就可以很大程度地控制它的行为。Kaka的
文档涵盖了所有的配置参数,我们将在这一章的后面部分介绍其中几个比较重要的参数。
实例化生产者对象后,接下来就可以开始发送消息了。发送消息主要有以下3种方式
发送并忘记(fre-and- forget)
我们把消息发送给服务器,但并不关心它是否正常到达。大多数情况下,消息会正常到
达,因为 Kafka是高可用的,而且生产者会自动尝试重发。不过,使用这种方式有时候
也会丢失一些消息。
同步发送
我们使用send()方法发送消息,它会返回一个 Future对象,调用get()方法进行等待,
就可以知道消息是否发送成功。
异步发送
我们调用send()方法,并指定一个回调函数,服务器在返回响应时调用该函数。
在下面的几个例子中,我们会介绍如何使用上述几种方式来发送消息,以及如何处理可能
发生的异常情况。
本章的所有例子都使用单线程,但其实生产者是可以使用多线程来发送消息的。刚开始的
时候可以使用单个消费者和单个线程。如果需要更高的吞吐量,可以在生产者数量不变的
前提下增加线程数量。如果这样做还不够,可以增加生产者数量。
3.3发送消息到 Kafka
最简单的消息发送方式如下所示。
Producer Record<string, String> record
new Producer Record<>(Customer Country","Precision Products
" France");①
try i
producer. send (record);(2
3 catch(Exception e)I
e printstackTrace;(3
34|第3章①生产者的send()方法将 Producer Record对象作为参数,所以我们要先创建一个
Producer Record对象。 Producer Record有多个构造函数,稍后我们会详细讨论。这里使
用其中一个构造函数,它需要目标主题的名字和要发送的键和值对象,它们都是字符
串。键和值对象的类型必须与序列化器和生产者对象相匹配。
e我们使用生产者的send()方法发送 Producer Record对象。从生产者的架构图里可以看
到,消息先是被放进缓冲区,然后使用单独的线程发送到服务器端。end()方法会返
回一个包含 RecordMetadata的 Future对象,不过因为我们会忽略返回值,所以无法知
道消息是否发送成功。如果不关心发送结果,那么可以使用这种发送方式。比如,记录
Twitter消息日志,或记录不太重要的应用程序日志
③我们可以忽略发送消息时可能发生的错误或在服务器端可能发生的错误,但在发送消
息之前,生产者还是有可能发生其他的异常。这些异常有可能是 Serialization Exception
(说明序列化消息失败)、 Buffer Exhausted Exception或 TimeoutException(说明缓冲区已
满),又或者是 Interrupt Exception(说明发送线程被中断)。
331同步发送消息
最简单的同步发送消息方式如下所示。
Producer Record<string, String> recor
new ProducerRecorde>("Customer Country","Precision Products","France )
try i
producer. send (record).get();
catch(Exception e)t
e printStackTrace(;2
①在这里, producer,send()方法先返回一个 Future对象,然后调用 Future对象的get()
方法等待Kaka响应。如果服务器返回错误,get()方法会抛出异常。如果没有发生错
误,我们会得到一个 RecordMetadata对象,可以用它获取消息的偏移量。
e如果在发送数据之前或者在发送过程中发生了任何错误,比如 broker返回了一个不允
许重发消息的异常或者已经超过了重发的次数,那么就会抛出异常。我们只是简单地把
异常信息打印出来。
KafkaProducer一般会发生两类错误。其中一类是可重试错误,这类错误可以通过重发消息
来解决。比如对于连接错误,可以通过再次建立连接来解决,“无主( no leader)”错误则可
以通过重新为分区选举首领来解决。 KafkaProducer可以被配置成自动重试,如果在多次重
试后仍无法解决问题,应用程序会收到一个重试异常。另一类错误无法通过重试解决,比如
“消息太大”异常。对于这类错误, KafkaProducer不会进行任何重试,直接抛出异常。
332异步发送消息
假设消息在应用程序和Kaka集群之间一个来回需要10ms。如果在发送完每个消息后都
等待回应,那么发送100个消息需要1秒。但如果只发送消息而不等待响应,那么发送
100个消息所需要的时间会少很多。大多数时候,我们并不需要等待响应——尽管Kaka
会把目标主题、分区信息和消息的偏移量发送回来,但对于发送端的应用程序来说不是必
Kafka生产者向Kaka写入数据35需的。不过在遇到消息发送失败时,我们需要抛出异常、记录错误日志,或者把消息写入
错误消息”文件以便日后分析。
为了在异步发送消息的同时能够对异常情况进行处理,生产者提供了回调支持。下面是使
用回调的一个例子。
private class DemoProducer Callback implements Callback (1
@Override
public void on Completion( RecordMetadata recordMetadata, Exception e)i
if (e ! null)[
eprintStackTrace();(2
Producer Record<String, String> record
new Producer Recorde>(" Customer Country","Biomedical Materials","USA");3
producer. send (record, new DemoProducer Callback ();4
0为了使用回调,需要一个实现了og. apache. kafka. clients. producer. Callback接口的
类,这个接口只有一个 inCompletion方法。
②如果 Katka返回一个错误, on Completion方法会抛出一个非空( non null)异常。这里
我们只是简单地把它打印出来,但是在生产环境应该有更好的处理方式。
记录与之前的一样。
4在发送消息时传进去一个回调对象。
34生产者的配置
到目前为止,我们只介绍了生产者的几个必要配置参数 bootstrap servers API以及序
列化器。
生产者还有很多可配置的参数,在Kaka文档里都有说明;它们大部分都有合理的默认
值,所以没有必要去修改它们。不过有几个参数在内存使用、性能和可靠性方面对生产者
影响比较大,接下来我们会一一说明
1. acks
aCks参数指定了必须要有多少个分区副本收到消息,生产者才会认为消息写入是成功的。
这个参数对消息丢失的可能性有重要影响。该参数有如下选项。
如果acks=0,生产者在成功写入消息之前不会等待任何来自服务器的响应。也就是说,
如果当中出现了问题,导致服务器没有收到消息,那么生产者就无从得知,消息也就丢
失了。不过,因为生产者不需要等待服务器的响应,所以它可以以网络能够支持的最大
速度发送消息,从而达到很高的吞吐量。
如果acks=1,只要集群的首领节点收到消息,生产者就会收到一个来自服务器的成功
响应。如果消息无法到达首领节点(比如首领节点崩溃,新的首领还没有被选举出来),
生产者会收到一个错误响应,为了避免数据丢失,生产者会重发消息。不过,如果一个
没有收到消息的节点成为新首领,消息还是会丢失。这个时候的吞吐量取决于使用的是
36
第3章同步发送还是异步发送。如果让发送客户端等待服务器的响应(通过调用 Future对象
的get()方法),显然会增加延迟(在网络上传输一个来回的延迟)。如果客户端使用回
调,延迟问题就可以得到缓解,不过吞吐量还是会受发送中消息数量的限制(比如,生
产者在收到服务器响应之前可以发送多少个消息)
如果acks=aLl,只有当所有参与复制的节点全部收到消息时,生产者才会收到一个来自
服务器的成功响应。这种模式是最安全的,它可以保证不止一个服务器收到消息,就算
有服务器发生崩溃,整个集群仍然可以运行(第5章将讨论更多的细节)。不过,它的
延迟比acks=1时更高,因为我们要等待不只一个服务器节点接收消息。
2. buffer memory
该参数用来设置生产者内存缓冲区的大小,生产者用它缓冲要发送到服务器的消息。如果
应用程序发送消息的速度超过发送到服务器的速度,会导致生产者空间不足。这个时候
send()方法调用要么被阻塞,要么抛出异常,取决于如何设置 block.on. buffer.fuL参数
(在0.9.0.0版本里被替换成了 max. block.ms,表示在抛出异常之前可以阻塞一段时间)。
3. compression type
默认情况下,消息发送时不会被压缩。该参数可以设置为 Snappy、gzip或Lz4,它指定了
消息被发送给 broker之前使用哪一种压缩算法进行压缩。 Snappy压缩算法由 Google发明,
它占用较少的CPU,却能提供较好的性能和相当可观的压缩比,如果比较关注性能和网
络带宽,可以使用这种算法。gzip压缩算法一般会占用较多的CPU,但会提供更高的压缩
比,所以如果网络带宽比较有限,可以使用这种算法。使用压缩可以降低网络传输开销和
存储开销,而这往往是向 Kafka发送消息的瓶颈所在。
4. retries
生产者从服务器收到的错误有可能是临时性的错误(比如分区找不到首领)。在这种情况
下, retries参数的值决定了生产者可以重发消息的次数,如果达到这个次数,生产者会
放弃重试并返回错误。默认情况下,生产者会在每次重试之间等待100ms,不过可以通过
retry backoff.ms参数来改变这个时间间隔。建议在设置重试次数和重试时间间隔之前,
先测试一下恢复一个崩溃节点需要多少时间(比如所有分区选举出首领需要多长时间),
让总的重试时间比Kaka集群从崩溃中恢复的时间长,否则生产者会过早地放弃重试。不
过有些错误不是临时性错误,没办法通过重试来解决(比如“消息太大”错误)。一般情
况下,因为生产者会自动进行重试,所以就没必要在代码逻辑里处理那些可重试的错误。
你只需要处理那些不可重试的错误或重试次数超出上限的情况。
5. batch size
当有多个消息需要被发送到同一个分区时,生产者会把它们放在同一个批次里。该参数指
定了一个批次可以使用的内存大小,按照字节数计算(而不是消息个数)。当批次被填满,
批次里的所有消息会被发送出去。不过生产者并不一定都会等到批次被填满才发送,半满
的批次,甚至只包含一个消息的批次也有可能被发送。所以就算把批次大小设置得很大,
也不会造成延迟,只是会占用更多的内存而已。但如果设置得太小,因为生产者需要更频
繁地发送消息,会增加一些额外的开销。
Kafka生产者—向Kaka写入数据|376. linger. ms
该参数指定了生产者在发送批次之前等待更多消息加入批次的时间。 KafkaProducer会在
批次填满或 linger.πs达到上限时把批次发送出去。默认情况下,只要有可用的线程,生
产者就会把消息发送出去,就算批次里只有一个消息。把 linger.ms设置成比0大的数,
让生产者在发送批次之前等待一会儿,使更多的消息加入到这个批次。虽然这样会增加延
迟,但也会提升吞吐量(因为一次性发送更多的消息,每个消息的开销就变小了)。
7. client id
该参数可以是任意的字符串,服务器会用它来识别消息的来源,还可以用在日志和配额指
标里。
8. max in flight. requests per connection
该参数指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高,就会占用
越多的内存,不过也会提升吞吐量。把它设为1可以保证消息是按照发送的顺序写入服务
器的,即使发生了重试。
9. timeout. ms, request timeout. ms A metadata. fetch timeout. ms
request. timeout. ms指定了生产者在发送数据时等待服务器返回响应的时间, metadata.
fetch. timeout. ms指定了生产者在获取元数据(比如目标分区的首领是谁)时等待服务器
返回响应的时间。如果等待响应超时,那么生产者要么重试发送数据,要么返回一个错误
(抛出异常或执行回调)。 timeout.ns指定了 broker等待同步副本返回消息确认的时间,与
asks的配置相匹配—如果在指定时间内没有收到同步副本的确认,那么 broker就会返回
一个错误。
10. max block. ms
该参数指定了在调用send()方法或使用 partitions For()方法获取元数据时生产者的阻塞
时间。当生产者的发送缓冲区已满,或者没有可用的元数据时,这些方法就会阻塞。在阻
塞时间达到nax.b1ock.ns时,生产者会抛出超时异常。
11. max request size
该参数用于控制生产者发送的请求大小。它可以指能发送的单个消息的最大值,也可以指
单个请求里所有消息总的大小。例如,假设这个值为MB,那么可以发送的单个最大消
息为1MB,或者生产者可以在单个请求里发送一个批次,该批次包含了1000个消息,每
个消息大小为1KB。另外, broker对可接收的消息最大值也有自己的限制( message.max
bytes),所以两边的配置最好可以匹配,避免生产者发送的消息被 broker拒绝。
12. receive buffer. bytes A send buffer bytes
这两个参数分别指定了 TCP Socket接收和发送数据包的缓冲区大小。如果它们被设为-1,
就使用操作系统的默认值。如果生产者或消费者与 broker处于不同的数据中心,那么可以
适当增大这些值,因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽。
38|第3章顺序保证
Kaka可以保证同一个分区里的消息是有序的。也就是说,如果生产者按照
定的顺序发送消息, broker就会按照这个顺序把它们写入分区,消费者也
会按照同样的顺序读取它们。在某些情况下,顺序是非常重要的。例如,往
个账户存入100元再取出来,这个与先取钱再存钱是截然不同的!不过,
有些场景对顺序不是很敏感。
如果把 retries设为非零整数,同时把nax.in. flight. requests per. connection
设为比1大的数,那么,如果第一个批次消息写入失败,而第二个批次写入
成功, broker会重试写入第一个批次。如果此时第一个批次也写入成功,那
么两个批次的顺序就反过来了。
一般来说,如果某些场景要求消息是有序的,那么消息是否写入成功也是
很关键的,所以不建议把 retries设为0。可以把nax.in. flight. requests
per. connection设为1,这样在生产者尝试发送第一批消息时,就不会有其
他的消息发送给 broker。不过这样会严重影响生产者的吞吐量,所以只有在
对消息的顺序有严格要求的情况下才能这么做。
3.5序列化器
我们已经在之前的例子里看到,创建一个生产者对象必须指定序列化器。我们已经知道如
何使用默认的字符串序列化器,Kaka还提供了整型和字节数组序列化器,不过它们还不
足以满足大部分场景的需求。到最后,我们需要序列化的记录类型会越来越多。
接下来演示如何开发自己的序列化器,并介绍Ao序列化器作为推荐的备选方案。
3.5.1自定义序列化器
如果发送到Kaka的对象不是简单的字符串或整型,那么可以使用序列化框架来创建消息
记录,如Avro、Thri或 Protobuf,或者使用自定义序列化器。我们强烈建议使用通用的
序列化框架。不过,为了了解序列化器的工作原理,也为了说明为什么要使用序列化框
架,让我们一起来看看如何自定义一个序列化器
假设你创建了一个简单的类来表示一个客户
public class Customer t
private int customer ID;
private String customer Name;
public Customer(int ID, String name)t
this. customer ID ID:
this customer Name name;
public int getIDO I
Kafka生产者—向Kaka写入数据39return customer iD
public String getNameo)[
return customer Name
现在我们要为这个类创建一个序列化器,它看起来可能是这样的
import org. apache. kafka. common errors. SerializationException
import java nio. ByteBuffe
import java. util. Map
public class Customer Serializer implements Serializer<Customer>
@Override
public void configure(Map configs, boolean isKey)[
//不做任何配置
oVerride
Customer对象被序列化成
表示 customer Id的4字节整数
表示 customer Name长度的4字节整数(如果 customer Name为空,则长度为0)
表示 cus tomer Name的N个字节
public byte [] serialize(String topic, Customer data)[
try t
tell
serLallzedName,
int stringSize;
if (data = null
return null
if (data getName()!= null)[
serializedName data getName(). getBytes("UTF-8")
stringSize serializedName. length
f else i
serializedName new byte [0]:
stringsize =0
Byte Buffer buffer ByteBuffer allocate(4 +4+stringSize)
buffer. putInt(data. getIDO);
buffer. putInt(stringSize)
buffer put(serializedName)
return buffer array o
3 catch(Exception e)(
throw new SerializationException ("Error when serializing Customer to
byte[]
40|第3章@Override
public void close()t
//不需要关闭任何东西
只要使用这个 Customer Serializer,就可以把消息记录定义成 Producer Record<string,
Customer>,并且可以直接把 Customer对象传给生产者。这个例子很简单,不过代码看起
来太脆弱了——如果我们有多种类型的消费者,可能需要把 customer ID字段变成长整型,
或者为 Customer添加 startDate字段,这样就会出现新旧消息的兼容性问题。在不同版
本的序列化器和反序列化器之间调试兼容性问题着实是个挑战——你需要比较原始的字节
数组。更糟糕的是,如果同一个公司的不同团队都需要往Kaka写入 Customer数据,那
么他们就需要使用相同的序列化器,如果序列化器发生改动,他们几乎要在同一时间修改
代码。
基于以上几点原因,我们不建议使用自定义序列化器,而是使用已有的序列化器和反序列
化器,比如JSON、Avro、 Thrift或 Protobuf。下面我们将会介绍Avro,然后演示如何序列
化Avro记录并发送给Kaka
3.52使用Aro序列化
Apache Avro(以下简称Avro)是一种与编程语言无关的序列化格式。 Doug Cutting创建了
这个项目,目的是提供一种共享数据文件的方式。
Avro数据通过与语言无关的 schema来定义。 schema通过JsON来描述,数据被序列化
成二进制文件或JSON文件,不过一般会使用二进制文件。Avro在读写文件时需要用到
schema, schema一般会被内嵌在数据文件里。
Avro有一个很有意思的特性是,当负责写消息的应用程序使用了新的 schema,负责读
消息的应用程序可以继续处理消息而无需做任何改动,这个特性使得它特别适合用在像
Kafka这样的消息系统上。
假设最初的 schema是这样的
["namespace":"customerManagement avro",
"type :"record",
name
Customer
"fields":[
name":"id","type :"int"],
[name:"name
string]
["name": "faxNumber",type:["null",string"],"default":null"](1
0id和nane字段是必需的, faxNumber是可选的,默认为nuLL
假设我们已经使用了这个 schema几个月的时间,并用它生成了几个太字节的数据。现在,
我们决定在新版本里做一些修改。因为在21世纪不再需要 faxNumber字段,需要用 email
Kaka生产者—向 Kafka写入数据4字段来代替它
新的 schema如下
[ namespace:"customerManagementavro
name":Customer
fields":[
["name":"id",type":"int")
t
string"]
"nane":"enaiU","type":["nuLU","string"],"default":"nuL"
更新到新版的 schema后,旧记录仍然包含 faxNumber字段,而新记录则包含emai字段。
部分负责读取数据的应用程序进行了升级,那么它们是如何处理这些变化的呢?
在应用程序升级之前,它们会调用类似 getName()、 getId()和 getFaxNumber()这样的方
法。如果碰到使用新 schema构建的消息, getName()和 getId()方法仍然能够正常返回,
但 getFaxNumber()方法会返回nul,因为消息里不包含传真号码。
在应用程序升级之后, getEmait()方法取代了 getFaxNumber()方法。如果碰到一个使用旧
schema构建的消息,那么 getEmai()方法会返回nu,因为旧消息不包含邮件地址。
现在可以看出使用Avro的好处了:我们修改了消息的 schema,但并没有更新所有负责读
取数据的应用程序,而这样仍然不会出现异常或阻断性错误,也不需要对现有数据进行大
幅更新。
不过这里有以下两个需要注意的地方。
用于写入数据和读取数据的 schema必须是相互兼容的。Avro文档提到了一些兼容性
原则
反序列化器需要用到用于写入数据的 schema,即使它可能与用于读取数据的 schema不
样。Avro数据文件里就包含了用于写入数据的 schema,不过在Kaka里有一种更好
的处理方式,下一小节我们会介绍它
3.53在 Kafka里使用Avro
Avro的数据文件里包含了整个 schema,不过这样的开销是可接受的。但是如果在每条
Kaka记录里都嵌入 schema,会让记录的大小成倍地增加。不过不管怎样,在读取记录
时仍然需要用到整个 schema,所以要先找到 schema。我们遵循通用的结构模式并使用
schema注册表”来达到目的。 schema注册表并不属于Kaka,现在已经有一些开源的
schema注册表实现。在这个例子里,我们使用的是 Confluent Schema Registry。该注册表
的代码可以在 GitHub上找到,你也可以把它作为 Confluent平台的一部分进行安装。如果
你决定使用这个注册表,可以参考它的文档。
我们把所有写入数据需要用到的 schema保存在注册表里,然后在记录里引用 schema的标
识符。负责读取数据的应用程序使用标识符从注册表里拉取 schema来反序列化记录。序
列化器和反序列化器分别负责处理 schema的注册和拉取。Avro序列化器的使用方法与其
42|第3章他序列化器是一样的
生产者
包含 schema
消费者
ID的信息
序列化器
反序列化器
Broke
当前 schema版本
hema
注册表
图32:Avo记录的序列化和反序列化流程图
下面的例子演示了如何把生成的Avro对象发送到 Kafka(关于如何使用Avro生成代码请
参考Ao文档):
Properties props new Properties()
props. put("boot
trap servers",loca lhost: 9092 );
props. put("key. serializer
io confluent. kafka. serializers. KafkaAvroserializer")
props. put (" value. serializer",
io confLuent. kafka. serializers. KafkaAvroSerializer"); 1
props. put(schema registry. url", schemaUr1);2
String topic =customer Contacts
Producer<String, Customer> producer new KafkaProducer<String,
Customer>(props);3
//不断生成事件,直到有人按下CtrL+C组合键
while (true)t
Customer customer Customer Generator getNext();
System. out. println("Generated customer"+
customer tostring)
roducerRecord<String, Customer> record
new Producer Record<>(topic, customer getId(),cus
tomer);④
producer. send (record);5
①使用Avro的 KafkaAvroSerializer来序列化对象。注意, Avro Serializer也可以处理原
语,这就是我们以后可以使用字符串作为记录键、使用客户对象作为值的原因
2 schema registry.urt是一个新的参数,指向 schema的存储位置。
Customer是生成的对象。我们会告诉生产者 Customer对象就是记录的值。
④实例化一个 Producer record对象,并指定 Customer为值的类型,然后再传给它一个
Customer对象。
Kafka生产者—向Kaka写入数据43⑤把 Customer对象作为记录发送出去, KafkaAvroSerializer会处理剩下的事情。
如果你选择使用一般的Avro对象而非生成的Avro对象该怎么办?不用担心,这个时候你
只需提供 schema就可以了:
Properties props new Properties(;
props. put("bootstrap servers","Localhost: 9092
props. put( key serializer
ioconfluent. kafka. serializers. KafkaAvroserializer );(1
props. put( value. serial
io confluent. kafka. serializers. KafkaAvroSerializer
props.put("schema registry.url",url);2
String schemaString ="i"namespace\":\"customer Management. avro\
\"type\":\"record\",+3
\"name\":"Customer\","+
fields":
"\" name":"id\",\"type\":\"int\"},"+
name
type\":\"string\"]
ti"name\":\"email\",\"type\":["null\",\"string
\"],I"default\": \"null\"]"
]}";
Producer<String, GenericRecord> producer
new KafkaProducer<String, GenericRecord>(props);4
Schema Parser parser new Schema Parser(
Schema schema parser parse(schemaString)
for (int cUstomers =0; cUstomers customers; nCustomers++)[
String name =exampleCustomer " cUstomers
Stringemail="example"+cUstomers+"@example.com
GenericRecord customer new GenericData Record(schema);5
customer put(" id", cUstomers)
customer put(" name", name)
customer put("email", email
Producer Record<String, GenericRecord> data
new Producer Record<String
GenericRecord( "customer Contacts
name, customer
producer. send (data)
①仍然使用同样的 KafkaAvroSerializer。
2提供同样的 schema注册表URI。
这里需要提供 Avro schema,因为我们没有使用Avo生成的对象。
④对象类型是 Avro GenericRecord,我们通过 schema和需要写入的数据来初始化它。
⑤ Producer Record的值就是一个 enericRecord对象,它包含了 schema和数据。序列化器
知道如何从记录里获取 schema,把它保存到注册表里,并用它序列化对象数据。
44|第3章3.6分区
在之前的例子里, Producer Record对象包含了目标主题、键和值。 Kafka的消息是一个个
键值对, Producer record对象可以只包含目标主题和值,键可以设置为默认的nuLl,不
过大多数应用程序会用到键。键有两个用途:可以作为消息的附加信息,也可以用来
决定消息该被写到主题的哪个分区。拥有相同键的消息将被写到同一个分区。也就是
说,如果一个进程只从一个主题的分区读取数据(第4章会介绍更多细节),那么具有相
同键的所有记录都会被该进程读取。要创建一个包含键值的记录,只需像下面这样创建
Producer Record对象:
ProducerRecord<Integer, String> record
new ProducerRecord<>("Customer Country","Laboratory Equipment" ,USA");
如果要创建键为nuLL的消息,不指定键就可以了:
Producer Record<Integer, String> record
new ProducerRecord<>("Customer Country","USA");1
0这里的键被设为nuL
如果键值为nul,并且使用了默认的分区器,那么记录将被随机地发送到主题内各个可用
的分区上。分区器使用轮询( Round robin)算法将消息均衡地分布到各个分区上。
如果键不为空,并且使用了默认的分区器,那么 Kafka会对键进行散列(使用Kaka自己
的散列算法,即使升级Java版本,散列值也不会发生变化),然后根据散列值把消息映射
到特定的分区上。这里的关键之处在于,同一个键总是被映射到同一个分区上,所以在进
行映射时,我们会使用主题所有的分区,而不仅仅是可用的分区。这也意味着,如果写入
数据的分区是不可用的,那么就会发生错误。但这种情况很少发生。我们将在第6章讨论
Kafka的复制功能和可用性
只有在不改变主题分区数量的情况下,键与分区之间的映射才能保持不变。举个例子,在
分区数量保持不变的情况下,可以保证用户045189的记录总是被写到分区34。在从分
区读取数据时,可以进行各种优化。不过,一旦主题增加了新的分区,这些就无法保证
了—旧数据仍然留在分区34,但新的记录可能被写到其他分区上。如果要使用键来映射
分区,那么最好在创建主题的时候就把分区规划好(第2章介绍了如何确定合适的分区数
量),而且永远不要增加新分区。
实现自定义分区策略
我们已经讨论了默认分区器的特点,它是使用次数最多的分区器。不过,除了散列分区之
外,有时候也需要对数据进行不一样的分区。假设你是一个B2B供应商,你有一个大客
户,它是手持设备 Banana的制造商。 Banana占据了你整体业务10%的份额。如果使用默
认的散列分区算法, Banana的账号记录将和其他账号记录一起被分配给相同的分区,导致
这个分区比其他分区要大一些。服务器可能因此出现存储空间不足、处理缓慢等问题。我
们需要给 Banana分配单独的分区,然后使用散列分区算法处理其他账号。
Kafka生产者向Kaka写入数据45下面是一个自定义分区器的例子:
import org. apache. kafka. clients. producer Partitioner
import org. apache. kafka. common Cluster
import org. apache. kafka. common. PartitionInfo
import org. apache. kafka. common record. InvalidRecordException;
import org. apache. kafka. common utils. Utils
public class BananaPartitioner implements Partitioner
public void configure (Map<String, ? configs)0
public int partition(String topic, object key, byte[] keyBytes
Object value, byte[] valueBytes
Cluster cluster)[
List<PartitionInfo> partitions
cluster. partitionsFor Topic(topic);
int numPartitions partitions size)
if ((keyBytes = null)II((key instanceof String)))2
throw new InvalidRecordException ("We expect all messages
to have customer name as key ")
if (((String) key).equals("Banana")
return numPartitions;// Banana总是被分配到最后一个分区
/其他记录被散列到其他分区
return(Math. abs(Utils. murmur 2(keyBytes))%(numPartitions- 1))
public void close()0
1 Partitioner接口包含了 configure、 partition和 close这3个方法。这里我们只实现
partition方法,不过我们真不应该在 partition方法里硬编码客户的名字,而应该通
过 configure方法传进来。
②我们只接受字符串作为键,如果不是字符串,就抛出异常。
3.7旧版的生产者AP
在这一章,我们讨论了生产者的Java客户端,它是org. apache. kafka. clients包的一部
分。在写到这一章的时候,Kaka还有两个旧版的 Scala客户端,它们是 Kafka. producer
包的一部分,同时也是Kaka的核心模块,它们是 SyncProducer(根据acks参数的具
体配置情况,在发送更多的消息之前,它会等待服务器对已发消息或批次进行确认)和
AsyncProducer(在后台将消息分为不同的批次,使用单独的线程发送这些批次,不为客户
端提供发送结果)。
因为当前版本的生产者API同时支持上述两种发送方式,而且为开发者提供了更高的可靠
性和灵活性,所以我们不再讨论旧版的API。如果你想使用它们,那么在使用之前请再三
考虑,如果确定要使用,可以从Kaka文档中了解更多的信息、。
46|第3章3.8总结
我们以一个生产者示例开始了本章的内容—使用10行代码将消息发送到Kaka。然后我
们在代码中加入错误处理逻辑,并介绍了同步和异步两种发送方式。接下来,我们介绍了
生产者的一些重要配置参数以及它们对生产者行为的影响。我们还讨论了用于控制消息格
式的序列化器,并深入探讨了Avo一种在 Kafka中得到广泛应用的序列化方式。最
后,我们讨论了 Kafka的分区机制,并给出了一个自定义分区的例子
现在我们已经知道如何向Kaka写入消息,在第4章,我们将学习如何从Kaka读取消息。
Kaka生产者—向Kaka写入数据47第4章
Kafka消费者—从Kaka读取数据
应用程序使用 Kafkaconsumer向Kaka订阅主题,并从订阅的主题上接收消息。从Kaka
读取数据不同于从其他消息系统读取数据,它涉及一些独特的概念和想法。如果不先理解
这些概念,就难以理解如何使用消费者AP。所以我们接下来先解释这些重要的概念,然
后再举几个例子,演示如何使用消费者API实现不同的应用程序。
4.1 Kafkaconsume概念
要想知道如何从 Kafka读取消息,需要先了解消费者和消费者群组的概念。以下章节将解
释这些概念。
4.1.1消费者和消费者群组
假设我们有一个应用程序需要从一个 Kafka主题读取消息并验证这些消息,然后再把它们
保存起来。应用程序需要创建一个消费者对象,订阅主题并开始接收消息,然后验证消息
并保存结果。过了一阵子,生产者往主题写入消息的速度超过了应用程序验证数据的速
度,这个时候该怎么办?如果只使用单个消费者处理消息,应用程序会远跟不上消息生成
的速度。显然,此时很有必要对消费者进行横向伸缩。就像多个生产者可以向相同的主题
写入消息一样,我们也可以使用多个消费者从同一个主题读取消息,对消息进行分流
Kafka消费者从属于消费者群组。一个群组里的消费者订阅的是同一个主题,每个消费者
接收主题一部分分区的消息。
假设主题T1有4个分区,我们创建了消费者C1,它是群组G1里唯一的消费者,我们用
它订阅主题T1。消费者C1将收到主题T1全部4个分区的消息,如图4-1所示。
48主题T
消费者群组1
分区0
消费者1
分区1
分区2
分区3
图4-1:1个消费者收到4个分区的消息
如果在群组G1里新增一个消费者C2,那么毎个消费者将分别从两个分区接收消息。我们
假设消费者C1接收分区0和分区2的消息,消费者C2接收分区1和分区3的消息,如图
4-2所示。
主题T1
消费者群组1
分区0
消费者1
分区1
消费者2
分区2
分区3
图42:2个消费者收到4个分区的消息
如果群组G1有4个消费者,那么每个消费者可以分配到一个分区,如图4-3所示。
主题T1
消费者群组1
分区0
一消费者
分区1
消费者2
分区2
者3
消费者
分区3
消费者
图43:4个消费者收到4个分区的消息
Kaka消费者—从 Kafka读取数据49如果我们往群组里添加更多的消费者,超过主题的分区数量,那么有一部分消费者就会被
闲置,不会接收到任何消息,如图4-4所示。
主题T1
消费者群组1
分区0
消费者1
分区1
消费者2
分区2
3
消费者
分区3
消费者4
消费者5
图44:5个消费者收到4个分区的消息
往群组里增加消费者是横向伸缩消費能力的主要方式。Kaka消费者经常会做一些髙延迟
的操作,比如把数据写到数据库或HDFS,或者使用数据进行比较耗时的计算。在这些情
况下,单个消费者无法跟上数据生成的速度,所以可以增加更多的消费者,让它们分担负
载,每个消费者只处理部分分区的消息,这就是横向伸缩的主要手段。我们有必要为主题
创建大量的分区,在负载增长时可以加入更多的消费者。不过要注意,不要让消费者的数
量超过主题分区的数量,多余的消费者只会被闲置。第2章介绍了如何为主题选择合适的
分区数量。
除了通过增加消费者来横向伸缩单个应用程序外,还经常出现多个应用程序从同一个主题
读取数据的情况。实际上,Kaka设计的主要目标之一,就是要让Kaka主题里的数据能
够满足企业各种应用场景的需求。在这些场景里,每个应用程序可以获取到所有的消息
而不只是其中的一部分。只要保证毎个应用程序有自己的消费者群组,就可以让它们获取
到主题所有的消息。不同于传统的消息系统,横向伸缩 Katka消费者和消费者群组并不会
对性能造成负面影响。
在上面的例子里,如果新增一个只包含一个消费者的群组G2,那么这个消费者将从主题
T1上接收所有的消息,与群组G1之间互不影响。群组G2可以增加更多的消费者,每个
消费者可以消费若干个分区,就像群组G1那样,如图4-5所示。总的来说,群组G2还是
会接收到所有消息,不管有没有其他群组存在。
简而言之,为每一个需要获取一个或多个主题全部消息的应用程序创建一个消费者群组,
然后往群组里添加消费者来伸缩读取能力和处理能力,群组里的每个消费者只处理一部分
消息。
50第4章主题TF
消费者群组1
分区0
消费者1
分区1
消费者
分区2
消费者3
分区3
消费者4
消费者群组2
老
消费
消费者2
图45:两个消费者群组对应一个主题
4.12消费者群组和分区再均衡
我们已经从上一个小节了解到,群组里的消费者共同读取主题的分区。一个新的消费者加
入群组时,它读取的是原本由其他消费者读取的消息。当一个消费者被关闭或发生崩溃
时,它就离开群组,原本由它读取的分区将由群组里的其他消费者来读取。在主题发生变
化时,比如管理员添加了新的分区,会发生分区重分配。
分区的所有权从一个消费者转移到另一个消费者,这样的行为被称为再均衡。再均衡非常
重要,它为消费者群组带来了高可用性和伸缩性(我们可以放心地添加或移除消费者),
不过在正常情况下,我们并不希望发生这样的行为。在再均衡期间,消费者无法读取消
息,造成整个群组一小段时间的不可用。另外,当分区被重新分配给另一个消费者时,消
费者当前的读取状态会丢失,它有可能还需要去刷新缓存,在它重新恢复状态之前会拖慢
应用程序。我们将在本章讨论如何进行安全的再均衡,以及如何避免不必要的再均衡。
消费者通过向被指派为群组协调器的 broker(不同的群组可以有不同的协调器)发送心跳
来维持它们和群组的从属关系以及它们对分区的所有权关系。只要消费者以正常的时间
间隔发送心跳,就被认为是活跃的,说明它还在读取分区里的消息。消费者会在轮询消息
(为了获取消息)或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长,会
话就会过期,群组协调器认为它已经死亡,就会触发一次再均衡
如果一个消费者发生崩溃,并停止读取消息,群组协调器会等待几秒钟,确认它死亡了才
会触发再均衡。在这几秒钟时间里,死掉的消费者不会读取分区里的消息。在清理消费者
时,消费者会通知协调器它将要离开群组,协调器会立即触发一次再均衡,尽量降低处理
停顿。在本章的后续部分,我们将讨论一些用于控制发送心跳频率和会话过期时间的配置
Kafka消费者—从 Kafka读取数据51参数,以及如何根据实际需要来配置这些参数。
心跳行为在最近版本中的变化
在0.10.1版本里, Kafka社区引入了一个独立的心跳线程,可以在轮询消息的空档发
送心跳。这样一来,发送心跳的频率(也就是消费者群组用于检测发生崩溃的消费者
或不再发送心跳的消费者的时间)与消息轮询的频率(由处理消息所花费的时间来确
定)之间就是相互独立的。在新版本的 Kafka里,可以指定消费者在离开群组并触发
再均衡之前可以有多长时间不进行消息轮询,这样可以避免出现活锁( (livelock),比
如有时候应用程序并没有崩溃,只是由于某些原因导致无法正常运行。这个配置与
sessIon, timeout.ns是相互独立的,后者用于控制检测消费者发生崩渍的时间和停止
发送心跳的时间。
本章的剩余部分将会讨论使用旧版本Kaka会面临的一些问题,以及如何解决这些问
題。本章还包括如何应对需要较长时间来处理消息的情况的讨论,这些与0.10.1或更
高版本的 Kafka没有太大关系。如果你使用的是较新版本的 Kafka,并且需要处理耗
费较长时间的消息,只需要加大ηax,poL. interval.ns的值来増加轮询间隔的时长。
分配分区是怎样的一个过程
当消费者要加入群组时,它会向群组协调器发送一个 Join group请求。第
个加入群组的消费者将成为“群主”。群主从协调器那里获得群组的成员列
表(列表中包含了所有最近发送过心跳的消费者,它们被认为是活跃的),
并负责给每一个消费者分配分区。它使用一个实现了 PartitionAssignor接
口的类来决定哪些分区应该被分配给哪个消费者。
Katka内置了两种分配策略,在后面的配置参数小节我们将深入讨论。分配
完毕之后,群主把分配情况列表发送给群组协调器,协调器再把这些信息发
送给所有消费者。每个消费者只能看到自己的分配信息,只有群主知道群组
里所有消费者的分配信息。这个过程会在每次再均衡时重复发生。
4.2创建 Kafka消费者
在读取消息之前,需要先创建一个 Kafkaconsumer对象。创建 Kafkaconsumer对象与创建
KafkaProducer对象非常相似—把想要传给消费者的属性放在 Properties对象里。本章
后续部分会深入讨论所有的属性。在这里,我们只需要使用3个必要的属性: bootstrap
servers、key. deserializer和vaue. deserializer。
第1个属性 bootstrap. servers指定了Kaka集群的连接字符串。它的用途与在
KafkaProducer中的用途是一样的,可以参考第3章了解它的详细定义。另外两个属性key
deserializer和 value deserializer与生产者的 serializer定义也很类似,不过它们不是使
用指定的类把Java对象转成字节数组,而是使用指定的类把字节数组转成Java对象。
52第4章第4个属性 group.讨d不是必需的,不过我们现在姑且认为它是必需的。它指定了
KafkaConsumer属于哪一个消费者群组。创建不属于任何一个群组的消费者也是可以的,只
是这样做不太常见,在本书的大部分章节,我们都假设消费者是属于某个群组的
下面的代码片段演示了如何创建一个 Kafkaconsumer对象:
Properties props new Properties(
props. put("bootstrap servers","broker1: 9092, broker2: 9092 )7
props. put("group id","Country Counter )i
props. put("key. deserializer",
org. apache. kafka. common. serialization StringDeserializer
props. put("value. deserializer
org. apache. kafka. common, serialization. StringDeserializer )
KafkaConsumer<String, String> consumer new KafkaConsumer<string
String>(props);
如果在第3章看过如何创建生产者,就应该很熟悉上面的这段代码。我们假设消费的键和
值都是字符串类型,所以使用的是内置的 StringDeserializer,并且使用字符串类型创建
了 KafkaConsumer对象。唯一不同的是新增了 group.ud属性,它指定了消费者所属群组的
名字。
4.3订阅主题
创建好消费者之后,下一步可以开始订阅主题了。 subscribe()方法接受一个主题列表作
为参数,使用起来很简单:
consumer subscribe( Collections. sing letonList("customer Countries"));
①为了简单起见,我们创建了一个只包含单个元素的列表,主题的名字叫作“ customer
Countries。
我们也可以在调用 subscribe()方法时传入一个正则表达式。正则表达式可以匹配多个主
题,如果有人创建了新的主题,并且主题的名字与正则表达式匹配,那么会立即触发一次
再均衡,消费者就可以读取新添加的主题。如果应用程序需要读取多个主题,并且可以处
理不同类型的数据,那么这种订阅方式就很管用。在Kaka和其他系统之间复制数据时,
使用正则表达式的方式订阅多个主题是很常见的做法。
要订阅所有与test相关的主题,可以这样做
consumer subscribe("test. *")
4.4轮询
消息轮询是消费者AP的核心,通过一个简单的轮询向服务器请求数据。一旦消费者订阅
了主题,轮询就会处理所有的细节,包括群组协调、分区再均衡、发送心跳和获取数据
开发者只需要使用一组简单的AP来处理从分区返回的数据。消费者代码的主要部分如下
所示:
Kafka消费者—从Kka读取数据|53try t
thile(true)[1
ConsumerRecords<string, String> records= consumer po
(100);2
for(Consumer Record<string, String> record records)3
log debug("topic %s, partition = %s, offset = %d, customer=%s
country=‰s\n",
record.topico), record. partition(), record. offset(
record key o), record value)
int updated Count =1
if (custCountryMap
countains Value(record value()))[
updated Count custCountryMap get(record, value())+
custCountryMap. put(record value(), updatedCount)
JSoNobject json new JSoNObject(custCountryMap)
System. out. println(json. tostring(4))4
j finally
consumer. close();⑤
①这是一个无限循环。消费者实际上是一个长期运行的应用程序,它通过持续轮询向
Kaka请求数据。稍后我们会介绍如何退出循环,并关闭消费者
这一行代码非常重要。就像鲨鱼停止移动就会死掉一样,消费者必须持续对Kaka进
行轮询,否则会被认为已经死亡,它的分区会被移交给群组里的其他消费者。传给
pL()方法的参数是一个超时时间,用于控制po()方法的阻塞时间(在消费者的缓
冲区里没有可用数据时会发生阻塞)。如果该参数被设为0,poL()会立即返回,否则
它会在指定的毫秒数内一直等待 broker返回数据。
③pol()方法返回一个记录列表。每条记录都包含了记录所属主题的信息、记录所在分
区的信息、记录在分区里的偏移量,以及记录的键值对。我们一般会遍历这个列表,逐
条处理这些记录。po1()方法有一个超时参数,它指定了方法在多久之后可以返回,
不管有没有可用的数据都要返回。超时时间的设置取决于应用程序对响应速度的要求,
比如要在多长时间内把控制权归还给执行轮询的线程。
④把结果保存起来或者对已有的记录进行更新,处理过程也随之结束。在这里,我们的目
的是统计来自各个地方的客户数量,所以使用了一个散列表来保存结果,并以JSON的
格式打印结果。在真实场景里,结果一般会被保存到数据存储系统里
在退出应用程序之前使用cose()方法关闭消费者。网络连接和 socket也会随之关闭,
并立即触发一次再均衡,而不是等待群组协调器发现它不再发送心跳并认定它已死亡,
因为那样需要更长的时间,导致整个群组在一段时间内无法读取消息。
轮询不只是获取数据那么简单。在第一次调用新消费者的p()方法时,它会负责查找
Group Coordinator,然后加入群组,接受分配的分区。如果发生了再均衡,整个过程也是
在轮询期间进行的。当然,心跳也是从轮询里发送出去的。所以,我们要确保在轮询期间
所做的任何处理工作都应该尽快完成。
54|第4章线程安全
在同一个群组里,我们无法让一个线程运行多个消费者,也无法让多个线
程安全地共享一个消费者。按照规则,一个消费者使用一个线程。如果
要在同一个消费者群组里运行多个消费者,需要让每个消费者运行在自己
的线程里。最好是把消费者的逻辑封装在自己的对象里,然后使用Java
的 Executor Service启动多个线程,使每个消费者运行在自己的线程上。
Confluent的博客(htts:/w. confluent. io/blog/)上有一个教程介绍如何处
理这种情况。
4.5消费者的配置
到目前为止,我们学习了如何使用消费者AP,不过只介绍了几个配置属性— bootstrap
servers、 group.id、key. deserializer和 value. deserializer。 Kafka的文档列出了所有与消费者相
关的配置说明。大部分参数都有合理的默认值,一般不需要修改它们,不过有一些参数与消费
者的性能和可用性有很大关系。接下来介绍这些重要的属性。
1. fetch. min. bytes
该属性指定了消费者从服务器获取记录的最小字节数。 broker在收到消费者的数据请求时,
如果可用的数据量小于 fetch min. bytes指定的大小,那么它会等到有足够的可用数据时
才把它返回给消费者。这样可以降低消费者和 broker的工作负载,因为它们在主题不是很
活跃的时候(或者一天里的低谷时段)就不需要来来回回地处理消息。如果没有很多可用
数据,但消费者的CPU使用率却很高,那么就需要把该属性的值设得比默认值大。如果
消费者的数量比较多,把该属性的值设置得大一点可以降低 broker的工作负载。
2. fetch. max wait. ms
我们通过 fetch. min. bytes告诉Kaka,等到有足够的数据时才把它返回给消费者。而feth
nax,wait,ms则用于指定 broker的等待时间,默认是500ms。如果没有足够的数据流入
Kaka,消费者获取最小数据量的要求就得不到满足,最终导致500ms的延迟。如果要降低
潜在的延迟(为了满足SLA),可以把该参数值设置得小一些。如果feth.max. wait. ms被设
为100ms,并且 fetch min, bytes被设为1MB,那么Kaka在收到消费者的请求后,要么返
回IMB数据,要么在100ms后返回所有可用的数据,就看哪个条件先得到满足
3. max partition. fetch bytes
该属性指定了服务器从每个分区里返回给消费者的最大字节数。它的默认值是MB,也
就是说, KafkaConsumer.poL()方法从每个分区里返回的记录最多不超过 max partition
fetch. bytes指定的字节。如果一个主题有20个分区和5个消费者,那么每个消费者需要
至少4MB的可用内存来接收记录。在为消费者分配内存时,可以给它们多分配一些,因
为如果群组里有消费者发生崩溃,剩下的消费者需要处理更多的分区。max, partition
fetch bytes的值必须比 broker能够接收的最大消息的字节数(通过max. message.size属
性配置)大,否则消费者可能无法读取这些消息,导致消费者一直挂起重试。在设置该属
Kafka消费者—从 Kafka读取数据55性时,另一个需要考虑的因素是消费者处理数据的时间。消费者需要频繁调用poL()方法
来避免会话过期和发生分区再均衡,如果单次调用poL()返回的数据太多,消费者需要更
多的时间来处理,可能无法及时进行下一个轮询来避免会话过期。如果出现这种情况,可
以把 max partition. fetch bytes值改小,或者延长会话过期时间。
4. session timeout. ms
该属性指定了消费者在被认为死亡之前可以与服务器断开连接的时间,默认是3s。如
果消费者没有在 session, timeout.ns指定的时间内发送心跳给群组协调器,就被认为
已经死亡,协调器就会触发再均衠,把它的分区分配给群组里的其他消费者。该属性与
heartbeat. interval.ns紧密相关。 heartbeat. interva.ms指定了poL()方法向协调器
发送心跳的频率, sessIon. timeout.ns则指定了消费者可以多久不发送心跳。所以,
般需要同时修改这两个属性, heartbeat. interval.ns必须比 session. timeout.ms小,
般是 sessIon. timeout. ms的三分之一。如果 sessIon. timeout.ns是3s,那么 heartbeat
interval.ns应该是ls。把 sessIon. timeout.ms值设得比默认值小,可以更快地检测和恢
复崩溃的节点,不过长时间的轮询或垃圾收集可能导致非预期的再均衡。把该属性的值设
置得大一些,可以减少意外的再均衡,不过检测节点崩溃需要更长的时间。
5. auto offset. reset
该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下(因消费者长
时间失效,包含偏移量的记录已经过时并被删除)该作何处理。它的默认值是 latest,意
思是说,在偏移量无效的情况下,消费者将从最新的记录开始读取数据(在消费者启动之
后生成的记录)。另一个值是 earliest,意思是说,在偏移量无效的情况下,消费者将从
起始位置读取分区的记录。
6. enable auto commit
我们稍后将介绍几种不同的提交偏移量的方式。该属性指定了消费者是否自动提交偏移
量,默认值是 true。为了尽量避免出现重复数据和数据丢失,可以把它设为 false,由自
己控制何时提交偏移量。如果把它设为true,还可以通过配置auto. commit. interval.ns
属性来控制提交的频率。
7. partition assignment strategy
我们知道,分区会被分配给群组里的消费者。 PartitionAssignor根据给定的消费者和主
题,决定哪些分区应该被分配给哪个消费者。Kaka有两个默认的分配策略。
Range
该策略会把主题的若干个连续的分区分配给消费者。假设消费者C1和消费者C2同时
订阅了主题T1和主题T2,并且每个主题有3个分区。那么消费者C1有可能分配到这
两个主题的分区0和分区1,而消费者C2分配到这两个主题的分区2。因为每个主题
拥有奇数个分区,而分配是在主题内独立完成的,第一个消费者最后分配到比第二个消
費者更多的分区。只要使用了Ranεe策略,而且分区数量无法被消费者数量整除,就会
出现这种情况。
56|第4章Roundrobin
该策略把主题的所有分区逐个分配给消费者。如果使用 Roundrobin策略来给消费者C1
和消费者C2分配分区,那么消费者C1将分到主题T1的分区0和分区2以及主题T2
的分区1,消费者C2将分配到主题T1的分区1以及主题T2的分区0和分区2。一般
来说,如果所有消费者都订阅相同的主题(这种情况很常见), Roundrobin策略会给所
有消费者分配相同数量的分区(或最多就差一个分区)。
可以通过设置 partition, assignment. strategy来选择分区策略。默认使用的是org
apache. kafka. clients. consumer. RangeAssignor,这个类实现了 Range策略,不过也可以
把它改成
org. apache. kafka. clients. consumer. RoundRobinAssignor o
我们还可以使用自定
义策略,在这种情况下, partition.as5 gnment. strategy属性的值就是自定义类的名字。
8. client id
该属性可以是任意字符串, broker用它来标识从客户端发送过来的消息,通常被用在日志
度量指标和配额里。
9. max poll. records
该属性用于控制单次调用cal()方法能够返回的记录数量,可以帮你控制在轮询里需要处
理的数据量。
10. receive buffer bytes FA send buffer bytes
socket在读写数据时用到的TCP缓冲区也可以设置大小。如果它们被设为-1,就使用操
作系统的默认值。如果生产者或消费者与 broker处于不同的数据中心内,可以适当增大这
些值,因为跨数据中心的网络一般都有比较高的延迟和比较低的带宽
4.6提交和偏移量
每次调用poLl()方法,它总是返回由生产者写入Kaka但还没有被消费者读取过的记录,
我们因此可以追踪到哪些记录是被群组里的哪个消费者读取的。之前已经讨论过, Kafka
不会像其他JMS队列那样需要得到消费者的确认,这是 Kafka的一个独特之处。相反,消
费者可以使用Kaka来追踪消息在分区里的位置(偏移量)。
我们把更新分区当前位置的操作叫作提交。
那么消费者是如何提交偏移量的呢?消费者往一个叫作_ consumer_ offset的特殊主题发送
消息,消息里包含毎个分区的偏移量。如果消费者一直处于运行状态,那么偏移量就没有
什么用处。不过,如果消费者发生崩溃或者有新的消费者加入群组,就会触发再均衡,完
成再均衡之后,每个消费者可能分配到新的分区,而不是之前处理的那个。为了能够继续
之前的工作,消费者需要读取毎个分区最后一次提交的偏移量,然后从偏移量指定的地方
继续处理。
如果提交的偏移量小于客户端处理的最后一个消息的偏移量,那么处于两个偏移量之间的
消息就会被重复处理,如图46所示。
Kafka消费者—从 Kafka读取数据|57正在处理的事件
这些事件在进行再均衡时会
被重新处理,导致重复
上一次轮询
上一次提交的偏移量
返回的事件
图4-6:提交的偏移量小于客户端处理的最后一个消息的偏移量
如果提交的偏移量大于客户端处理的最后一个消息的偏移量,那么处于两个偏移量之间的
消息将会丢失,如图4-7所示。
正在处理的事件
上一次轮询
返回的事件
这些事件在进行
再均衡时会丢失
上一次提交的偏移量
图47:提交的偏移量大于客户端处理的最后一个消息的偏移量
所以,处理偏移量的方式对客户端会有很大的影响。
Kafka Consumer API提供了很多种方式来提交偏移量。
4.6.1自动提交
最简单的提交方式是让消费者自动提交偏移量。如果 enable,auto. commit被设为true,那
么每过5s,消费者会自动把从polL()方法接收到的最大偏移量提交上去。提交时间间隔
由auto. commit. interval.ns控制,默认值是5s。与消费者里的其他东西一样,自动提交
也是在轮询里进行的。消费者每次在进行轮询时会检査是否该提交偏移量了,如果是,那
么就会提交从上一次轮询返回的偏移量
不过,在使用这种简便的方式之前,需要知道它将会带来怎样的结果。
假设我们仍然使用默认的5s提交时间间隔,在最近一次提交之后的3s发生了再均衡,再
均衡之后,消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后
了3s,所以在这3s内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地
提交偏移量,减小可能出现重复消息的时间窗,不过这种情况是无法完全避免的。
58第4章在使用自动提交时,每次调用轮询方法都会把上一次调用返回的偏移量提交上去,它并不
知道具体哪些消息已经被处理了,所以在再次调用之前最好确保所有当前调用返回的消息
都已经处理完毕(在调用 cLose()方法之前也会进行自动提交)。一般情况下不会有什么问
题,不过在处理异常或提前退出轮询时要格外小心。
自动提交虽然方便,不过并没有为开发者留有余地来避免重复处理消息。
4.6.2提交当前偏移量
大部分开发者通过控制偏移量提交时间来消除丢失消息的可能性,并在发生再均衡时减少
重复消息的数量。消费者API提供了另一种提交偏移量的方式,开发者可以在必要的时候
提交当前偏移量,而不是基于时间间隔
把auto. commit. offset设为 false,让应用程序决定何时提交偏移量。使用 commitsync()
提交偏移量最简单也最可靠。这个AP会提交由poL()方法返回的最新偏移量,提交成
功后马上返回,如果提交失败就抛出异常。
要记住, commitsync()将会提交由poLL()返回的最新偏移量,所以在处理完所有记录后要
确保调用了 commitsync(),否则还是会有丟失消息的风险。如果发生了再均衡,从最近
批消息到发生再均衡之间的所有消息都将被重复处理。
下面是我们在处理完最近一批消息后使用 commitsync()方法提交偏移量的例子
While(true)[
onsumer Records<String, String> records consumer poll(100);
for(Consumer Record<String, String> record records)
System. out. printf("topic = %s, partition =%s, offset
customer = %s, country
record. topicO, record. partition()
record. offset, record key(), record value());1
try t
consumer. commitSync();2
o catch(CommitFailedException e)t
log error("commit failed",e)
①我们假设把记录内容打印出来就算处理完毕,这个是由应用程序根据具体的使用场景来
决定的
②处理完当前批次的消息,在轮询更多的消息之前,调用 commitsync()方法提交当前批
次最新的偏移量。
8只要没有发生不可恢复的错误, commitsync()方法会一直尝试直至提交成功。如果提交
失败,我们也只能把异常记录到错误日志里
4.6.3异步提交
手动提交有一个不足之处,在 broker对提交请求作出回应之前,应用程序会一直阻塞,这
Kafka消费者—从Kafa读取数据|59样会限制应用程序的吞吐量。我们可以通过降低提交频率来提升吞吐量,但如果发生了再
均衡,会增加重复消息的数量。
这个时候可以使用异步提交API。我们只管发送提交请求,无需等待 broker的响应。
ile(true)[
Consumer Records<String, String> records consumer poll(100);
for (Consumer Record<String, String> record records)
System.out. printf("topic =%, partition =%s
offset=‰d, customer=‰s, country=‰s\n"
record. topic(), record. partition(), record. offset)
record key(), record value))
consumer. commitAsyncO;(1
①提交最后一个偏移量,然后继续做其他事情。
在成功提交或碰到无法恢复的错误之前, commitsync()
会
直重试,但是 commi tAsync()
不会,这也是 commitAsync()不好的一个地方。它之所以不进行重试,是因为在它收到
服务器响应的时候,可能有一个更大的偏移量已经提交成功。假设我们发出一个请求用
于提交偏移量2000,这个时候发生了短暂的通信问题,服务器收不到请求,自然也不会
作出任何响应。与此同时,我们处理了另外一批消息,并成功提交了偏移量3000。如果
commitAsync()重新尝试提交偏移量200,它有可能在偏移量3000之后提交成功。这个时
候如果发生再均衡,就会出现重复消息。
我们之所以提到这个问题的复杂性和提交顺序的重要性,是因为 commitAsync()也支持回
调,在 broker作出响应时会执行回调。回调经常被用于记录提交错误或生成度量指标,不
过如果你要用它来进行重试,一定要注意提交的顺序。
while(true)[
Consumer Records<String, String> records= consumer poll(100)
or (Consumer Record<String, String> record: records)[
System. out. printf( topic %s, partition =%s
%d, customer = %s, country %s\
record. topico)
record key)
record. partition (), record. offset(),
record value ();
consumer.commitasync(newoffsetcommitcallbacK()[
public void onComplete(Map<TopicPartition
offsetAndMetadata> offsets, Exception e)[
f e != null
log error (" Commit failed for offsets []", offsets, e)
});①
①发送提交请求然后继续做其他事情,如果提交失败,错误信息和偏移量会被记录下来
0|第4章重试异步提交
我们可以使用一个单调递增的序列号来维护异步提交的顺序。在每次提交偏
移量之后或在回调里提交偏移量时递增序列号。在进行重试前,先检查回调
的序列号和即将提交的偏移量是否相等,如果相等,说明没有新的提交,那
么可以安全地进行重试。如果序列号比较大,说明有一个新的提交已经发送
出去了,应该停止重试。
4.6.4同步和异步组合提交
般情况下,针对偶尔岀现的提交失败,不进行重试不会有太大问题,因为如果提交失败
是因为临时问题导致的,那么后续的提交总会有成功的。但如果这是发生在关闭消费者或
再均衡前的最后一次提交,就要确保能够提交成功
因此,在消费者关闭前一般会组合使用 commitasync()和 commitsync()。它们的工作原理
如下(后面讲到再均衡监听器时,我们会讨论如何在发生再均衡前提交偏移量)
try t
while (true)I
Consumer Records<String, String> records consumer poll(100);
for(Consumer Record<String, String> record: records)[
System. out. println("topic =%, partition =%S, offset = %d,
customer=%s, country=‰sⅦn",
record. topicO, record. partition()
record. offset, record key(), record value())
consumer. commitAsync(:(1
y catch(Exception e)i
Log error ("Unexpected error", e);
finally t
try f
consumer. commitsync(;2
I finally i
consumer. close o;
①如果一切正常,我们使用 commitasync()方法来提交。这样速度更快,而且即使这次提
交失败,下一次提交很可能会成功。
②如果直接关闭消费者,就没有所谓的“下一次提交”了。使用 commitsync()方法会
直重试,直到提交成功或发生无法恢复的错误。
4.6.5提交特定的偏移量
提交偏移量的频率与处理消息批次的频率是一样的。但如果想要更频繁地提交该怎么办?
如果poL()方法返回一大批数据,为了避免因再均衡引起的重复处理整批消息,想要在批
次中间提交偏移量该怎么办?这种情况无法通过调用 commitsync()或 commi tAsync()来实
Kafka消费者—从 Kafka读取数据61现,因为它们只会提交最后一个偏移量,而此时该批次里的消息还没有处理完。
幸运的是,消费者API允许在调用 commitsync()和 commitAsync()方法时传进去希望提交
的分区和偏移量的map。假设你处理了半个批次的消息,最后一个来自主题“ customers
分区3的消息的偏移量是500,你可以调用 commitsync()方法来提交它。不过,因为消
费者可能不只读取一个分区,你需要跟踪所有分区的偏移量,所以在这个层面上控制偏移
量的提交会让代码变复杂。
下面是提交特定偏移量的例子
private Map<TopicPartition, OffsetAndMetadata> currentoffsets
new HashMap<>O; 1
int count =0
while(true)i
Consumer Records<String, String> records consumer poll(100)
for Consumer Record<string, String> record
records
System. out. printf( topic =%s, partition = %s, offset = %d,
customer =%s, country =%s\n",
record. topic(), record. partition(), record. offset
record key o), record value();2
currentoffsets. put(new TopicPartition(record. topico)
record. partition(),new
offsetAndMetadata(record. offset()+1,"no metadata ));3
f( count%100==0)4
consumer.commitasync(currentoffsets,null);6
count++
①用于跟踪偏移量的map。
2记住, printf只是处理消息的临时方案。
③在读取每条记录之后,使用期望处理的下一个消息的偏移量更新map里的偏移量。下
次就从这里开始读取消息。
今我们决定每处理1000条记录就提交一次偏移量。在实际应用中,你可以根据时间或记
录的内容进行提交。
9这里调用的是 commi aSync(),不过调用 commitsync()也是完全可以的。当然,在提交
特定偏移量时,仍然要处理可能发生的错误。
4.7再均衡监听器
在提交偏移量一节中提到过,消费者在退岀和进行分区再均衡之前,会做一些清理工作。
你会在消费者失去对一个分区的所有权之前提交最后一个已处理记录的偏移量。如果消费
者准备了一个缓冲区用于处理偶发的事件,那么在失去分区所有权之前,需要处理在缓冲
区累积下来的记录。你可能还需要关闭文件句柄、数据库连接等。
62第4章在为消费者分配新分区或移除旧分区时,可以通过消费者API执行一些应用程序代
码,在调用 subscribe()方法时传进去一个 Consumer Rebalancelistener实例就可以了。
Consumer Rebalancelistener有两个需要实现的方法。
(1) public void onPartitionsRevoked( Collection< TopicPartition> partitions)方法会在
再均衡开始之前和消费者停止读取消息之后被调用。如果在这里提交偏移量,下一个接
管分区的消费者就知道该从哪里开始读取了。
(2) public void onPartitionsAssigned( Collection< TopicPartition> partitions)方法会在
重新分配分区之后和消费者开始读取消息之前被调用。
下面的例子将演示如何在失去分区所有权之前通过 onPartitions Revoked()方法来提交偏
移量。在下一节,我们会演示另一个同时使用了 onPartitionsAssigned()方法的例子
private Map<TopicPartition, OffsetAndMetadata> currentoffsets=
new HashMap<>o
private class HandleRebalance implements Consumer RebalanceListener [(1
public void on PartitionsAssigned( Collection<TopicPartition>
partitions){②
public void on PartitionsRevoked Collection<TopicPartition>
partitions )[
System. out. println (" Lost partitions in rebalance
Committing current
offsets: currentoffsets)
consumer. commitSync(currentoffsets);3
try t
consumer subscribe(topics, new HandleRebalance);4
while(true)i
Consumer Records<string, String> records
consumer poll(100)
for ( Consumer Record<String, String> record: records)
System. out. println("topic =%s, partition = %s, offset = %d
customer=‰s, country=‰s
record. topico, record. partition o, record. offset)
record key, record value o)
currentoffsets. put(new TopicPartition(record. topico
record. partition)
offsetAndMetadata(record. offset(+1,"no metadata"))
consumer. commitAsync(currentoffsets, null);
3 catch(Wakeup Exception e)t
/忽略异常,正在关闭消费者
3 catch(Exception e)t
Logerror("Unexpected error", e);
finally f
try t
Kaka消费者—从 Kafka读取数据63consumer. commitSync(currentoffsets)
f finally i
consumer. close;
System. out. println(" Closed consumer and we are done )
①首先实现 Consumer Rebalancelistener接口。
②在获得新分区后开始读取消息,不需要做其他事情。
③如果发生再均衡,我们要在即将失去分区所有权时提交偏移量。要注意,提交的是最近
处理过的偏移量,而不是批次中还在处理的最后一个偏移量。因为分区有可能在我们还
在处理消息的时候被撤回。我们要提交所有分区的偏移量,而不只是那些即将失去所有
权的分区的偏移量——因为提交的偏移量是已经处理过的,所以不会有什么问题。调用
commitsync()方法,确保在再均衡发生之前提交偏移量。
④把 Consumer Rebalancelistener对象传给 subscribe()方法,这是最重要的一步。
4.8从特定偏移量处开始处理记录
到目前为止,我们知道了如何使用pol()方法从各个分区的最新偏移量处开始处理消息。
不过,有时候我们也需要从特定的偏移量处开始读取消息。
如果你想从分区的起始位置开始读取消息,或者直接跳到分区的末尾开始读取消息,可以使
seekToBeginning( Collection<TopicPartition> tp)FA seekToEnd Collection<TopicPartition>
tp)这两个方法。
不过, Kafka也为我们提供了用于查找特定偏移量的API。它有很多用途,比如向后回退
几个消息或者向前跳过几个消息(对时间比较敏感的应用程序在处理滞后的情况下希望能
够向前跳过若干个消息)。在使用 Kafka以外的系统来存储偏移量时,它将给我们带来更
大的惊喜。
试想一下这样的场景:应用程序从Kaka读取事件(可能是网站的用户点击事件流),对
它们进行处理(可能是使用自动程序清理点击操作并添加会话信息),然后把结果保存到
数据库、 NOSQL存储引擎或Hado。假设我们真的不想丢失任何数据,也不想在数据库
里多次保存相同的结果。
这种情况下,消费者的代码可能是这样的
while (true)i
Consumer Records<String, String> records consumer poll(100);
for ( Consumer Record<string, String> record records)
currentoffsets. put(new TopicPartition(record. topico)
record. partition ())
new OffsetAndMetadata(record. offset (+1)
rocessRecord(record)
storeRecordInDB (record)
consumer. commitAsync(currentoffsets):
64|第4章在这个例子里,毎处理一条记录就提交一次偏移量。尽管如此,在记录被保存到数据库之
后以及偏移量被提交之前,应用程序仍然有可能发生崩溃,导致重复处理数据,数据库里
就会出现重复记录。
如果保存记录和偏移量可以在一个原子操作里完成,就可以避免出现上述情况。记录和偏
移量要么都被成功提交,要么都不提交。如果记录是保存在数据库里而偏移量是提交到
Kafka上,那么就无法实现原子操作。
不过,如果在同一个事务里把记录和偏移量都写到数据库里会怎样呢?那么我们就会知道
记录和偏移量要么都成功提交,要么都没有,然后重新处理记录。
现在的问题是:如果偏移量是保存在数据库里而不是 Kafka里,那么消费者在得到新分区
时怎么知道该从哪里开始读取?这个时候可以使用seek()方法。在消费者启动或分配到新
分区时,可以使用seek()方法查找保存在数据库里的偏移量。
下面的例子大致说明了如何使用这个API。使用 ConsumerRebalancelistener和sek()方
法确保我们是从数据库里保存的偏移量所指定的位置开始处理消息的。
public class SaveOffsetsOnRebalance implements
Consumer Reba lancelistener f
public void on PartitionsRevoked Collection<TopicPartition>
partitions)t
commitDBTransaction (; 1
public void onPartitionsAssigned( Collection<TopicPartition>
partitions )I
for(TopicPartition partition partitions)
consumer seek(partition, getoffsetFromDB(partition));2
consumersubscribe (topics, new SaveOffsetonRebalance(consumer))
consumer poll(O);
for (TopicPartition partition: consumer assignment()
consumerseek(partition, getoffset FromDB(partition));3
while (true)i
Consumer RecordsString, String> records
consumer poll(100);
for (Consumer Record<String, String> record records
processRecord(record);
storeRecordInDB (record);
storeOffsetInDB(record. topic(), record. partition(
record. offset());4
commitDBTransaction o:
Kafka消费者—从 Kafka读取数据65①使用一个虚构的方法来提交数据库事务。大致想法是这样的:在处理完记录之后,将记
录和偏移量插入数据库,然后在即将失去分区所有权之前提交事务,确保成功保存了这
些信息。
②使用另一个虚构的方法来从数据库获取偏移量,在分配到新分区的时候,使用seek()
方法定位到那些记录。
③订阅主题之后,开始启动消费者,我们调用一次po(方法,让消费者加入到消费者
群组里,并获取分配到的分区,然后马上调用seek()方法定位分区的偏移量。要记住,
seek()方法只更新我们正在使用的位置,在下一次调用polL()时就可以获得正确的消
息。如果seek()发生错误(比如偏移量不存在),poL()就会抛出异常。
④另一个虚构的方法,这次要更新的是数据库里用于保存偏移量的表。假设更新记录的速
度非常快,所以毎条记录都需要更新一次数据库,但提交的速度比较慢,所以只在每个
批次末尾提交一次。这里可以通过很多种方式进行优化。
通过把偏移量和记录保存到同一个外部系统来实现单次语义可以有很多种方式,不过它们
都需要结合使用 ConsumerRebalancelistener和seek()方法来确保能够及时保存偏移量,
并保证消费者总是能够从正确的位置开始读取消息、。
4.9如何退出
在之前讨论轮询时就说过,不需要担心消费者会在一个无限循环里轮询消息,我们会告诉
消费者如何优雅地退出循环。
如果确定要退出循环,需要通过另一个线程调用 consumer. wakeup()方法。如果循环运行
在主线程里,可以在 ShutdownHook里调用该方法。要记住, consumer. wakeup()是消费者
唯一一个可以从其他线程里安全调用的方法。调用 consumer wakeup()可以退出po(),
并抛出 WakeupException异常,或者如果调用 consumer∴ wakeup()时线程没有等待轮询,那
么异常将在下一轮调用poL()时抛出。我们不需要处理 Wakeup Exception,因为它只是用
于跳出循环的一种方式。不过,在退出线程之前调用 consumer,cose()是很有必要的,它
会提交任何还没有提交的东西,并向群组协调器发送消息,告知自己要离开群组,接下来
就会触发再均衡,而不需要等待会话超时。
下面是运行在主线程上的消费者退出线程的代码。这些代码经过了简化,你可以在这里查
看完整的代码:htp:/btly2u47e9A。
Runtime. getRuntime(). add Hook(new Thread()t
public void run )[
System. out. println("Starting exit...
consumer wakeup ;(1
try t
mainThread. join ():
catch(InterruptedException e)i
e printStackTrace o)
66|第4章try t
/循环,直到按下ctr+C键,关闭的钩子会在退出时进行清理
while(true)
ConsumerRecords<String String> records
movingAvg. consumer poll(1000)
System. out. printIn( System. currentTimeMillis()+
waiting for data . "
for(Consumer Record<String, String> record:
records)i
System. out. printf("offset =%d, key %s,
value %s\n",
record. offset(), record key()
record value )
for(TopicPartition tp: consumer assignment())
System. out. println ("Committing offset at
pos
consumer position( tp));
movingAvg. consumer. commitSyncO
o catch(WakeupException e)i
/忽略关闭异常2
finally i
consumer. close();③
System. out. println("Closed consumer and we are done);
① ShutdownHook运行在单独的线程里,所以退出循环最安全的方式只能是调用 wakeup()
方法
②在另一个线程里调用 wakeup()方法,导致pol()抛出 Wakeup Exception。你可能想捕获
异常以确保应用不会意外终止,但实际上这不是必需的
③在退出之前,确保彻底关闭了消费者。
4.10反序列化器
在之前的章节里提到过,生产者需要用序列化器把对象转换成字节数组再发送给 Kafka
类似地,消费者需要用反序列化器把从Kaka接收到的字节数组转换成Java对象。在
前面的例子里,我们假设每个消息的键值对都是字符串,所以我们使用了默认的 String
Deserializer。
在上一章讲解Kaka生产者的时候,我们已经介绍了如何序列化自定义对象类型,以及如
何使用Avro和 AvroSerializer根据定义好的 schema生成Avo对象。现在来看看如何为
对象自定义反序列化器,以及如何使用Avro和Avro反序列化器。
很显然,生成消息使用的序列化器与读取消息使用的反序列化器应该是一一对应的。使用
Intserializer序列化,然后使用 String Deserializer进行反序列化,会出现不可预测的结果。
对于开发者来说,必须知道写入主题的消息使用的是哪一种序列化器,并确保每个主题里
Kafka消费者—从 Kafka读取数据6只包含能够被反序列化器解析的数据。使用Avo和 schema注册表进行序列化和反序列化
的优势在于: Avroserializer可以保证写入主题的数据与主题的 schema是兼容的,也就是
说,可以使用相应的反序列化器和 schema来反序列化数据。另外,在生产者或消费者里
出现的任何一个与兼容性有关的错误都会被捕捉到,它们都带有消息描述,也就是说,在
出现序列化错误时,就没必要再去调试字节数组了。
尽管不建议使用自定义的反序列化器,我们仍然会简单地演示如何自定义反序列化器,然
后再举例演示如何使用Aⅴo来反序列化消息的键和值。
1.自定义反序列化器
我们以在第3章使用过的自定义对象为例,为它写一个反序列化器。
public class Customer
private int customer ID
rivate String customerName
public Customer(int ID, String name)[
this customer ID ID;
this customer Name name:
public int getID( C
return customer ID
public String getName()[
return customer Name
自定义反序列化器看起来是这样的:
import org. apache. kafka. common errors. Serialization Exception;
import java nio ByteBuffer
import java.util. Map;
public class Customer Deserializer implements
Deserializer <Customer>[1
public void configure( Map configs, boolean iskey)[
//不需要做任何配置
@Override
public Customer deserialize(String topic, byte[] data)i
int id
int namesize
String
try t
if (data =s null)
68第4章return null
if (data length 8)
throw new Serialization Exception ("Size of data received by
Integer Deserializer is shorter than expected");
ByteBuffer buffer ByteBuffer wrap(data)
id buffer getInto)
nameSize buffer getInto
byte[] nameBytes new byte [nameSize];
buffer. get(name Bytes )
name new String (nameBytes," UTF-8 )
return new Customer(id, name);2
3 catch(Exception e)t
throw new Serialization Exception ("Error when serializing
to bytel]
@Override
public void close()i
//不需要关闭任何东西
①消费者也需要使用 Customer类,这个类和序列化器在生产者和消费者应用程序里要相
互匹配。在一个大型的企业里,会有很多消费者和生产者共享这些数据,这对于企业来
说算是一个挑战
②我们把序列化器的逻辑反过来,从字节数组里获取 customer Id和nane,再用它们构建
需要的对象
使用反序列化器的消费者代码看起来是这样的
Properties props new Properties()
props. put("bootstrap servers","
broker 1: 9092, broker 2: 9092
props. put("group id","Country Counter )i
props. put("key. deserializer
org. apache. kafka. common. serial
zation. StringDeserializer )
props. put("value. deserializer
org. apache. kafka. common serialization. CustomerDeserializer
KafkaConsumer<String, Customer> consumer
new KafkaConsumer<>(props);
consumer subscribe( Collections. singletonList("customer Countries"))
hile(true)t
ConsumerRecords<String, Customer> records
consumer poLl(100);
for(Consumer Record<String, Customer> record: records)
Kafka消费者——从 Kafka读取数据69System. out. println ("current customer Id:
record value().getIDO+"and
current customer name:+ record value().getName )
再强调一次,我们并不建议使用自定义序列化器和自定义反序列化器。它们把生产者和消
费者紧紧地耦合在一起,并且很脆弱,容易出错。我们建议使用标准的消息格式,比如
JSON、 Thrift、 Protobuf或Avro。接下来,我们会看到如何在消费者里使用Avro反序列化
器。可以回到第3章查看有关Avro的项目背景、 schema和 schema的兼容能力
2在消费者里进行AVo反序列化
我们在Avo里使用第3章出现过的 Customer类,为了读取这些对象,需要实现一个类似
这样的消费者应用程序
Properties props new Propertieso)
props.put("bootstrap servers","broker1: 9092, broker 2: 9092")
props.put("group id","Country Counter ")
props. put( key serializer
org. apache. kafka. common serialization. StringDeserializer")
props. put(" value. serializer
io confLuent. kafka. serializers. KafkaAvroDeserializer");0
props.put("schema registry. url", schemaUr1);2
String topic =customer Contacts
KafkaConsumer consumer new
KafkaConsumer(create Consumer Config(brokers, groupId, url))
consumersubscribe( Collections. singletonList(topic))
System. out. printLn ("Reading topic: " topic)
while(true)[
Consumer Records<String, Customer> record
consumer poll(1000);3
for( ConsumerRecord<String, Customer> record: records)i
System. out. println("Current customer name is
record value(). getName );4
consumer. commitSynco
①使用 Kafka AvroDeserializer来反序列化Avro消息。
2 schema. registry.ur1是一个新的参数,它指向 schema的存放位置。消费者可以使用由
生产者注册的 schema来反序列化消息
将生成的类 Customer作为值的类型。
4 record. value(返回的是一个 Customer实例,接下来就可以使用它了。
70|第4章4.11独立消费者—为什么以及怎样使用没有
群组的消费者
到目前为止,我们讨论了消费者群组,分区被自动分配给群组里的消费者,在群组里新增
或移除消费者时自动触发再均衡。通常情况下,这些行为刚好是你所需要的,不过有时候
你需要一些更简单的东西。比如,你可能只需要一个消费者从一个主题的所有分区或者某
个特定的分区读取数据。这个时候就不需要消费者群组和再均衡了,只需要把主题或者分
区分配给消费者,然后开始读取消息并提交偏移量。
如果是这样的话,就不需要订阅主题,取而代之的是为自己分配分区。一个消费者可以订
阅主题(并加入消费者群组),或者为自己分配分区,但不能同时做这两件事情。
下面的例子演示了一个消费者是如何为自己分配分区并从分区里读取消息的:
ListsPartitionInfo> partitionInfos null
partitionInfos consumer partitions For("topic");O
f (partitionInfos ! null)t
for(PartitionInfo partition partitionInfos)
partitions. add(new TopicPartition(partition topico
partition partition ( ))
consumer assign(partitions);2
while(true)[
Consumer Records<String, String> records
consumer poll(1000)
for(Consumer Record<String, String> record: records)t
ystem out println("topic =%s, partition =%s, offset =%d,
customer =%s, country %sn
record. topic(), record. partition(), record. offset)
record key), record value())
consumer. commitsynco:
①向集群请求主题可用的分区。如果只打算读取特定分区,可以跳过这一步。
2知道需要哪些分区之后,调用 assign()方法。
除了不会发生再均衡,也不需要手动查找分区,其他的看起来一切正常。不过要记住,如
果主题增加了新的分区,消费者并不会收到通知。所以,要么周期性地调用 consumer
partitions Fo()方法来检查是否有新分区加入,要么在添加新分区后重启应用程序。
4.12旧版的消费者AP
我们在这一章讨论的 Java KafkaConsumer客户端是org. apache. kafka. clients包的一部分。在本
书写到这一章的时候, Kafka还有两个旧版本的 Scala消费者客户端,它们是 kafka. consumer
Kafka消费者—从Kaka读取数据|71包的一部分,属于Kaka核心模块。它们分别被叫作 SimpleConsumer(简单消费者,实际
上也不是那么简单,它们是对 Katka apl的轻度包装,可以用于从特定的分区和偏移量开
始读取消息)和高级消费者。高级消费者指的就是 Zookeeper ConsumerConnector,它有点
像现在的消费者,有消费者群组,有分区再均衡,不过它使用 Zookeeper来管理消费者群
组,并不具备提交偏移量和再均衡的可操控性。
因为现在的消费者同时支持以上两种行为,并且为开发人员提供了更高的可靠性和可操控
性,所以我们不打算讨论旧版AP。如果你想使用它们,那么请三思,如果确定要使用,
可以从Kaka文档中了解更多的信息
4.13总结
我们在本章开头解释了Kaka消费者群组概念,消费者群组支持多个消费者从主题上读取
消息。在介绍完概念之后,我们又给出了一个消费订阅主题并持续读取消息的例子。然后
介绍了一些重要的消费者配置参数以及它们对消费者行为的影响。我们用一大部分内容解
释偏移量以及消费者是如何管理偏移量的。了解消费者提交偏移量的方式有助更好地使用
消费者客户端,所以我们介绍了几种不同的偏移量提交方式。然后又探讨了消费者AP的
其他话题,比如如何处理再均衡以及如何关闭消费者。
最后,我们介绍了反序列化器,消费者客户端使用反序列化器将保存在Kaka里的字节转
换成应用程序可以处理的Java对象。我们主要介绍了Avro反序列化器,虽然有很多可用
的反序列化器,但在Kaka里,Avro是最为常用的一个。
现在,我们已经知道如何生成和读取Kaka消息,下一章将介绍Kaka内部的实现细节。
72第4章第5章
深入 Kafka
如果只是为了开发Kaka应用程序,或者只是在生产环境使用Kaka,那么了解Kaka的
内部工作原理不是必需的。不过,了解一个设计和实现细节,而是集中讨论以下
ka的内部工作原理有助于理解Kaka的行为,
也有助于诊断问题。本章并不会涵盖 Kafka
3个有意思的话题
Kafka如何进行复制
Kaka如何处理来自生产者和消费者的请求;
Kaka的存储细节,比如文件格式和索引。
在对Kaka进行调优时,深入理解这些问题是很有必要的。了解了内部机制,可以更有目
的性地进行深入的调优,而不只是停留在表面,隔靴搔痒。
5.1集群成员关系
Kaka使用 Zookeeper维护集群成员的信息。每个 broker都有一个唯一标识符,这个
标识符可以在配置文件里指定,也可以自动生成。在 broker启动的时候,它通过创建
临时节点把自己的ID注册到 Zookeeper Kafka组件订阅 Zookeeper的/ /brokers/ids路径
( broker在 Zookeeper上的注册路径),当有 broker加入集群或退出集群时,这些组件就
可以获得通知。
如果你要启动另一个具有相同ID的 broker,会得到一个错误—新 broker会试着进行注
册,但不会成功,因为 Zookeeper E已经有一个具有相同I的 broker.
在 broker停机、出现网络分区或长时间垃圾回收停顿时, broker会从 Zookeeper上断开连
接,此时bker在启动时创建的临时节点会自动从 Zookeeper上移除。监听 broker列表的
Kafka组件会被告知该 broker已移除。在关闭 broker时,它对应的节点也会消失,不过它的I会继续存在于其他数据结构中。
例如,主题的副本列表(下面会介绍)里就可能包含这些ID。在完全关闭一个 broker之
后,如果使用相同的⑩启动另一个全新的 broker,它会立即加入集群,并拥有与旧 broker
相同的分区和主题。
52控制器
控制器其实就是一个 broker,只不过它除了具有一般 broker的功能之外,还负责分区
首领的选举(我们将在5.3节讨论分区首领选举)。集群里第一个启动的 broker通过在
Zookeeper里创建一个临时节点/ controller让自己成为控制器。其他 broker在启动时也
会尝试创建这个节点,不过它们会收到一个“节点已存在”的异常,然后“意识”到控制
器节点已存在,也就是说集群里已经有一个控制器了。其他 broker在控制器节点上创建
Zookeeper watch对象,这样它们就可以收到这个节点的变更通知。这种方式可以确保集群
里一次只有一个控制器存在。
如果控制器被关闭或者与 Zookeeper断开连接, Zookeeper上的临时节点就会消失。集群
里的其他 broker通过 watch对象得到控制器节点消失的通知,它们会尝试让自己成为新的
控制器。第一个在 Zookeeper里成功创建控制器节点的 broker就会成为新的控制器,其他
节点会收到“节点已存在”的异常,然后在新的控制器节点上再次创建 watch对象。每个
新选出的控制器通过 Zookeeper的条件递增操作获得一个全新的、数值更大的 controller
epch。其他 broker在知道当前 controller. epoch后,如果收到由控制器发出的包含较旧
epoch的消息,就会忽略它们。
当控制器发现一个 broker已经离开集群(通过观察相关的 Zookeeper路径),它就知道,那
些失去首领的分区需要一个新首领(这些分区的首领刚好是在这个 broker上)。控制器遍
历这些分区,并确定谁应该成为新首领(简单来说就是分区副本列表里的下一个副本),
然后向所有包含新首领或现有跟随者的 broker发送请求。该请求消息包含了谁是新首领以
及谁是分区跟随者的信息。随后,新首领开始处理来自生产者和消费者的请求,而跟随者
开始从新首领那里复制消息
当控制器发现一个 broker加入集群时,它会使用 broker iD来检查新加入的 broker是否包
含现有分区的副本。如果有,控制器就把变更通知发送给新加入的 broker和其他 broker,
新 broker上的副本开始从首领那里复制消息。
简而言之,Kaka使用 Zookeeper的临时节点来选举控制器,并在节点加入集群或退出集
群时通知控制器。控制器负责在节点加入或离开集群时进行分区首领选举。控制器使用
epoch来避免“脑裂”。“脑裂”是指两个节点同时认为自己是当前的控制器。
5.3复制
复制功能是Kaka架构的核心。在Kaka的文档里,Kaka把自己描述成“一个分布式的
可分区的、可复制的提交日志服务”。复制之所以这么关键,是因为它可以在个别节点失
效时仍能保证 Kafka的可用性和持久性。
74|第5章Kafka使用主题来组织数据,每个主题被分为若干个分区,每个分区有多个副本。那些副
本被保存在 broker上,每个 broker可以保存成百上千个属于不同主题和分区的副本
副本有以下两种类型。
首领副本
每个分区都有一个首领副本。为了保证一致性,所有生产者请求和消费者请求都会经过
这个副本。
跟随者副本
首领以外的副本都是跟随者副本。跟随者副本不处理来自客户端的请求,它们唯一的任
务就是从首领那里复制消息,保持与首领一致的状态。如果首领发生崩溃,其中的一个
跟随者会被提升为新首领。
首领的另一个任务是搞清楚哪个跟随者的状态与自己是一致的。跟随者为了保持与首领的
状态一致,在有新消息到达时尝试从首领那里复制消息,不过有各种原因会导致同步失
败。例如,网络拥塞导致复制变慢, broker发生崩溃导致复制滞后,直到重启 broker后复
制才会继续。
为了与首领保持同步,跟随者向首领发送获取数据的请求,这种请求与消费者为了读取消
息而发送的请求是一样的。首领将响应消息发给跟随者。请求消息里包含了跟随者想要获
取消息的偏移量,而且这些偏移量总是有序的。
个跟随者副本先请求消息1,接着请求消息2,然后请求消息3,在收到这3个请求的响
应之前,它是不会发送第4个请求消息的。如果跟随者发送了请求消息4,那么首领就知
道它已经收到了前面3个请求的响应。通过査看毎个跟随者请求的最新偏移量,首领就会
知道毎个跟随者复制的进度。如果跟随者在10s内没有请求任何消息,或者虽然在请求消
息,但在10s内没有请求最新的数据,那么它就会被认为是不同步的。如果一个副本无法
与首领保持一致,在首领发生失效时,它就不可能成为新首领——毕竟它没有包含全部的
消息。
相反,持续请求得到的最新消息副本被称为同步的副本。在首领发生失效时,只有同步副
本才有可能被选为新首领。
跟随者的正常不活跃时间或在成为不同步副本之前的时间是通过 replica.Lag. time. max.ns
参数来配置的。这个时间间隔直接影响着首领选举期间的客户端行为和数据保留机制。我
们将在第6章讨论可靠性保证,到时候会深入讨论这个问题。
除了当前首领之外,每个分区都有一个首选首领—创建主题时选定的首领就是分区的首
选首领。之所以把它叫作首选首领,是因为在创建分区时,需要在 broker之间均衡首领
(后面会介绍在 broker间分布副本和首领的算法)。因此,我们希望首选首领在成为真正的
首领时, broker间的负载最终会得到均衡。默认情况下,Kaka的ato. Leader. reba lance
enable被设为true,它会检查首选首领是不是当前首领,如果不是,并且该副本是同步
的,那么就会触发首领选举,让首选首领成为当前首领。
深入 Kafka|75找到首选首领
从分区的副本清单里可以很容易找到首选首领(可以使用 kafka. topics. sh工
具查看副本和分区的详细信息,我们将在第10章介绍管理工具)。清单里的
第一个副本一般就是首选首领。不管当前首领是哪一个副本,都不会改变这
个事实,即使使用副本分配工具将副本重新分配给其他 broker。要记住,如
果你手动进行副本分配,第一个指定的副本就是首选首领,所以要确保首选
首领被传播到其他 broker上,避免让包含了首领的 broker负载过重,而其他
broker却无法为它们分担负载
5.4处理请求
broker的大部分工作是处理客户端、分区副本和控制器发送给分区首领的请求。Kaka提
供了一个二进制协议(基于TCP),指定了请求消息的格式以及 broker如何对请求作出
响应——包括成功处理请求或在处理请求过程中遇到错误。客户端发起连接并发送请求,
broker处理请求并作出响应。 broker按照请求到达的顺序来处理它们这种顺序保证让
Kaka具有了消息队列的特性,同时保证保存的消息也是有序的。
所有的请求消息都包含一个标准消息头
Request type(也就是 API key)
Request version( broker可以处理不同版本的客户端请求,并根据客户端版本作出不同
的响应)
Correlation ID
个具有唯一性的数字,用于标识请求消息,同时也会出现在响应消
息和错误日志里(用于诊断问题)
Client Id用于标识发送请求的客户端
我们不打算在这里描述该协议,因为在Kaka文档里已经有很详细的说明。不过,了解
broker如何处理请求还是有必要的—后面在我们讨论 Kafka监控和各种配置选项时,你
就会了解到那些与队列和线程有关的度量指标和配置参数。
broker会在它所监听的每一个端口上运行一个 Acceptor线程,这个线程会创建一个连接,
并把它交给 Processor线程去处理。 Processor线程(也被叫作“网络线程”)的数量是可
配置的。网络线程负责从客户端获取请求消息,把它们放进请求队列,然后从响应队列获
取响应消息,把它们发送给客户端。图5-1为Kaka处理请求的内部流程。
请求消息被放到请求队列后,O线程会负责处理它们。下面是几种最常见的请求类型。
生产请求
生产者发送的请求,它包含客户端要写入 broker的消息
获取请求
在消费者和跟随者副本需要从 broker读取消息时发送的请求。
76|第5章连接
/Processor
请求队列
线程
响
]应队列
线程
图5-1: Kafka处理请求的内部流程
生产请求和获取请求都必须发送给分区的首领副本。如果 broker收到一个针对特定分区的
请求,而该分区的首领在另一个 broker上,那么发送请求的客户端会收到一个“非分区
首领”的错误响应。当针对特定分区的获取请求被发送到一个不含有该分区首领的 broker
上,也会出现同样的错误。 Kafka客户端要自己负责把生产请求和获取请求发送到正确的
broker上。
那么客户端怎么知道该往哪里发送请求呢?客户端使用了另一种请求类型,也就是元数据
请求。这种请求包含了客户端感兴趣的主题列表。服务器端的响应消息里指明了这些主题
所包含的分区、每个分区都有哪些副本,以及哪个副本是首领。元数据请求可以发送给任
意一个 broker,因为所有 broker都缓存了这些信息。
般情况下,客户端会把这些信息缓存起来,并直接往目标 broker上发送生产请求和
获取请求。它们需要时不时地通过发送元数据请求来刷新这些信息(刷新的时间间隔通
过 metadata, max,age,ns参数来配置),从而知道元数据是否发生了变更
比如,在新
broker加入集群时,部分副本会被移动到新的 broker上(如图5-2所示)。另外,如果客户
端收到“非首领”错误,它会在尝试重发请求之前先刷新元数据,因为这个错误说明了客
户端正在使用过期的元数据信息,之前的请求被发到了错误的 broker上。
客户端
任意 broker
元数据请求
元数据
元数据
缓存
缓存
元数据响应
到分区0的生产者请求分区0的首
领所在的
从分区区回给生产者的购应(认 broker
图52:客户端路由请求
深入 Kafka|775.4.1生产请求
我们在第3章讨论如何配置生产者的时候,提到过acks这个配置参数——该参数指定了需
要多少个 broker确认才可以认为一个消息写入是成功的。不同的配置对“写入成功”的界
定是不一样的,如果acks=1,那么只要首领收到消息就认为写入成功;如果acks=aL,那
么需要所有同步副本收到消息才算写入成功;如果acks=0,那么生产者在把消息发出去之
后,完全不需要等待 broker的响应。
包含首领副本的 broker在收到生产请求时,会对请求做一些验证。
发送数据的用户是否有主题写入权限?
请求里包含的acks值是否有效(只允许出现0、1或aLL)?
·如果acks=aLl,是否有足够多的同步副本保证消息已经被安全写入?(我们可以对
broker进行配置,如果同步副本的数量不足, broker可以拒绝处理新消息。在第6章介
绍Kaka持久性和可靠性保证时,我们会讨论更多这方面的细节。)
之后,消息被写入本地磁盘。在 Linux系统上,消息会被写到文件系统缓存里,并不保证
它们何时会被刷新到磁盘上。Kaka不会一直等待数据被写到磁盘上—它依赖复制功能
来保证消息的持久性。
在消息被写入分区的首领之后, broker开始检查acks配置参数——如果acks被设为θ或1
那么 broker立即返回响应;如果acks被设为al,那么请求会被保存在一个叫作炼狱的缓冲
区里,直到首领发现所有跟随者副本都复制了消息,响应才会被返回给客户端。
5.42获取请求
broker处理获取请求的方式与处理生产请求的方式很相似。客户端发送请求,向 broker请
求主题分区里具有特定偏移量的消息,好像在说:“请把主题Test分区0偏移量从53开始
的消息以及主题Test分区3偏移量从64开始的消息发给我。”客户端还可以指定 broker最
多可以从一个分区里返回多少数据。这个限制是非常重要的,因为客户端需要为 broker返
回的数据分配足够的内存。如果没有这个限制, broker返回的大量数据有可能耗尽客户端
的内存。
我们之前讨论过,请求需要先到达指定的分区首领上,然后客户端通过查询元数据来确保
请求的路由是正确的。首领在收到请求时,它会先检査请求是否有效—一比如,指定的偏
移量在分区上是否存在?如果客户端请求的是已经被删除的数据,或者请求的偏移量不存
在,那么 broker将返回一个错误。
如果请求的偏移量存在, broker将按照客户端指定的数量上限从分区里读取消息,再把消
息返回给客户端。 Kafka使用零复制技术向客户端发送消息——也就是说, Kafka直接把消
息从文件(或者更确切地说是 Linux文件系统缓存)里发送到网络通道,而不需要经过任
何中间缓冲区。这是Kaka与其他大部分数据库系统不一样的地方,其他数据库在将数据
发送给客户端之前会先把它们保存在本地缓存里。这项技术避免了字节复制,也不需要管
理内存缓冲区,从而获得更好的性能
78第5章客户端除了可以设置 broker返回数据的上限,也可以设置下限。例如,如果把下限设置为
10KB,就好像是在告诉 broker:“等到有10KB数据的时候再把它们发送给我。”在主题消
息流量不是很大的情况下,这样可以减少CPU和网络开销。客户端发送一个请求, broker
等到有足够的数据时才把它们返回给客户端,然后客户端再发出请求,而不是让客户端每
隔几毫秒就发送一次请求,每次只能得到很少的数据甚至没有数据。(如图5-3所示。)对
比这两种情况,它们最终读取的数据总量是一样的,但前者的来回传送次数更少,因此开
销也更小。
生产者
消息
broker
获取请求
消费者
消息
等待累积
足够多的
消息
消息
图5-3: broker延迟作出响应以便累积足够的数据
当然,我们不会让客户端一直等待 broker累积数据。在等待了一段时间之后,就可以把
可用的数据拿回处理,而不是一直等待下去。所以,客户端可以定义一个超时时间,告
诉 broker:“如果你无法在X毫秒内累积满足要求的数据量,那么就把当前这些数据返回
给我。”
有意思的是,并不是所有保存在分区首领上的数据都可以被客户端读取。大部分客户端只
能读取已经被写入所有同步副本的消息(跟随者副本也不行,尽管它们也是消费者——一否
则复制功能就无法工作)。分区首领知道毎个消息会被复制到哪个副本上,在消息还没有
被写入所有同步副本之前,是不会发送给消费者的——尝试获取这些消息的请求会得到空
的响应而不是错误。
因为还没有被足够多副本复制的消息被认为是“不安全”的—如果首领发生崩溃,另
个副本成为新首领,那么这些消息就丢失了。如果我们允许消费者读取这些消息,可能就
会破坏一致性。试想,一个消费者读取并处理了这样的一个消息,而另一个消费者发现这
个消息其实并不存在。所以,我们会等到所有同步副本复制了这些消息,才允许消费者读
取它们(如图5-4所示)。这也意味着,如果 broker间的消息复制因为某些原因变慢,那
么消息到达消费者的时间也会随之变长(因为我们会先等待消息复制完毕)。延迟时间可
以通过参数 replica.Lag.time.nax,ns来配置,它指定了副本在复制消息时可被允许的最大
延迟时间。
深入 Kafka副本0
副本1
副本2
消息0
消息0
消息0
消费者只能看
到这些消息
消息1
消息1
消息1
消息2
消息2
消息2
高水位
消息
消息3
消息4
图5-4:消费者只能看到已经复制到R的消息
54.3其他请求
到此为止,我们讨论了Kaka最为常见的几种请求类型:元数据请求、生产请求和获取
请求。重要的是,我们讨论的是客户端在网络上使用的通用二进制协议。 Kafka内置了由
开源社区贡献者实现和维护的Java客户端,同时也有用其他语言实现的客户端,如C、
Python、Go语言等。Kaka网站上有它们的完整清单,这些客户端就是使用这个二进制协
议与 broker通信的。
另外, broker之间也使用同样的通信协议。它们之间的请求发生在Kaka内部,客户端不
应该使用这些请求。例如,当一个新首领被选举出来,控制器会发送 Leader andisr请求给
新首领(这样它就可以开始接收来自客户端的请求)和跟随者(这样它们就知道要开始跟
随新首领)。
在我们写这本书的时候,Kaka协议可以处理20种不同类型的请求,而且会有更多的类
型加入进来。协议在持续演化—随着客户端功能的不断增加,我们需要改进协议来满足
需求。例如,之前的Kaka消费者使用 Zookeeper来跟踪偏移量,在消费者启动的时候,
它通过检査保存在 Zookeeper上的偏移量就可以知道从哪里开始处理消息。因为各种原
因,我们决定不再使用 Zookeeper来保存偏移量,而是把偏移量保存在特定的Kaka主题
上。为了达到这个目的,我们不得不往协议里增加几种请求类型:0 ffsetcommitRequest、
0 ffsetFetchRequest和 ListoffsetsRequest。现在,在应用程序调用 commitoffset()方法
时,客户端不再把偏移量写入 Zookeeper,而是往 Kafka发送 offsetCommitRequest请求。
主题的创建仍然需要通过命令行工具来完成,命令行工具会直接更新 Zookeeper里的主题
列表, broker监听这些主题列表,在有新主题加入时,它们会收到通知。我们正在改进
Kaka,增加了 Create TopicRequest请求类型,这样客户端(包括那些不支持 Zookeeper客
户端的编程语言)就可以直接向 broker请求创建新主题了。
除了往协议里增加新的请求类型外,我们也会通过修改已有的请求类型来给它们增加新功
能。例如,从 Kafka0.90到 Kafka0.10.0,我们希望能够让客户端知道谁是当前的控制器,
于是把控制器信息添加到元数据响应消息里。我们还在元数据请求消息和响应消息里添加了
个新的 version字段。现在,0.9.0版本的客户端发送的元数据请求里 version为0(0.9.0版
本客户端的 version不会是1)。不管是09.0版本的 broker,还是0.10.0版本的 broker,它们
都知道应该返回 version为0的响应,也就是不包含控制器信息的响应。0.9.0版本的客户端
第5章不需要控制器的信息,而且也没必要知道如何去解析它。0.10.0版本的客户端会发送 version
为1的元数据请求,0.10.0版本的 broker会返回 version为1的响应,里面包含了控制器的
信息。如果0.10.0版本的客户端发送 version为1的请求给090版本的 broker,这个版本的
broker不知道该如何处理这个请求,就会返回一个错误。这就是为什么我们建议在升级客户
端之前先升级 broker,因为新的 broker知道如何处理旧的请求,反过来则不然。
我们在0.10.0版本的Kaka里加入了 ApiversionRequest-—客户端可以询问 broker支持哪
些版本的请求,然后使用正确的版本与 broker通信。如果能够正确使用这个新功能,客户
端就可以与旧版本的 broker通信,只要 broker支持这个版本的协议。
5.5物理存储
Kafka的基本存储单元是分区。分区无法在多个 broker间进行再细分,也无法在同一个
broker的多个磁盘上进行再细分。所以,分区的大小受到单个挂载点可用空间的限制(
个挂载点由单个磁盘或多个磁盘组成,如果配置了JBOD,就是单个磁盘,如果配置了
RAD,就是多个磁盘。请参考第2章)。
在配置Kaka的时候,管理员指定了一个用于存储分区的目录清单—也就是 log dirs参
数的值(不要把它与存放错误日志的目录混淆了,日志目录是配置在log4 1. properties文件
里的)。该参数一般会包含每个挂载点的目录。
接下来我们会介绍Kaka是如何使用这些目录来存储数据的。首先,我们要知道数据是如
何被分配到集群的 broker上以及 broker的目录里的。然后,我们还要知道 broker是如何管
理这些文件的,特别是如何进行数据保留的。随后,我们会深入探讨文件和索引格式。最
后,我们会讨论日志压缩及其工作原理。日志压缩是Kaka的一个高级特性,因为有了这
个特性, Kafka可以用来长时间地保存数据。
5.51分区分配
在创建主题时, Kafka首先会决定如何在 broker间分配分区。假设你有6个 broker,打算
创建一个包含10个分区的主题,并且复制系数为3。那么 Kafka就会有30个分区副本,
它们可以被分配给6个 broker。在进行分区分配时,我们要达到如下的目标。
在 broker间平均地分布分区副本。对于我们的例子来说,就是要保证每个 broker可以
分到5个副本。
确保每个分区的每个副本分布在不同的 broker上。假设分区0的首领副本在 broker2上,
那么可以把跟随者副本放在 broker3和 broker4上,但不能放在 broker2上,也不能两
个都放在 broker3上。
·如果为 broker指定了机架信息,那么尽可能把每个分区的副本分配到不同机架的 broker
上。这样做是为了保证一个机架的不可用不会导致整体的分区不可用。
为了实现这个目标,我们先随机选择一个 broker(假设是4),然后使用轮询的方式给每
个 broker分配分区来确定首领分区的位置。于是,首领分区0会在 broker4上,首领分区
1会在 broker5上,首领分区2会在 broker0上(只有6个 broker),并以此类推。然后,
深入 Kafka|81我们从分区首领开始,依次分配跟随者副本。如果分区0的首领在 broker4上,那么它的
第一个跟随者副本会在 broker5上,第二个跟随者副本会在 broker0上。分区1的首领在
broker5上,那么它的第一个跟随者副本在 broker o上,第二个跟随者副本在 broker1上。
如果配置了机架信息,那么就不是按照数字顺序来选择 broker了,而是按照交替机架的方式
来选择 broker。假设 broker0、 broker1和 broker2放置在同一个机架上, broker3、 broker4
和 broker5分别放置在其他不同的机架上。我们不是按照从0到5的顺序来选择 broker,而
是按照0,3,1,4,2,5的顺序来选择,这样每个相邻的 broker都在不同的机架上(如图
5-5所示)。于是,如果分区0的首领在 broker4上,那么第一个跟随者副本会在 broker2上,
这两个 broker在不同的机架上。如果第一个机架下线,还有其他副本仍然活跃着,所以分区
仍然可用。这对所有副本来说都是一样的,因此在机架下线时仍然能够保证可用性
机架1
机架2
broker
broker
broker l
broker
图55:分配给不同机架 broker的分区和副本
为分区和副本选好合适的 broker之后,接下来要决定这些分区应该使用哪个目录。我们单
独为每个分区分配目录,规则很简单:计算每个目录里的分区数量,新的分区总是被添加
到数量最小的那个目录里。也就是说,如果添加了一个新磁盘,所有新的分区都会被创建
到这个磁盘上。因为在完成分配工作之前,新磁盘的分区数量总是最少的。
注意磁盘空间
要注意,在为 broker分配分区时并没有考虑可用空间和工作负载问题,但在
将分区分配到磁盘上时会考虑分区数量,不过不考虑分区大小。也就是说,
如果有些 broker的磁盘空间比其他 broker要大(有可能是因为集群同时使
用了旧服务器和新服务器),有些分区异常大,或者同一个 broker上有大小
不同的磁盘,那么在分配分区时要格外小心。在后面的章节中,我们会讨论
Kafka管理员该如何解决这种 broker负载不均衡的问题。
5.52文件管理
保留数据是 Kafka的一个基本特性, Kafka不会一直保留数据,也不会等到所有消费者都
读取了消息之后才删除消息。相反, Kafka管理员为毎个主题配置了数据保留期限,规定
数据被删除之前可以保留多长时间,或者清理数据之前可以保留的数据量大小。
82第5章因为在一个大文件里査找和删除消息是很费时的,也很容易出错,所以我们把分区分成若
干个片段。默认情况下,毎个片段包含1GB或一周的数据,以较小的那个为准。在 broker
往分区写入数据时,如果达到片段上限,就关闭当前文件,并打开一个新文件。
当前正在写入数据的片段叫作活跃片段。活动片段永远不会被删除,所以如果你要保留数
椐1天,但片段里包含了5天的数据,那么这些数据会被保留5天,因为在片段被关闭之
前这些数据无法被删除。如果你要保留数据一周,而且每天使用一个新片段,那么你就会
看到,每天在使用一个新片段的同时会删除一个最老的片段—所以大部分时间该分区会
有7个片段存在。
我们在第2章讲过, broker会为分区里的每个片段打开一个文件句柄,哪怕片段是不活跃
的。这样会导致打开过多的文件句柄,所以操作系统必须根据实际情况做一些调优。
5.5.3文件格式
我们把 Kafka的消息和偏移量保存在文件里。保存在磁盘上的数据格式与从生产者发送过
来或者发送给消费者的消息格式是一样的。因为使用了相同的消息格式进行磁盘存储和网
络传输, Kafka可以使用零复制技术给消费者发送消息,同时避免了对生产者已经压缩过
的消息进行解压和再压缩。
除了键、值和偏移量外,消息里还包含了消息大小、校验和、消息格式版本号、压缩算法
( Snappy、Gzip或LZ4)和时间戳(在0.10.0版本里引入的)。时间戳可以是生产者发送消
息的时间,也可以是消息到达 broker的时间,这个是可配置的。
如果生产者发送的是压缩过的消息,那么同一个批次的消息会被压缩在一起,被当作“包
装消息”进行发送(如图5-6所示)。于是, broker就会收到一个这样的消息,然后再把它
发送给消费者。消费者在解压这个消息之后,会看到整个批次的消息,它们都有自己的时
间戳和偏移量。
消息
偏移量魔术数压缩和解国时间戳键的
大小/值的
大小
值
压缩
和解
偏移量魔术数压缩和解压时间戳值的移受米缩
付数的
大小
的
包装消息包含了
3个压缩过的消息
图56:普通消息和包装消息
也就是说,如果在生产者端使用了压缩功能(极力推荐),那么发送的批次越大,就意味
着在网络传输和磁盘存储方面会获得越好的压缩性能,同时意味着如果修改了消费者使用
的消息格式(例如,在消息里增加了时间戳),那么网络传输和磁盘存储的格式也要随之
深入 Kafka83修改,而且bker要知道如何处理包含了两种消息格式的文件。
Kaka附带了一个叫 DumpLog Segment的工具,可以用它查看片段的内容。它可以显示每
个消息的偏移量、校验和、魔术数字节、消息大小和压缩算法
bin/ /kafka-run: class. sh kafka. tools. DumpLog Segments运行该工具的方法如下:
如果使用了-deep- iteration参数,可以显示被压缩到包装消息里的消息。
5.54索引
消费者可以从 Kafka的任意可用偏移量位置开始读取消息。假设消费者要读取从偏移量100
开始的1MB消息,那么 broker必须立即定位到偏移量100(可能是在分区的任意一个片段
里),然后开始从这个位置读取消息。为了帮助 broker更快地定位到指定的偏移量, Kafka
为毎个分区维护了一个索引。索引把偏移量映射到片段文件和偏移量在文件里的位置
索引也被分成片段,所以在删除消息时,也可以删除相应的索引。 Kafka不维护索引的
校验和。如果索引岀现损坏, Kafka会通过重新读取消息并录制偏移量和位置来重新生
成索引。如果有必要,管理员可以删除索引,这样做是绝对安全的,Kaka会自动重新
生成这些索引。
5.55清理
一般情况下, Kafka会根据设置的时间保留数据,把超过时效的旧数据删除掉。不过,试
想一下这样的场景,如果你使用 Kafka保存客户的收货地址,那么保存客户的最新地址比
保存客户上周甚至去年的地址要有意义得多,这样你就不用担心会用错旧地址,而且短时
间内客户也不会修改新地址。另外一个场景,一个应用程序使用Kaka保存它的状态,每
次状态发生变化,它就把状态写入 Kafka。在应用程序从崩溃中恢复时,它从 Kafka读取
消息来恢复最近的状态。在这种情况下,应用程序只关心它在崩溃前的那个状态,而不关
心运行过程中的那些状态。
Kafka通过改变主题的保留策略来满足这些使用场景。早于保留时间的旧事件会被删除,
为每个键保留最新的值,从而达到清理的效果。很显然,只有当应用程序生成的事件里
包含了键值对时,为这些主题设置 compact策略才有意义。如果主题包含nuL键,清理
就会失败。
5.5.6清理的工作原理
每个日志片段可以分为以下两个部分。
干净的部分
这些消息之前被清理过,每个键只有一个对应的值,这个值是上一次清理时保留下来的。
污浊的部分
这些消息是在上一次清理之后写入的。
两个部分的日志片段示意如图5-7所示。
84|第5章这一部分是“干净”的
这部分是日志的
注意这里缺失了一些
“污浊”部分,稍
偏移量,它们是被清
后会被清理。
理掉了。
图57:包含干净和污浊两个部分的分区
如果在Kaka启动时启用了清理功能(通过配置Log. cLeaner, enabled参数),每个 broker
会启动一个清理管理器线程和多个清理线程,它们负责执行清理任务。这些线程会选择污
浊率(污浊消息占分区总大小的比例)较高的分区进行清理。
为了清理分区,清理线程会读取分区的污浊部分,并在内存里创建一个 mapo map里的
毎个元素包含了消息键的散列值和消息的偏移量,键的散列值是16B,加上偏移量总共是
24B。如果要清理一个1GB的日志片段,并假设每个消息大小为1KB,那么这个片段就包
含一百万个消息,而我们只需要用24MB的map就可以清理这个片段。(如果有重复的键,
可以重用散列项,从而使用更少的内存。)这是非常高效的!
管理员在配置Kaka时可以对map使用的内存大小进行配置。每个线程都有自己的map,
而这个参数指的是所有线程可使用的内存总大小。如果你为map分配了1GB内存,并使
用了5个清理线程,那么每个线程可以使用200MB内存来创建自己的map。Kaka并不要
求分区的整个污浊部分来适应这个map的大小,但要求至少有一个完整的片段必须符合
如果不符合,那么Kaka就会报错,管理员要么分配更多的内存,要么减少清理线程数
量。如果只有少部分片段可以完全符合,Kaka将从最旧的片段开始清理,等待下一次清
理剩余的部分。
凊理线程在创建好偏移量map后,开始从干净的片段处读取消息,从最旧的消息开始,把
它们的内容与map里的内容进行比对。它会检査消息的键是否存在于map中,如果不存在,
那么说明消息的值是最新的,就把消息复制到替换片段上。如果键已存在,消息会被忽略
因为在分区的后部已经有一个具有相同键的消息存在。在复制完所有的消息之后,我们就将
替换片段与原始片段进行交换,然后开始清理下一个片段。完成整个清理过程之后,每个键
对应一个不同的消息—这些消息的值都是最新的。清理前后的分区片段如图5-8所示。
图58:清理前后的分区片段
深入 Kafka|85.57被删除的事件
如果只为每个键保留最近的一个消息,那么当需要删除某个特定键所对应的所有消息时,
我们该怎么办?这种情况是有可能发生的,比如一个用户不再使用我们的服务,那么完全
可以把与这个用户相关的所有信息从系统中删除。
为了彻底把一个键从系统里删除,应用程序必须发送一个包含该键且值为nuL的消息。清
理线程发现该消息时,会先进行常规的清理,只保留值为nuL的消息。该消息(被称为墓
碑消息)会被保留一段时间,时间长短是可配置的。在这期间,消费者可以看到这个墓碑
消息,并且发现它的值已经被删除。于是,如果消费者往数据库里复制Kaka的数据,当
它看到这个墓碑消息时,就知道应该要把相关的用户信息从数据库里删除。在这个时间段
过后,清理线程会移除这个墓碑消息,这个键也将从Kaka分区里消失。重要的是,要留
给消费者足够多的时间,让他看到墓碑消息,因为如果消费者离线几个小时并错过了墓碑
消息,就看不到这个键,也就不知道它已经从Kaka里删除,从而也就不会去删除数据库
里的相关数据了。
558何时会清理主题
就像 delete策略不会删除当前活跃的片段一样,coηpact策略也不会对当前片段进行凊理。
只有旧片段里的消息才会被清理。
在0.100和更早的版本里,Kaka会在包含脏记录的主题数量达到50%时进行清理。这样
做的目的是避免太过频繁的清理(因为清理会影响主题的读写性能),同时也避免存在太
多脏记录(因为它们会占用磁盘空间)。浪费50%的磁盘空间给主题存放脏记录,然后进
行一次清理,这是个合理的折中,管理员也可以对它进行调整。
我们计划在未来的版本中加入宽限期,在宽限期内,我们保证消息不会被清理。对于想看
到主题的每个消息的应用程序来说,它们就有了足够的时间,即使时间有点滞后。
59总结
我们无法在这一章里涵盖所有的内容,但希望大家能够对我们在这个项目上所做的设计和
优化有所了解,同时本章也为大家解释了在使用Kaka时可能碰到的一些晦涩难懂的现象
和参数配置问题。
如果大家真的对 Kafka内部原理感兴趣,唯一的途径是阅读它的源代码。 Kafka开发者邮
件组(dev@ kafka. apache. org)是一个非常友好的社区,在那里会有人回答有关Kaka工作
原理的问题。或许你在阅读源代码时还能够修复一些缺陷——开源社区的大门总是向贡献
者敞开。
86第5章第6章
可靠的数据传递
对于系统来说,可靠的数据传递不能成为马后炮。与性能一样,在系统的设计之初就应该
考虑可靠性问题,而不能在事后才来考虑。而且,可靠性是系统的一个属性,而不是
个独立的组件,所以在讨论 Kafka的可靠性保证时,还是要从系统的整体出发。说到可靠
性,那些与Kaka集成的系统与 Kafka本身一样重要。正因为可靠性是系统层面的概念,
所以它不只是某个个体的事情。 Kafka管理员、 Linux系统管理员、网络和存储管理员以及
应用程序开发者,所有人必须协同作战,才能构建一个可靠的系统。
Kaka在数据传递可靠性方面具备很大的灵活性。我们知道, Kafka可以被用在很多场景
里,从跟踪用户点击动作到处理信用卡支付操作。有些场景要求很高的可靠性,而有些则
更看重速度和简便性。 Kafka被设计成高度可配置的,而且它的客户端API可以满足不同
程度的可靠性需求。
不过,灵活性有时候也很容易让人掉入陷阱。有时候,你的系统看起来是可靠的,但实
际上有可能不是。本章先讨论各种各样的可靠性及其在Kaka场景中的含义。然后介绍
Kafka的复制功能,以及它是如何提高系统可靠性的。随后探讨如何配置Kaka的 broker
和主题来满足不同的使用场景需求,也会涉及生产者和消费者以及如何在各种可靠性场景
里使用它们。最后介绍如何验证系统的可靠性,因为系统的可靠性涉及方方面面—些
前提条件必须先得到满足。
6.1可靠性保证
在讨论可靠性时,我们一般会使用保证这个词,它是指确保系统在各种不同的环境下能够
发生一致的行为
ACD大概是大家最熟悉的一个例子,它是关系型数据库普遍支持的标准可靠性保证。
87AC⑩D指的是原子性、一致性、隔离性和持久性。如果一个供应商说他们的数据库遵循
ACID规范,其实就是在说他们的数据库支持与事务相关的行为。
有了这些保证,我们才能相信关系型数据库的事务特性可以确保应用程序的安全。我们知
道系统承诺可以做到些什么,也知道在不同条件下它们会发生怎样的行为。我们了解这些
保证机制,并基于这些保证机制开发安全的应用程序。
所以,了解系统的保证机制对于构建可靠的应用程序来说至关重要,这也是能够在不同条
件下解释系统行为的前提。那么 Kafka可以在哪些方面作出保证呢?
Kaka可以保证分区消息的顺序。如果使用同一个生产者往同一个分区写入消息,而且
消息B在消息A之后写入,那么Kaka可以保证消息B的偏移量比消息A的偏移量大,
而且消费者会先读取消息A再读取消息B
只有当消息被写入分区的所有同步副本时(但不一定要写入磁盘),它才被认为是“已
提交”的。生产者可以选择接收不同类型的确认,比如在消息被完全提交时的确认,或
者在消息被写入首领副本时的确认,或者在消息被发送到网络时的确认。
只要还有一个副本是活跃的,那么已经提交的消息就不会丢失
消费者只能读取已经提交的消息
这些基本的保证机制可以用来构建可靠的系统,但仅仅依赖它们是无法保证系统完全可靠
的。构建一个可靠的系统需要作出一些权衡,Kaka管理员和开发者可以在配置参数上作
出权衡,从而得到他们想要达到的可靠性。这种权衡一般是指消息存储的可靠性和一致性
的重要程度与可用性、高吞吐量、低延迟和硬件成本的重要程度之间的权衡。下面将介绍
Kaka的复制机制,并探讨Kaka是如何实现可靠性的,最后介绍一些重要的配置参数。
62复制
Kaka的复制机制和分区的多副本架构是Kaka可靠性保证的核心。把消息写入多个副本
可以使Kaka在发生崩溃时仍能保证消息的持久性。
我们已经在第5章深入解释了Kaka的复制机制,现在重新回顾一下主要内容。
Kaka的主题被分为多个分区,分区是基本的数据块。分区存储在单个磁盘上, Kafka可以
保证分区里的事件是有序的,分区可以在线(可用),也可以离线(不可用)。每个分区可
以有多个副本,其中一个副本是首领。所有的事件都直接发送给首领副本,或者直接从首
领副本读取事件。其他副本只需要与首领保持同步,并及时复制最新的事件。当首领副本
不可用时,其中一个同步副本将成为新首领。
分区首领是同步副本,而对于跟随者副本来说,它需要满足以下条件才能被认为是同步的。
与 Zookeeper之间有一个活跃的会话,也就是说,它在过去的6s(可配置)内向
Zookeeper发送过心跳。
在过去的10s内(可配置)从首领那里获取过消息。
在过去的10s内从首领那里获取过最新的消息。光从首领那里获取消息是不够的,它还
必须是几乎零延迟的。
88
第6章如果跟随者副本不能满足以上任何一点,比如与 Zookeeper断开连接,或者不再获取新消
息,或者获取消息滞后了10s以上,那么它就被认为是不同步的。一个不同步的副本通过
与 Zookeeper重新建立连接,并从首领那里获取最新消息,可以重新变成同步的。这个过
程在网络出现临时问题并很快得到修复的情况下会很快完成,但如果 broker发生崩溃就需
要较长的时间。
非同步副本
如果一个或多个副本在同步和非同步状态之间快速切换,说明集群内部出现
了问题,通常是Java不恰当的垃圾回收配置导致的。不恰当的垃圾回收配置
会造成几秒钟的停顿,从而让 broker与 Zookeeper之间断开连接,最后变成
不同步的,进而发生状态切换
个滞后的同步副本会导致生产者和消费者变慢,因为在消息被认为已提交之前,客户端
会等待所有同步副本接收消息。而如果一个副本不再同步了,我们就不再关心它是否已经
收到消息。虽然非同步副本同样滞后,但它并不会对性能产生任何影响。但是,更少的同
步副本意味着更低的有效复制系数,在发生宕机时丢失数据的风险更大。
我们将在下一节讲解在实际项目中这将意味着什么。
63 broker配置
broker有3个配置参数会影响 Kafka消息存储的可靠性。与其他配置参数一样,它们可以
应用在 broker级别,用于控制所有主题的行为,也可以应用在主题级别,用于控制个别主
题的行为。
在主题级别控制可靠性,意味着Kaka集群可以同时拥有可靠的主题和非可靠的主题。例
如,在银行里,管理员可能把整个集群设置为可靠的,但把其中的一个主题设置为非可靠
的,用于保存来自客户的投诉,因为这些消息是允许丢失的。
让我们来逐个介绍这些配置参数,看看它们如何影响消息存储的可靠性,以及Kaka在哪
些方面作出了权衡。
63.1复制系数
主题级别的配置参数是 replication factor,而在 broker级别则可以通过 default
replication. factor来配置自动创建的主题。
在这本书里,我们假设主题的复制系数都是3,也就是说每个分区总共会被3个不同的
broker复制3次。这样的假设是合理的,因为Kaka的默认复制系数就是3——不过用户
可以修改它。即使是在主题创建之后,也可以通过新增或移除副本来改变复制系数。
如果复制系数为N,那么在M-1个 broker失效的情况下,仍然能够从主题读取数据或向主
题写入数据。所以,更高的复制系数会带来更高的可用性、可靠性和更少的故障。另一方
面,复制系数N需要至少N个 broker,而且会有N个数据副本,也就是说它们会占用N
可靠的数据传递倍的磁盘空间。我们一般会在可用性和存储硬件之间作出权衡。
那么该如何确定一个主题需要几个副本呢?这要看主题的重要程度,以及你愿意付出多少
成本来换取可用性。有时候这与你的偏执程度也有点关系。
如果因 broker重启导致的主题不可用是可接受的(这在集群里是很正常的行为),那么把
复制系数设为1就可以了。在作出这个权衡的时候,要确保这样不会对你的组织和用户造
成影响,因为你在节省了硬件成本的同时也降低了可用性。复制系数为2意味着可以容忍
个 broker发生失效,看起来已经足够了。不过要记住,有时候1个 broker发生失效会导
致集群不稳定(通常是旧版的 Kafka),迫使你重启另一个 broker-集群控制器。也就是
说,如果将复制系数设为2,就有可能因为重启等问题导致集群不可用。所以这是一个两
难的选择。
基于以上几点原因,我们建议在要求可用性的场景里把复制系数设为3。在大多数情况下,
这已经足够安全了—不过我们也见过有些银行使用5个副本,以防不测。
副本的分布也很重要。默认情况下,Kaka会确保分区的每个副本被放在不同的 broker上。
不过,有时候这样仍然不够安全。如果这些 broker处于同一个机架上,一旦机架的交换
机发生故障,分区就会不可用,这时候把复制系数设为多少都不管用。为了避免机架级别
的故障,我们建议把 broker分布在多个不同的机架上,并使用 broker,rack参数来为每个
broker配置所在机架的名字。如果配置了机架名字,Kaka会保证分区的副本被分布在多
个机架上,从而获得更高的可用性。我们已经在第5章介绍了如何在 broker和机架上分布
副本,如果你对此感兴趣,可以参考第5章的内容。
632不完全的首领选举
unclean. Leader, election只能在 broker级别(实际上是在集群范围内)进行配置,它的默
认值是true。
我们之前提到过,当分区首领不可用时,一个同步副本会被选为新首领。如果在选举过程
中没有丢失数据,也就是说提交的数据同时存在于所有的同步副本上,那么这个选举就是
完全”的。
但如果在首领不可用时其他副本都是不同步的,我们该怎么办呢?
这种情况会在以下两种场景里出现。
分区有3个副本,其中的两个跟随者副本不可用(比如有两个 broker发生崩溃)。这个
时候,如果生产者继续往首领写入数据,所有消息都会得到确认并被提交(因为此时首
领是唯一的同步副本)。现在我们假设首领也不可用了(又一个 broker发生崩溃),这
个时候,如果之前的一个跟随者重新启动,它就成为了分区的唯一不同步副本。
分区有3个副本,因为网络问题导致两个跟随者副本复制消息滞后,所以尽管它们还在
复制消息,但已经不同步了。首领作为唯一的同步副本继续接收消息。这个时候,如果
首领变为不可用,另外两个副本就再也无法变成同步的了。
对于这两种场景,我们要作出一个两难的选择。
90|第6章如果不同步的副本不能被提升为新首领,那么分区在旧首领(最后一个同步副本)恢复
之前是不可用的。有时候这种状态会持续数小时(比如更换内存芯片)
如果不同步的副本可以被提升为新首领,那么在这个副本变为不同步之后写入旧首领的
消息会全部丢失,导致数据不一致。为什么会这样呢?假设在副本0和副本1不可用时,
偏移量100~200的消息被写入副本2(首领)。现在副本2变为不可用的,而副本0变
为可用的。副本0只包含偏移量0~100的消息,不包含偏移量100~200的消息。如果我
们允许副本0成为新首领,生产者就可以继续写入数据,消费者可以继续读取数据。于是,
新首领就有了偏移量100~200的新消息。这样,部分消费者会读取到偏移量100~200的
旧消息,部分消费者会读取到偏移量100-200的新消息,还有部分消费者读取的是二者
的混合。这样会导致非常不好的结果,比如生成不准确的报表。另外,副本2可能会重
新变为可用,并成为新首领的跟随者。这个时候,它会把比当前首领旧的消息全部删除,
而这些消息对于所有消费者来说都是不可用的。
简而言之,如果我们允许不同步的副本成为首领,那么就要承担丢失数据和出现数据不
致的风险。如果不允许它们成为首领,那么就要接受较低的可用性,因为我们必须等待原
先的首领恢复到可用状态。
如果把
unclean. Leader, election enable设为true,就是允许不同步的副本成为首领(也
就是“不完全的选举”),那么我们将面临丢失消息的风险。如果把这个参数设为 false,
就要等待原先的首领重新上线,从而降低了可用性。我们经常看到一些对数据质量和数据
一致性要求较高的系统会禁用这种不完全的首领选举(把这个参数设为fase)。银行系统
是这方面最好的例子,大部分银行系统宁愿选择在几分钟甚至几个小时内不处理信用卡支
付事务,也不会冒险处理错误的消息。不过在对可用性要求较高的系统里,比如实时点击
流分析系统,一般会启用不完全的首领选举。
633最少同步副本
在主题级别和 broker级别上,这个参数都叫 min insync replicas
我们知道,尽管为一个主题配置了3个副本,还是会出现只有一个同步副本的情况。如果
这个同步副本变为不可用,我们必须在可用性和一致性之间作出选择这是一个两难的
选择。根据 Kafka对可靠性保证的定义,消息只有在被写入到所有同步副本之后才被认为
是已提交的。但如果这里的“所有副本”只包含一个同步副本,那么在这个副本变为不可
用时,数据就会丢失
如果要确保已提交的数据被写入不止一个副本,就需要把最少同步副本数量设置为大一点
的值。对于一个包含3个副本的主题,如果min. insync, replicas被设为2,那么至少要存
在两个同步副本才能向分区写入数据。
如果3个副本都是同步的,或者其中一个副本变为不可用,都不会有什么问题。不过,如
果有两个副本变为不可用,那么 broker就会停止接受生产者的请求。尝试发送数据的生产
者会收到 NotEnoughReplicas Exception异常。消费者仍然可以继续读取已有的数据。实际
上,如果使用这样的配置,那么当只剩下一个同步副本时,它就变成只读了,这是为了避
免在发生不完全选举时数据的写入和读取出现非预期的行为。为了从只读状态中恢复,必
可靠的数据传递91须让两个不可用分区中的一个重新变为可用的(比如重启 broker),并等待它变为同步的。
64在可靠的系统里使用生产者
即使我们尽可能把 broker配置得很可靠,但如果没有对生产者进行可靠性方面的配置,整
个系统仍然有可能出现突发性的数据丢失。
请看以下两个例子。
为 broker配置了3个副本,并且禁用了不完全首领选举,这样应该可以保证万无一失
我们把生产者发送消息的acks设为1(只要首领接收到消息就可以认为消息写入成功)。
生产者发送一个消息给首领,首领成功写入,但跟随者副本还没有接收到这个消息。首
领向生产者发送了一个响应,告诉它“消息写入成功”,然后它崩溃了,而此时消息还
没有被其他副本复制过去。另外两个副本此时仍然被认为是同步的(毕竟判定一个副本
不同步需要一小段时间),而且其中的一个副本成了新的首领。因为消息还没有被写入
这个副本,所以就丢失了,但发送消息的客户端却认为消息已成功写入。因为消费者看
不到丢失的消息,所以此时的系统仍然是一致的(因为副本没有收到这个消息,所以消
息不算已提交),但从生产者角度来看,它丢失了一个消息。
为 broker配置了3个副本,并且禁用了不完全首领选举。我们接受了之前的教训,把
生产者的acks设为aLL。假设现在往 Kafka发送消息,分区的首领刚好崩溃,新的首领
正在选举当中,Kaka会向生产者返回“首领不可用”的响应。在这个时候,如果生产
者没能正确处理这个错误,也没有重试发送消息直到发送成功,那么消息也有可能丢失
这算不上是 broker的可靠性问题,因为 broker并没有收到这个消息。这也不是一致性
问题,因为消费者并没有读到这个消息。问题在于如果生产者没能正确处理这些错误,
弄丢消息的是它们自己。
那么,我们该如何避免这些悲剧性的后果呢?从上面两个例子可以看出,每个使用Kaka
的开发人员都要注意两件事情。
根据可靠性需求配置恰当的acks值。
在参数配置和代码里正确处理错误。
第3章已经深入讨论了生产者的几种模式,现在回顾几个要点。
6.4.1发送确认
生产者可以选择以下3种不同的确认模式
acks=意味着如果生产者能够通过网络把消息发送出去,那么就认为消息已成功写入
Kafka。在这种情况下还是有可能发生错误,比如发送的对象无法被序列化或者网卡发
生故障,但如果是分区离线或整个集群长时间不可用,那就不会收到任何错误。即使是
在发生完全首领选举的情况下,这种模式仍然会丢失消息,因为在新首领选举过程中它
并不知道首领已经不可用了。在acks=0模式下的运行速度是非常快的(这就是为什么
很多基准测试都是基于这个模式),你可以得到惊人的吞吐量和带宽利用率,不过如果
选择了这种模式,一定会丢失一些消息。
92|第6章acks=1意味着首领在收到消息并把它写入到分区数据文件(不一定同步到磁盘上)时
会返回确认或错误响应。在这个模式下,如果发生正常的首领选举,生产者会在选举时
收到一个 Leader NotAvailable Exception异常,如果生产者能恰当地处理这个错误(参
考6.42节),它会重试发送消息,最终消息会安全到达新的首领那里。不过在这个模式
下仍然有可能丢失数据,比如消息已经成功写入首领,但在消息被复制到跟随者副本之
前首领发生崩溃。
acks=aLL意味着首领在返回确认或错误响应之前,会等待所有同步副本都收到消息。如
果和nin. Insync. replicas参数结合起来,就可以决定在返回确认前至少有多少个副本
能够收到消息。这是最保险的做法——生产者会一直重试直到消息被成功提交。不过这
也是最慢的做法,生产者在继续发送其他消息之前需要等待所有副本都收到当前的消息。
可以通过使用异步模式和更大的批次来加快速度,但这样做通常会降低吞吐量。
642配置生产者的重试参数
生产者需要处理的错误包括两部分:一部分是生产者可以自动处理的错误,还有一部分是
需要开发者手动处理的错误。
如果 broker返回的错误可以通过重试来解决,那么生产者会自动处理这些错误。生产者向
broker发送消息时, broker可以返回一个成功响应码或者一个错误响应码。错误响应码可以
分为两种,一种是在重试之后可以解决的,还有一种是无法通过重试解决的。例如,如果
broker返回的是 LEADER_NoT_ AVAILABLE错误,生产者可以尝试重新发送消息。也许在这个时
候一个新的首领被选举出来了,那么这次发送就会成功。也就是说, LEADER_ NOT_ AVAILABLE
是一个可重试错误。另一方面,如果 broker返回的是 INVALID_C0NFIG错误,即使通过重试
也无法改变配置选项,所以这样的重试是没有意义的。这种错误是不可重试错误。
般情况下,如果你的目标是不丢失任何消息,那么最好让生产者在遇到可重试错误时
能够保持重试。为什么要这样?因为像首领选举或网络连接这类问题都可以在几秒钟之
内得到解决,如果让生产者保持重试,你就不需要额外去处理这些问题了。经常会有人
问:“为生产者配置多少重试次数比较好?”这个要看你在生产者放弃重试并抛出异常之
后想做些什么。如果你想抓住异常并再多重试几次,那么就可以把重试次数设置得多
点,让生产者继续重试;如果你想直接丢弃消息,多次重试造成的延迟已经失去发送消息
的意义;如果你想把消息保存到某个地方然后回过头来再继续处理,那就可以停止重试。
Kafka的跨数据中心复制工具( MirrorMaker,我们将在第8章介绍)默认会进行无限制的
重试(例如 retries= MAX INT)。作为一个具有高可靠性的复制工具,它决不会丢失消息。
要注意,重试发送一个已经失败的消息会带来一些风险,如果两个消息都写入成功,会导
致消息重复。例如,生产者因为网络问题没有收到 broker的确认,但实际上消息已经写入
成功,生产者会认为网络出现了临时故障,就重试发送该消息(因为它不知道消息已经写
入成功)。在这种情况下, broker会收到两个相同的消息。重试和恰当的错误处理可以保
证每个消息“至少被保存一次”,但当前的 Kafka版本(0.10.0)无法保证每个消息“只被
保存一次”。现实中的很多应用程序在消息里加入唯一标识符,用于检测重复消息,消费
者在读取消息时可以对它们进行清理。还要一些应用程序可以做到消息的“幂等”,也就
是说,即使出现了重复消息,也不会对处理结果的正确性造成负面影响。例如,消息“这
可靠的数据传递个账号里有110美元”就是幂等的,因为即使多次发送这样的消息,产生的结果都是一样
的。不过消息“往这个账号里增加10美元”就不是幂等的。
643额外的错误处理
使用生产者内置的重试机制可以在不造成消息丢失的情况下轻松地处理大部分错误,不过
对于开发人员来说,仍然需要处理其他类型的错误,包括:
不可重试的 broker错误,例如消息大小错误、认证错误等;
在消息发送之前发生的错误,例如序列化错误;
在生产者达到重试次数上限时或者在消息占用的内存达到上限时发生的错误。
我们在第3章讨论了如何为同步发送消息和异步发送消息编写错误处理器。这些错误处理
器的代码逻辑与具体的应用程序及其目标有关。丢弃“不合法的消息”?把错误记录下
来?把这些消息保存在本地磁盘上?回调另一个应用程序?具体使用哪一种逻辑要根据具
体的架构来决定。只要记住,如果错误处理只是为了重试发送消息,那么最好还是使用生
产者内置的重试机制。
65在可靠的系统里使用消费者
我们已经学习了如何在保证Kaka可靠性的前提下生产数据,现在来看看如何在同样的前
提下读取数据。
在本章的开始部分可以看到,只有那些被提交到 Kafka的数据,也就是那些已经被写入所
有同步副本的数据,对消费者是可用的,这意味着消费者得到的消息已经具备了一致性。
消费者唯一要做的是跟踪哪些消息是已经读取过的,哪些是还没有读取过的。这是在读取
消息时不丢失消息的关键。
在从分区读取数据时,消费者会获取一批事件,检查这批事件里最大的偏移量,然后从这
个偏移量开始读取另外一批事件。这样可以保证消费者总能以正确的顺序获取新数据,不
会错过任何事件。
如果一个消费者退出,另一个消费者需要知道从什么地方开始继续处理,它需要知道前一个
消费者在退岀前处理的最后一个偏移量是多少。所谓的“另一个”消费者,也可能就是它自
己重启之后重新回来工作。这也就是为什么消费者要“提交”它们的偏移量。它们把当前读
取的偏移量保存起来,在退出之后,同一个群组里的其他消费者就可以接手它们的工作。如
果消費者提交了偏移量却未能处理完消息,那么就有可能造成消息丟失,这也是消费者丟失
消息的主要原因。在这种情况下,如果其他消费者接手了工作,那些没有被处理完的消息就
会被忽略,永远得不到处理。这就是为什么我们非常重视偏移量提交的时间点和提交的方式。
已提交消息与已提交偏移量
要注意,此处的已提交消息与之前讨论过的已提交消息是不一样的,它是指
已经被写入所有同步副本并且对消费者可见的消息,而已提交偏移量是指消
费者发送给Kaka的偏移量,用于确认它已经收到并处理好的消息位置。
94
第6章我们在第4章已经详细介绍了消费者API的使用,还介绍了多种提交偏移量的方式。下面
会介绍一些关键的注意事项,如果要了解消费者API的使用细节,请参考第4章。
65.1消费者的可靠性配置
为了保证消费者行为的可靠性,需要注意以下4个非常重要的配置参数。
第1个是groυp.讠d。这个参数在第4章已经详细解释过了,如果两个消费者具有相同的
group.td,并且订阅了同一个主题,那么每个消费者会分到主题分区的一个子集,也就是
说它们只能读到所有消息的一个子集(不过群组会读取主题所有的消息)。如果你希望消
费者可以看到主题的所有消息,那么需要为它们设置唯一的 group id。
第2个是auto. offset, reset。这个参数指定了在没有偏移量可提交时(比如消费者第1次
启动时)或者请求的偏移量在 broker上不存在时(第4章已经解释过这种场景),消费者
会做些什么。这个参数有两种配置。一种是 earliest,如果选择了这种配置,消费者会
从分区的开始位置读取数据,不管偏移量是否有效,这样会导致消费者读取大量的重复数
据,但可以保证最少的数据丢失。一种是 Latest,如果选择了这种配置,消费者会从分区
的末尾开始读取数据,这样可以减少重复处理消息,但很有可能会错过一些消息。
第3个是 enable.auto. commit。这是一个非常重要的配置参数,你可以让消费者基于任务
调度自动提交偏移量,也可以在代码里手动提交偏移量。自动提交的一个最大好处是,在
实现消费者逻辑时可以少考虑一些问题。如果你在消费者轮询操作里处理所有的数据,那
么自动提交可以保证只提交已经处理过的偏移量(如果忘了消费者轮询是什么,请回顾一
下第4章的内容)。自动提交的主要缺点是,无法控制重复处理消息(比如消费者在自动
提交偏移量之前停止处理消息),而且如果把消息交给另外一个后台线程去处理,自动提
交机制可能会在消息还没有处理完毕就提交偏移量。
第4个配置参数auto. commit. interval.s与第3个参数有直接的联系。如果选择了自动提
交偏移量,可以通过该参数配置提交的频度,默认值是毎5秒钟提交一次。一般来说,频
繁提交会增加额外的开销,但也会降低重复处理消息的概率。
6.52显式提交偏移量
如果选择了自动提交偏移量,就不需要关心显式提交的问题。不过如果希望能够更多地控
制偏移量提交的时间点,那么就要仔细想想该如何提交偏移量了——要么是为了减少重复
处理消息,要么是因为把消息处理逻辑放在了轮询之外。
这里我们不再重复说明这个机制以及如何使用相关的API,因为第4章里已经有很详细的
介绍。相反,我们会着重说明几个在开发具有可靠性的消费者应用程序时需要注意的事
项。我们先从简单的开始,再逐步深入
1.总是在处理完事件后再提交偏移量
如果所有的处理都是在轮询里完成,并且不需要在轮询之间维护状态(比如为了实现聚合
操作),那么可以使用自动提交,或者在轮询结束时进行手动提交。
可靠的数据传递952.提交频度是性能和重复消息数量之间的权衡
即使是在最简单的场景里,比如所有的处理都在轮询里完成,并且不需要在轮询之间维护
状态,你仍然可以在一个循环里多次提交偏移量(甚至可以在每处理完一个事件之后),
或者多个循环里只提交一次(与生产者的aks=a1L配置有点类似),这完全取决于你在性
能和重复处理消息之间作出的权衡。
3.确保对提交的偏移量心里有数
在轮询过程中提交偏移量有一个不好的地方,就是提交的偏移量有可能是读取到的最新偏
移量,而不是处理过的最新偏移量。要记住,在处理完消息后再提交偏移量是非常关键
的——否则会导致消费者错过消息。我们已经在第4章给出了示例。
4.再均衡
在设计应用程序时要注意处理消费者的再均衡问题。我们在第4章举了几个例子,一般要
在分区被撤销之前提交偏移量,并在分配到新分区时清理之前的状态。
5.消费者可能需要重试
有时候,在进行轮询之后,有些消息不会被完全处理,你想稍后再来处理。例如,假设
要把 Kafka的数据写到数据库里,不过那个时候数据库不可用,于是你想稍后重试。要注
意,你提交的是偏移量,而不是对消息的“确认”,这个与传统的发布和订阅消息系统不
太一样。如果记录#30处理失败,但记录#31处理成功,那么你不应该提交#31,否则会
导致#31以内的偏移量都被提交,包括#30在内,而这可能不是你想看到的结果。不过可
以采用以下两种模式来解决这个问题。
第一种模式,在遇到可重试错误时,提交最后一个处理成功的偏移量,然后把还没有处理
好的消息保存到缓冲区里(这样下一个轮询就不会把它们覆盖掉),调用消费者的 pause()
方法来确保其他的轮询不会返回数据(不需要担心在重试时缓冲区溢岀),在保持轮询的
同时尝试重新处理(关于为什么不能停止轮询,请参考第4章)。如果重试成功,或者重
试次数达到上限并决定放弃,那么把错误记录下来并丢弃消息,然后调用 resume()方法让
消费者继续从轮询里获取新数据。
第二种模式,在遇到可重试镨误时,把错误写入一个独立的主题,然后继续。一个独立的
消费者群组负责从该主题上读取错误消息,并进行重试,或者使用其中的一个消费者同时
从该主题上读取错误消息并进行重试,不过在重试时需要暂停该主题。这种模式有点像其
他消息系统里的 dead-letter-queue
6.消费者可能需要维护状态
有时候你希望在多个轮询之间维护状态,例如,你想计算消息的移动平均数,希望在首次
轮询之后计算平均数,然后在后续的轮询中更新这个结果。如果进程重启,你不仅需要从
上一个偏移量开始处理数据,还要恢复移动平均数。有一种办法是在提交偏移量的同时把
最近计算的平均数写到一个“结果”主题上。消费者线程在重新启动之后,它就可以拿到
最近的平均数并接着计算。不过这并不能完全地解决问题,因为 Kafka并没有提供事务支
持。消费者有可能在写入平均数之后来不及提交偏移量就崩溃了,或者反过来也一样。这
96|第6章是一个很复杂的问题,你不应该尝试自己去解决这个问题,建议尝试一下 KafkaStreams这
个类库,它为聚合、连接、时间窗和其他复杂的分析提供了高级的 DSL API。
7.长时间处理
有时候处理数据需要很长时间:你可能会从发生阻塞的外部系统获取信息,或者把数据
写到外部系统,或者进行一个非常复杂的计算。要记住,暂停轮询的时间不能超过几秒
钟。即使不想获取更多的数据,也要保持轮询,这样客户端才能往 broker发送心跳。在
这种情况下,一种常见的做法是使用一个线程池来处理数据,因为使用多个线程可以进
行并行处理,从而加快处理速度。在把数据移交给线程池去处理之后,你就可以暂停消
费者,然后保持轮询,但不获取新数据,直到工作线程处理完成。在工作线程处理完成
之后,可以让消费者继续获取新数据。因为消费者一直保持轮询,心跳会正常发送,就
不会发生再均衡。
8.仅一次传递
有些应用程序不仅仅需要“至少一次”( at-least-once)语义(意味着没有数据丢失),还
需要“仅一次”( exactly-once)语义。尽管 Kafka现在还不能完全支持仅一次语义,消费
者还是有一些办法可以保证Kaka里的每个消息只被写到外部系统一次(但不会处理向
Kafka写入数据时可能出现的重复数据)。
实现仅一次处理最简单且最常用的办法是把结果写到一个支持唯一键的系统里,比如键值
存储引擎、关系型数据库、 Elastic search或其他数据存储引擎。在这种情况下,要么消息
本身包含一个唯一键(通常都是这样),要么使用主题、分区和偏移量的组合来创建唯
键——它们的组合可以唯一标识一个Kaka记录。如果你把消息和一个唯一键写入系统,
然后碰巧又读到一个相同的消息,只要把原先的键值覆盖掉即可。数据存储引擎会覆盖已
经存在的键值对,就像没有出现过重复数据一样。这个模式被叫作幂等性写入,它是一种
很常见也很有用的模式。
如果写入消息的系统支持事务,那么就可以使用另一种方法。最简单的是使用关系型数据
库,不过HDFS里有一些被重新定义过的原子操作也经常用来达到相同的目的。我们把消
息和偏移量放在同一个事务里,这样它们就能保持同步。在消费者启动时,它会获取最近
处理过的消息偏移量,然后调用seek()方法从该偏移量位置继续读取数据。我们在第4章
已经介绍了一个相关的例子。
6.6验证系统可靠性
你经过了所有的流程,从确认可靠性需求,到配置 broker,再到配置客户端,并小心谨慎
地使用API…现在可以把所有东西都放到生产环境里去运行,然后高枕无忧,自信不会
丢失任何消息了,对吗?
你当然可以这么做,不过建议还是先对系统可靠性做一些验证。我们建议做3个层面的验
证——配置验证、应用程序验证以及生产环境的应用程序监控。让我们来看看每一步都要
做些什么以及该怎么做。
可靠的数据传递6.61配置验证
从应用程序里可以很容易对 broker和客户端配置进行验证,我们之所以建议这么做,有以
下两方面的原因。
验证配置是否满足你的需求。
帮助你理解系统的行为,了解系统的真正行为是什么,了解你对Kaka基本准则的理解
是否存在偏差,然后加以改进,同时了解这些准则是如何被应用到各种场景里的。这一
章的内容偏重理论,所以要确保你能够理解这些理论是如何运用于实际当中的。
Kaka提供了两个重要的工具用于验证配置: org. apache. kafka. tools包里的 Verifiable
Producer和 Verifiableconsumer这两个类。我们可以从命令行运行这两个类,或者把它们
嵌入到自动化测试框架里。
其思想是, Verifiableproducer生成一系列消息,这些消息包含从1到你指定的某个数
字。你可以使用与生产者相同的方式来配置 Verifiableproducer,比如配置相同的acks、
重试次数和消息生成速度。在运行 Verifiableproducer时,它会把每个消息是否成功发送
到 broker的结果打印出来。 VerifiableConsumer执行的是另一个检查——它读取事件(由
Verif讠 ableproducer生成)并按顺序打印出这些事件。它也会打印出已提交的偏移量和再
均衡的相关信息。
你可以考虑运行以下一些测试。
首领选举:如果我停掉首领会发生什么事情?生产者和消费者重新恢复正常状态需要多
长时间?
控制器选举:重启控制器后系统需要多少时间来恢复状态?
依次重启:可以依次重启 broker而不丢失任何数据吗?
不完全首领选举测试:如果依次停止所有副本(确保每个副本都变为不同步的),然后
启动一个不同步的 broker会发生什么?要怎样恢复正常?这样做是可接受的吗?
然后你从中选择一个场景,启动 Verifiableproducer和 Verifiableconsumer并开始测试这
个场景,例如,停掉正在接收消息的分区首领。如果期望在一个短暂的暂停之后状态恢复
正常并且没有任何数据丢失,那么只要确保生产者生成的数据个数与消费者读取的数据个
数是匹配的就可以了。
Kafka的代码库里包含了大量测试用例。它们大部分都遵循相同的准则——使用
Verif讠 able producer和 Verifiableconsumer来确保迭代的版本能够正常工作。
662应用程序验证
在确定 broker和客户端的配置可以满足你的需求之后,接下来要验证应用程序是否能够保
证达到你的期望。应用程序的验证包括检査自定义的错误处理代码、偏移量提交的方式
再均衡监听器以及其他使用了 Kafka客户端的地方。
因为应用程序是你自己的,关于如何测试应用程序的逻辑,我们无法提供更多的指导,但
愿你的开发流程里已经包含了集成测试。不管如何验证你的应用程序,我们都建议基于如
98第6章下的故障条件做一些测试:
客户端从服务器断开连接(系统管理员可以帮忙模拟网络故障);
首领选举;
依次重启 broker;
依次重启消费者;
依次重启生产者
你对每一个测试场景都会有期望的行为,也就是在开发应用程序时所期望看到的行为,然
后运行测试看看真实的结果是否符合预期。例如,在测试“依次重启消费者”这一场景
时,你期望看到“在短暂的再均衡之后出现的重复消息个数不超过1000个”。测试结果会
告诉我们应用程序提交偏移量的方式和处理再均衡的方式是否与预期的一样。
6.6.3在生产环境监控可靠性
测试应用程序是很重要的,不过它无法代替生产环境的持续监控,这些监控是为了确保数
据按照期望的方式流动。我们将会在第9章详细介绍如何监控Kaka集群,不过除了监控
集群的健康状况之外,监控客户端和数据流也是很重要的。
首先, Kafka的Java客户端包含了JMX度量指标,这些指标可以用于监控客户端的状态
和事件。对于生产者来说,最重要的两个可靠性指标是消息的 error-rate和 retry-rate(聚
合过的)。如果这两个指标上升,说明系统出现了问题。除此以外,还要监控生产者日
志—发送消息的错误日志被设为WARN级别,可以在“ Got error produce response with
correlation id5689 on topIc-partition[ topic-1,3], retrying( two attempts left).Eror:…”中找
到它们。如果你看到消息剩余的重试次数为0,说明生产者已经没有多余的重试机会。就
像我们在6.4节所讨论的那样,你也许可以增加重试次数,或者把造成这个错误的问题先
解决掉。
对于消费者来说,最重要的指标是 consumer-ag,该指标表明了消费者的处理速度与最近
提交到分区里的偏移量之间还有多少差距。理想情况下,该指标总是为0,消费者总能读
到最新的消息。不过在实际当中,因为pou()方法会返回很多消息,消费者在获取更多数
据之前需要花一些时间来处理它们,所以该指标会有些波动。关键是要确保消费者最终会
赶上去,而不是越落越远。因为该指标会正常波动,所以在告警系统里配置该指标有一定
难度。 Burrow是 LinkedIn公司开发的一个 consumer-lag检测工具,它可以让这件事情变
得容易一些。
监控数据流是为了确保所有生成的数据会被及时地读取(你的需求决定了“及时”的具体
含义)。为了确保数据能够被及时读取,你需要知道数据是什么时候生成的。O.10.0版本
的Kaka在消息里增加了时间戳,表明了消息的生成时间。如果你使用的是更早版本的
客户端,我们建议自己在消息里加入时间戳、应用程序的名字和机器名,这样有助于将
来诊断问题。
为了确保所有消息能够在合理的时间内被读取,应用程序需要记录生成消息的数量(一般
用每秒多少个消息来表示),而消费者需要记录已读取消息的数量(也用每秒多少个消息
来表示)以及消息生成时间(生成消息的时间)到当前时间(读取消息的时间)之间的时
可靠的数据传递99间差。然后,你需要使用工具来比较生产者和消费者记录的消息数量(为了确保没有丢失
消息),确保这两者之间的时间差不会超出我们允许的范围。为了做到更好的监控,我们
可以增加一个“监控消费者”,这个消费者订阅一个特别的主题,它只进行消息的计数操
作,并把数值与生成的消息数量进行对比,这样我们就可以在没有消费者的情况下仍然能
够准确地监控生产者。这种端到端的监控系统实现起来很耗费时间,具有一定挑战性。据
我们所知,目前还没有开源的实现。 Confluent提供了一个商业的实现版本,它是 Confluent
Control Center的一部分。
6.7总结
正如我们在本章开头所说的,可靠性并不只是Kaka单方面的事情。我们应该从整个系统
层面来考虑可靠性问题,包括应用程序的架构、生产者和消费者API的使用方式、生产
者和消费者的配置、主题的配置以及 broker的配置。系统的可靠性需要在许多方面作出权
衡,比如复杂性、性能、可用性和磁盘空间的使用。掌握 Katka的各种配置和常用模式
对使用场景的需求做到心中有数,你就可以在应用程序和Kaka的可靠性程度以及各种权
衡之间作出更好的选择。
100第6章第7章
构建数据管道
在使用Kaka构建数据管道时,通常有两种使用场景:第一种,把 Kafka作为数据管道的
两个端点之一,例如,把Kaka里的数据移动到S3上,或者把 MongoDB里的数据移动
到Kaka里;第二种,把 Kafka作为数据管道两个端点的中间媒介,例如,为了把 Twitter
的数据移动到 ElasticSearch上,需要先把它们移动到 Kafka里,再将它们从Kaka移动到
ElasticSearch上。
LinkedIn和其他一些大公司都将 Kafka用在上述两种场景中,后来,我们在0.9版本的
KaRa里增加了 Kafka Connect(以下简称 Connect)。我们注意到,企业在将Kaka与数据
管道进行集成时总会碰到一些特定的问题,所以决定往 Kafka里增加一些API来帮助他们
解决这些问题,而不是等着他们提出这些问题。
Kafka为数据管道带来的主要价值在于,它可以作为数据管道各个数据段之间的大型缓冲
区,有效地解耦管道数据的生产者和消费者。Kaka的解耦能力以及在安全和效率方面的
可靠性,使它成为构建数据管道的最佳选择。
数据集成的场景
有些组织把Kaka看成是数据管道的一个端点,他们会想“我怎么才能把数
A据从Kaka移到 ElasticSearch里”。这么想是理所当然的特别是当你需
要的数据在到达 ElasticSearch之前还停留在Kaka里的时候,其实我们也是
这么想的。不过我们要讨论的是如何在更大的场景里使用Kaka,这些场景
至少包含两个端点(可能会更多),而且这些端点都不是Kaka。对于那些面
临数据集成问题的人来说,我们建议他们从大局考虑问题,而不只是把注意
力集中在少量的端点上。过度聚焦在短期问题上,只会增加后期维护的复杂
性,付出更高的成本。
101本章将讨论在构建数据管道时需要考虑的几个常见问题。这些问题并非Kaka独有,它们
都是与数据集成相关的一般性问题。我们将解释为什么可以使用Kaka进行数据集成,以
及它是如何解决这些问题的。我们将讨论 Connect APl与普通的客户端API( Producer和
Consumer)之间的区别,以及这些客户端API分别适合在什么情况下使用。然后我们会介
绍 Connect. Connect F完整手册不在本章的讨论范围之内,不过我们会举几个例子来帮助
你入门,而且会告诉你可以从哪里了解到更多关于 Connect的信息。最后介绍其他数据集
成系统,以及如何将它们与Kaka集成起来。
7.1构建数据管道时需要考虑的问题
本书不打算讲解所有有关构建数据管道的细节,我们会着重讨论在集成多个系统时需要考
虑的几个最重要的问题。
7.1.1及时性
有些系统希望毎天一次性地接收大量数据,而有些则希望在数据生成几毫秒之内就能拿到
它们。大部分数据管道介于这两者之间。一个好的数据集成系统能够很好地支持数据管道
的各种及时性需求,而且在业务需求发生变更时,具有不同及时性需求的数据表之间可以
方便地进行迁移。 Kafka作为一个基于流的数据平台,提供了可靠且可伸缩的数据存储,
可以支持几近实时的数据管道和基于小时的批处理。生产者可以频繁地向 Katka写入数
据,也可以按需写入;消费者可以在数据到达的第一时间读取它们,也可以每隔一段时间
读取一次积压的数据。
Kaka在这里扮演了一个大型缓冲区的角色,降低了生产者和消费者之间的时间敏感度。
实时的生产者和基于批处理的消费者可以同时存在,也可以任意组合。实现回压策略也因
此变得更加容易, Kafka本身就使用了回压策略(必要时可以延后向生产者发送确认),消
费速率完全取决于消费者自己
7.1.2可靠性
我们要避免单点故障,并能够自动从各种故障中快速恢复。数据通过数据管道到达业务系
统,哪怕岀现几秒钟的故障,也会造成灾难性的影响,对于那些要求毫秒级的及时性系统
来说尤为如此。数据传递保证是可靠性的另一个重要因素。有些系统允许数据丢失,不过
在大多数情况下,它们要求至少一次传递。也就是说,源系统的每一个事件都必须到达目
的地,不过有时候需要进行重试,而重试可能造成重复传递。有些系统甚至要求仅一次传
递源系统的每一个事件都必须到达目的地,不允许丢失,也不允许重复。
我们已经在第6章深入讨论了 Kafka的可用性和可靠性保证。 Kafka本身就支持“至少
次传递”,如果再结合具有事务模型或唯一键特性的外部存储系统,Kaka也能实现“仅」
次传递”。因为大部分的端点都是数据存储系统,它们提供了“仅一次传递”的原语支持,
所以基于Kata的数据管道也能实现“仅一次传递”。值得一提的是, Connect API为集成
外部系统提供了处理偏移量的API,连接器因此可以构建仅一次传递的端到端数据管道。
实际上,很多开源的连接器都支持仅一次传递。
02|第7章7.1.3高吞吐量和动态吞吐量
为了满足现代数据系统的要求,数据管道需要支持非常高的吞吐量。更重要的是,在某些
情况下,数据管道还需要能够应对突发的吞吐量增长。
由于我们将Kaka作为生产者和消费者之间的缓冲区,消费者的吞吐量和生产者的吞吐量
就不会耦合在一起了。我们也不再需要实现复杂的回压机制,如果生产者的吞吐量超过了
消费者的吞吐量,可以把数据积压在Kaka里,等待消费者追赶上来。通过增加额外的消
费者或生产者可以实现Kaka的伸缩,因此我们可以在数据管道的任何一边进行动态的伸
缩,以便满足持续变化的需求
因为 Kafka是一个高吞吐量的分布式系统,一个适当规模的集群每秒钟可以处理数百兆的
数据,所以根本无需担心数据管道无法满足伸缩性需求。另外, Connect API不仅支持伸
缩,而且擅长并行处理任务。稍后,我们将会介绍数据源和数据池( Data sink)如何在多
个线程间拆分任务,最大限度地利用CPU资源,哪怕是运行在单台机器上。
Kaka支持多种类型的压缩,在增长吞吐量时, Kafka用户和管理员可以通过压缩来调整冈
络和存储资源的使用。
7.1.4数据格式
数据管道需要协调各种数据格式和数据类型,这是数据管道的一个非常重要的因素。数据
类型取决于不同的数据库和数据存储系统。你可能会通过AVro将XML或关系型数据加载
到Kaka里,然后将它们转成JSON写入 Elasticsearch,或者转成 Parquet写入HDFS,或
者转成CSV写入S3。
Kafka和 Connect apl与数据格式无关。我们已经在之前的章节介绍过,生产者和消费者可
以使用各种序列化器来表示任意格式的数据。 Connect api有自己的内存对象模型,包括
数据类型和 schema。不过,可以使用一些可插拔的转换器将这些对象保存成任意的格式,
也就是说,不管数据是什么格式的,都不会限制我们使用连接器。
很多数据源和数据池都有 schema,我们从数据源读取 schema,把它们保存起来,并用它
们验证数据格式的兼容性,甚至用它们更新数据池的 schema。从 MySQL到Hive的数据
管道就是一个很好的例子。如果有人在 MySQL里增加了一个字段,那么在加载数据时
数据管道可以保证Hive里也添加了相应的字段。
另外,数据池连接器将Kaka的数据写入外部系统,因此需要负责处理数据格式。有些连
接器把数据格式的处理做成可插拔的,比如HDFS的连接器就支持Avro和 Parquet
通用的数据集成框架不仅要支持各种不同的数据类型,而且要处理好不同数据源和数据池
之间的行为差异。例如,在关系型数据库向 Syslog发起抓取数据请求时, Syslog会将数据
推送给它们,而HDFS只支持追加写入模式,只能向HDFS写入新数据,而对于其他很多
系统来说,既可以追加数据,也可以更新已有的数据。
构建数据管道1037.1.5转换
数据转换比其他需求更具争议性。数据管道的构建可以分为两大阵营,即ETL和ELT
ETL表示提取一转换一加载( Extract- Transform-Ioad),也就是说,当数据流经数据管道
时,数据管道会负责处理它们。这种方式为我们节省了时间和存储空间,因为不需要经过
保存数据、修改数据、再保存数据这样的过程。不过,这种好处也要视情况而定。有时
候,这种方式会给我们带来实实在在的好处,但也有可能给数据管道造成不适当的计算和
存储负担。这种方式有一个明显不足,就是数据的转换会给数据管道下游的应用造成一些
限制,特别是当下游的应用希望对数据进行进一步处理的时候。假设有人在 MongoDB和
MySQL之间建立了数据管道,并且过滤掉了一些事件记录,或者移除了一些字段,那么
下游应用从 MySQL中访问到的数据是不完整的。如果它们想要访问被移除的字段,只能
重新构建管道,并重新处理历史数据(如果可能的话)
ELT表示提取-加载-转换( Extract-Load- Transform)。在这种模式下,数据管道只做少量的
转换(主要是数据类型转换),确保到达数据池的数据尽可能地与数据源保持一致。这种
情况也被称为高保真( high fidelity)数据管道或数据湖( data lake)架构。目标系统收集
原始数据”,并负责处理它们。这种方式为目标系统的用户提供了最大的灵活性,因为它
们可以访问到完整的数据。在这些系统里诊断问题也变得更加容易,因为数据被集中在同
个系统里进行处理,而不是分散在数据管道和其他应用里。这种方式的不足在于,数据
的转换占用了目标系统太多的CPU和存储资源。有时候,目标系统造价高昂,如果有可
能,人们希望能够将计算任务移出这些系统。
7.1.6安全性
安全性是人们一直关心的问题。对于数据管道的安全性来说,人们主要关心如下几个
方面。
我们能否保证流经数据管道的数据是经过加密的?这是跨数据中心数据管道通常需要考
虑的一个主要方面
谁能够修改数据管道?
如果数据管道需要从一个不受信任的位置读取或写入数据,是否有适当的认证机制?
Kafka支持加密传输数据,从数据源到Kaka,再从Kaka到数据池。它还支持认证(通过
SASL来实现)和授权,所以你可以确信,如果一个主题包含了敏感信息,在不经授权的
情况下,数据是不会流到不安全的系统里的。Kaka还提供了审计日志用于跟踪访问记录。
通过编写额外的代码,还可能跟踪到每个事件的来源和事件的修改者,从而在每个记录之
间建立起整体的联系。
7.1.7故障处理能力
我们不能总是假设数据是完美的,而要事先做好应对故障的准备。能否总是把缺损的数
据挡在数据管道之外?能否恢复无法解析的记录?能否修复(或许可以手动进行)并重
新处理缺损的数据?如果在若干天之后才发现原先看起来正常的数据其实是缺损数据,
该怎么办?
104第7章因为 Kafka会长时间地保留数据,所以我们可以在适当的时候回过头来重新处理出错
的数据。
7.1.8耦合性和灵活性
数据管道最重要的作用之一是解耦数据源和数据池。它们在很多情况下可能发生耦合。
临时数据管道
有些公司为每一对应用程序建立单独的数据管道。例如,他们使用 Logstash向
ElasticSearch导入日志,使用Fume向HDFS导入日志,使用 Golden gate将 Oracle的
数据导到HDFS,使用 Informatica将 MySQL的数据或XML导到 Oracle,等等。他们
将数据管道与特定的端点耦合起来,并创建了大量的集成点,需要额外的部署、维护和
监控。当有新的系统加入时,他们需要构建额外的数据管道,从而增加了采用新技术的
成本,同时遏制了创新。
元数据丢失
如果数据管道没有保留 schema元数据,而且不允许 schema发生变更,那么最终会导
致生产者和消费者之间发生紧密的耦合。没有了 schema,生产者和消费者需要额外的
信息来解析数据。假设数据从 Oracle流向HDFS,如果DBA在 Oracle里添加了一个字
段,而且没有保留 schema信息,也不允许修改 schema,那么从HDFS读取数据时可能
会发生错误,因此需要双方的开发人员同时升级应用程序才能解决这个问题。不管是哪
种情况,它们的解决方案都不具备灵活性。如果数据管道允许 schema发生变更,应
用程序各方就可以修改自己的代码,无需担心对整个系统造成破坏
末端处理
我们在讨论数据转换时就已提到,数据管道难免要做一些数据处理。在不同的系统之间
移动数据肯定会碰到不同的数据格式和不同的应用场景。不过,如果数据管道过多地处
理数据,那么就会给下游的系统造成一些限制。在构建数据管道时所做的设计决定都会
对下游的系统造成束缚,比如应该保留哪些字段或应该如何聚合数据,等等。如果下游
的系统有新的需求,那么数据管道就要作出相应的变更,这种方式不仅不灵活,而且低
效、不安全。更为灵活的方式是尽量保留原始数据的完整性,让下游的应用自己决定如
何处理和聚合数据。
7.2如何在 Connect AP和客户端AP之间作出选择
在向Kaka写入数据或从 Kafka读取数据时,要么使用传统的生产者和消费者客户端,就
像第3章和第4章所描述的那样,要么使用后面即将介绍的 Connect apl和连接器。在具
体介绍 Connect API之前,我们不妨先问自己一个问题:“什么时候适合用哪一个?”
我们知道, Kafka客户端是要被内嵌到应用程序里的,应用程序使用它们向Kaka写入数
据或从Kaka读取数据。如果你是开发人员,你会使用Kaka客户端将应用程序连接到
Kaka,并修改应用程序的代码,将数据推送到Kaka或者从Kaka读取数据。
如果要将Kaka连接到数据存储系统,可以使用 Connect,因为这些系统不是你开发的,
构建数据管道|105你无法或者也不想修改它们的代码。 Connect可以用于从外部数据存储系统读取数据,或
者将数据推送到外部存储系统。如果数据存储系统提供了相应的连接器,那么非开发人员
就可以通过配置连接器的方式来使用 Connect
如果你要连接的数据存储系统没有相应的连接器,那么可以考虑使用客户端AP或
Connect aPi开发一个应用程序。我们建议首选 Connect,因为它提供了一些开箱即用的
特性,比如配置管理、偏移量存储、并行处理、错误处理,而且支持多种数据类型和标准
的REST管理API。开发一个连接Kaka和外部数据存储系统的小应用程序看起来很简单,
但其实还有很多细节需要处理,比如数据类型和配置选项,这些无疑加大了开发的复杂
性— Connect处理了大部分细节,让你可以专注于数据的传输。
7.3 Kafka Connect
Connect是Kaka的一部分,它为在Kaka和外部数据存储系统之间移动数据提供了一种
可靠且可伸缩的方式。它为连接器插件提供了一组AP和一个运行时— Connect负责运
行这些插件,它们则负责移动数据。 Connect以 worker进程集群的方式运行,我们基于
worker进程安装连接器插件,然后使用 REST API来管理和配置 connector,这些 worker
进程都是长时间持续运行的作业。连接器启动额外的task,有效地利用工作节点的资源,
以并行的方式移动大量的数据。数据源的连接器负责从源系统读取数据,并把数据对象
提供给 worker进程。数据池的连接器负责从 worker进程获取数据,并把它们写入目标
系统。 Connect通过 connector在Kaka里存储不同格式的数据。Kaka支持JSON,而且
Confluent Schema Registry提供了Avro转换器。开发人员可以选择数据的存储格式,这些
完全独立于他们所使用的连接器
本章的内容无法完全覆盖 Connect的所有细节和各种连接器,这些内容可以单独写成一
本书。不过,我们会提供 Connect的概览,还会介绍如何使用它,并提供一些额外的参
考资料。
7.3.1运行 Connect
Connect随着Kaka一起发布,所以无需单独安装。如果你打算在生产环境使用 Connect来
移动大量的数据,或者打算运行多个连接器,那么最好把 Connect部署在独立于 broker的
报务器上。在所有的机器上安装Kaka,并在部分服务器上启动 broker,然后在其他服务
器上启动 Connect
启动 Connect进程与启动 broker差不多,在调用脚本时传入一个属性文件即可。
bin/connect-distributed sh config/connect-distributed properties
Connect进程有以下几个重要的配置参数。
· bootstrap. servers:该参数列出了将要与 Connect协同工作的 broker服务器,连接器将
会向这些 broker写入数据或者从它们那里读取数据。你不需要指定集群所有的 broker,
不过建议至少指定3个。
106|第7章group.id:具有相同 group id的 worker属于同一个 Connect集群。集群的连接器和它们
的任务可以运行在任意一个 worker上。
k
ey converter
和 value. converter: Connect可以处理存储在Kaka里的不同格式的
数据。这两个参数分别指定了消息的键和值所使用的转换器。默认使用Kaka提供的
JSONConverter,当然也可以配置成 Confluent Schema Registry提供的 AvroConverter
数。例如,通过将 key converter. schema enable设置成
true或者 false来指定JSON消息是否可以包含 schema。值转换器也有类似的配置,不过
它的参数名是vaue. converter. schema enable。Avro消息也包含了 schema,不过需要通
过 keyconverter. schema registry.ur1和 value, converter, schema, registry. url来指定
Schema Registry的位置。
我们一般通过 Connect的 REST API来配置和监控rest.host.nane和rest.port连接器。
你可以为 REST AP指定特定的端口。
在启动 worker集群之后,可以通过 REST API来验证它们是否运行正常。
gwenscurlhttp://localhost:8083/
[version 010.1.0-SNAPSHOT","commit": 561f45d747cd2a8c
这个 REST URI应该要返回当前 Connect的版本号。我运行的是 Katka0.10.1.0(预发行)
快照版本。我们还可以检查已经安装好的连接器插件:
gwenscurlhttp://localhost:8083/connector-plugins
[["class": "org. apache. kafka. connect file. FileStreamSour ce Connector y
[class":"org. apache. kafka. connect file. FilestreamSink Connector 1
我运行的是最简单的Kaka,所以只有文件数据源和文件数据池两种插件可用。
让我们先来看看如何配置和使用这些内置的连接器,然后再提供一些使用外部数据存储系
统的高级示例。
单机模式
要注意, Connect也支持单机模式。单机模式与分布式模式类似,只是在启
动时使用bin/ connect-standalone. sh代替bin/ connect-distributed.sh,也
可以通过命令行传入连接器的配置文件,这样就不需要使用 REST API了。
在单机模式下,所有的连接器和任务都运行在单独的 worker进程上。单机
模式使用起来更简单,特别是在开发和诊断问题的时候,或者是在需要让连
接器和任务运行在某台特定机器上的时候(比如 Syslog连接器会监听某个端
口,所以你需要知道它运行在哪台机器上)。
7.32连接器示例—文件数据源和文件数据池
这个例子使用了文件连接器和JSON转换器,它们都是 Kafka自带的。接下来要确保
Zookeeper和Kaka都处于运行状态。
构建数据管道|107首先启动一个分布式的 worker进程。为了实现高可用性,真实的生产环境一般需要至少
2~3个 worker集群。不过在这个例子里,我们只启动1个
bin/connect-distributed sh config/connect-distributed properties
现在开始启动一个文件数据源。为了方便,直接让它读取Kaka的配置文件—把Kaka
的配置文件内容发送到主题上。
echo '["name":"load-kafka-config","config connector class": "FileStream
source","fiLe": config/server properties","topic":"kafka-config-topic")1
curl-xpoSt-d@-http://localhost:8083/connectors--header"content
Type: application/ise
[name":"load-kafka-config", config":I "connector class": FileStream
Source","file":" config/server properties",topic": kafka-config
topic","name":"load-kafka-config"],"tasks":[I]
我写了一个JSON片段,里面包含了连接器的名字Load- kafka- config和连接器的配置信
息,配置信息包含了连接器的类名、需要加载的文件名和主题的名字。
下面通过Kaka的控制台消费者来验证配置文件是否已经被加载到主题上了:
gwens bin/kafka-console-consumer sh --new--bootstrap-server=localhost: 9092
topic kafka-config-topic --from-beginning
如果一切正常,可以看到如下的输出
[schema": " type":"string","optional": false), "pay load": " Licensed to the
Apache Software Foundation (ASF) under one or more y
≤省略部分>
["schema":[ type :"string","optional": false], "pay
Load":"####排#排######排### Server Basics
#############排####排##"}
schema
I"type":"string", "optional": false), "payload"
schema":t"type": string", "optional": false),"pay load":"# The id of the broker
This must be set to a unique integer for each broker
["schema":["type: string", "optional": false], "pay load": "broker. id=0")
[schema":f"type":"string","optional": false), "payload": ""I
<省略部分>
以上输出的是 config/ server. properties文件的内容,这些内容被一行一行地转成JsoN
记录,并被连接器发送到 kafka- config- topic主题上。默认情况下,JSON转换器会在每
个记录里附带上 schema。这里的 schema非常简单—只有一个 pay load列,它是字符串
类型,并且包含了文件里的一行内容。
现在使用文件数据池的转换器把主题里的内容导到文件里。导出的文件内容应该与原始
server properties文件的内容完全一样,JSON转化器将会把每个JSON记录转成单行
文本。
echo '["name": dump-kafka-confie
fig
108
第7章[connector class:" FileStreamSink""file": copy-of-server
properties",topics": kafka-config-topic"]
curl-xPost-d@-http://local
host: 8083/connectors --header"content-Type: application/json
[" name": dump-kafka-config","config":
["connector class":"FileStreamSink","file": copy-of-server
properties",topics":"kafka-config-topic","name": dump-kafka-config"],"tasks":
[]
这次的配置发生了变化:我们使用了类名 FileStreamsink,而不是 Filestream Source;文
件属性指向目标文件,而不是原先的文件;我们指定了 topics,而不是 topic。可以使用
数据池将多个主题写入一个文件,而一个数据源只允许被写入一个主题。
如果一切正常,你会得到一个叫作copy-of- server- properties的文件,该文件的内容与
config/ server. properties完全一样。
如果要删除一个连接器,可以运行下面的命令
curl-xDeleTehttp://localhost:8083/connectors/dump-kafka-config
在删除连接器之后,如果查看 Connect的日志,你会发现其他的连接器会重启它们的任务。
这是为了在 worker进程间平衡剩余的任务,确保删除连接器之后可以保持负载的均衡。
7.3.3连接器示例—一从 MySQL到 Elastic Search
接下来,我们要做一些更有用的事情。这次,我们将一个 MySQL的表数据导入到一个
Kaka主题上,再将它们加载到 ElasticSearch里,然后对它们的内容进行索引
我是在自己的Mac笔记本上运行测试的,使用了下面的命令来安装 MySQL和 ElasticSearch
brew install mysql
brew install elasticsearch
下一步要确保已经有可用的连接器。如果使用的是 Confluent Open Source,这个平台已经包
含了相关的连接器,否则就要从 Github上下载和安装。
(1)打开网页https://github.com/confluentinc/kafka-connect-elasticsearch
(2)把代码复制到本地。
(3)使用 myn install来构建项目。
(4)按照相同的步骤安装JDBC连接器:htps/github.com/confluentinc/kafka-connect-jdbc
接下来把每个 target目录下生成的JAR包复制到 Connect的类路径中。
gwens mkdir libs
ens cp./kafka-connect-jdbc/target/kafka-connect-jdbc-31.0-SNAPSHOTjar
libs/
gwens cp . /kafka-connect-elasticsearch/target/kafka-connect
elasticsearch-3. 2. 0-SNAPSHOT-package/share/java/kafka-connect-elasticsearch/*
libs/
如果 worker进程还没有启动,需要先启动它们,然后检查新的连接器插件是否已经安
装成功:
构建数据管道|109gwens bin/connect-distributed sh config/connect-distributed. properties
gwenscurlhttp://localhost:8083/connector-plugins
["class":"org. apache. kafka. connect file. FileStreamSour ce Connector
[c
Lass":"io confluent connect. elasticsearch ELasticsearchsink Connector 1,
class": "org. apache. kafka. connect file. FileStreamSink Connector )
[class: " io confluent connect jdbc. JdbcSour ce Connector"]
从上面的输出可以看到,新的连接器插件已经安装成功了。JDBC连接器还需要一个
MySQL驱动程序。我们从 Oracle网站下载了一个 MySQL的JDBC驱动程序,并将其解
压,然后把 mysql-connector-java5.1.40-bnja复制到bs/目录下。
下一步要在 MySQL里创建一张表。可以使用JDBC连接器将其以流的方式发送给 Kafka
gwens mysql server restart
mysql> create database test
Query OK, 1 row affected(0.00 sec)
mysql> use test;
Database changed
mysql> create table login (username varchar (30), login time datetime);
Query OK, 0 rows affected(0.02 sec)
mysql> insert into login values('gwenshap', nowO)
Query OK, 1 row affected(0.01 sec)
mysql> insert into login values ('tpalino', nowO))
Query OK, 1 row affected(0.00 sec)
mysql> commit
Query OK, 0 rows affected (0.01 sec)
我们创建了一个数据库和一张表,并插入了一些测试数据。
接下来要配置JDBC连接器。可以从文档中找到所有可用的配置项,也可以通过REST
API找到它们:
gwens curl -X PUT -d"1]" locaLhost: 8083/connector-plugins/JdbcSourceConnector
config/validate --header "content-Type: application/json"I python-m json tool
configs:[
definition":[
default value":""
dependents":[
display_name":"Timestamp Column Name
documentation":"The name of the timestamp column to use
etect new or modified rows. This column may not be
nullable
group":"Mode
importance":"MEDIUM"
name":"timestamp column name"
第7章order 3
required": false
STRING
width": MEDIUM
<省略部分>
我们向 REST API发起验证连接器的请求,并传给它一个空的配置,得到的是所有可用
的配置项,并以JSON的格式返回。为了方便阅读,下面使用 Python对JSON进行了格
式化。
有了这些信息,就可以创建和配置JDBC连接器了:
echo 'I"name mysql-login-connector","config":I "connector class":" JdbcSource
Connector"," connection. url":jdbc: mysql: //127.0.0.1: 3306/test?
user=root","mode": "timestamp","table. whitelist login","vali
date. non null": false, timestamp column. name":"login time","topic. pre
fix:"mysql."3icurl-xPosT-d@-http://localHost:8083/connectors--header
content-Type: application /json
tor","connection.url": "jdbc: mysqL: //127.0.0.1: 3306/test?S": "JdbcSourceConnec
I"name: mysql-Login-connector","config":"connector clas
user=root","mode":"timestamp","table. whitelist login","validate non null fal
se","timestamp column. name": "login time","topic. prefix mysql ","name":"mysql
Login-connector),"tasks": []
为了确保连接器工作正常,我们从 mysql. login主题上读取数据。
gwens bin/kafka-console-consumer sh --new --bootstrap-server=loca lhost: 9092
topic mysq
ogin --from-beginning
<省略部分>
I"schema: "type: struct","fields
[ type": " string","optional": true, " field":"username"],
[ type": " int64
, optional
n: true,"name
org. apache. kafka. connect data time
stamp","version":1, field": login time"]l,"optional": false, "name :"login"],"pay
load": "username":" gwenshap","login time":1476423962000J]
[type": struct","fields
[I type": "string","optional":true, "field":"username"]
I type" :" int64"," optional": true, name":"org. apache. kafka. connect data Time
stamp","version":1, "field":"login time "] optional": false, "name:Login"," pay-
Load": username" :"tpalino","login time: 1476423981000]1
如果得到一个“主题不存在”的错误信息,或者看不到任何数据,可以检查一下 Connect
的日志。
[2016-10-16 19: 39: 40, 482 ERROR Error while starting connector mysql-login
connector (org. apache. kafka. connect runtime Worker Connector: 108)
org. apache. kafka. connect errors. ConnectException: java. sqL SQLException: Access
denied for user 'root: '@'localhost(using password: NO)
at io confluent connect jdbc. JdbcSource Connector start(JdbcSource Connec
tor. java: 78)
构建数据管道|11我反复尝试了几次才找到正确的连接串。如果还有其他问题,请检查类路径里是否包含了
驱动程序,或者是否有数据表的读取权限。
你会看到,在连接器运行期间,向 login表插入的数据会立即出现在 mysql.logn主题上。
把数据从 MySQL移动到Kaka里就算完成了,接下来把数据从 Kafka写到 Elasticsearch
里,这个会更有意思。
首先启动 Elasticsearch,并验证是否访问本地端口:
gwens elasticsearch
gwenscurlhttp://localhost:9200/
name
Hammerhead
cluster name":" elasticsearch gwen
cluster uuid":42D5GrxoQFebf83DYgNl-g
version":
number":"2.4.1",
build hash":"c67dc32e24162035d18d6fe1e952c4cbcbe79d16
build timestamp":"2616-09-27T18:57:552",
build snapshot ": false
lucene version":5.5.2
tagline":You Know, for Search
下面启动连接器:
echo '"name":"elastic-login-connector","config":I"connector class:" ELastic
searchsinkconnector",connection.url":http://localhost
9200, type name:"mysql-data, topics: mysql login",key. ignore: true]]
curl-x PosT -d @-hTtp: //loca lhost: 8083/connectors --headercontent
Type: application/json
[name:"elastic-login-connector","config":[connector class": ELasticsearch
SinkConnector","connection.url":http://localhost:9200","type.name":"mysqldata",
topics":"mysql login","key. ignore": "true","name :"elastic-loginconnector")
tasks": [["connector": elastic- login-connector","task :0]
这里有一些配置项需要解释一下。 connection.ur1是本地 ElasticSearch服务器的地址。默
认情况下,每个 Kafka主题对应 Elastic search里的一个索引,主题的名字与索引的名字相
同。我们需要在主题内为即将写入的数据定义好类别。我们假设所有数据属于同一种类
别,所以硬编码了一个类别type.nane= mysql-data。只有 mysql. login主题里的数据会被
写入 ElasticSearch。另外,在创建 MySQL数据表时没有为其指定主键,而Kaka数据的
键是nu,所以要让 Elastic search连接器使用主题名字、分区i和偏移量作为数据的键。
同时,需要把key. lgnore设置为true。
先来验证是否已经为 mysql. login主题的数据创建好索引了。
gwens curl 'loca lhost: 9200/cat/indices? v
health status index
pri rep docs. count docs. deleted store size
pri store size
yellow open mysql login
10,7kb
1⊙.7kb
112第7章如果索引还没有创建好,可以检查一下 Connect的日志。如果出现错误,一般都是因为缺
少配置项或依赖包。如果一切正常,就可以查询到索引数据了。
gwenscurl-s-xget"http://localhost:9200/mysql.login/search?pretty=true
"took":29,
timed out": false,
shards
total":5,
successful": 5
failed": 0
hits :I
total :3
max score": 1.0,
hits
Lndex
mysql. login
type":"mysql-data
id":"mysql login+0+1
score": 1.0,
source":I
username":tpalino
" Login time":1476423981000
index":"mysql login
type":mysql-data
id":"mysql login+0+2
score
1.0
source
username
narked
" login time":1476672246000
index
mysql login",
type
mysql-data
id":mysql login+0+0",
score
1.0
source
username
gwenshap
" Login time":1476423962000
如果在 MySQL里插入新的数据,它们会自动出现在Kaka的rysq1.ogin主题以及
ElasticSearch相应的索引里。
现在,我们已经知道如何构建与安装JDBC连接器和 ElasticSearch连接器了,也可以根据
具体需要构建和安装任意的连接器。 Confluent提供了一个可用的连接器清单(hp://ww
confluent.io/ product/connectors/),一些公司和社区在开发和维护这些连接器。你可以从中
挑选你想用的连接器,从 Github上获取它们的代码,自行构建,并根据文档或者通过
REST APⅠ获取配置项,配置好以后在自己的 Connect集群上运行。
构建数据管道|113构建自己的连接器
任何人都可以基于公开的 Connector Apl创建自己的连接器。事实上,人们
创建了各种连接器,然后把它们发布到连接器中心( Connector Hub),并告
诉我们怎么使用它们。如果你在连接器中心找不到可以适配你要集成的数据
存储系统的连接器,可以开发自己的连接器。你也可以把自己的连接器贡献
给社区,让更多的人知道和使用它们。关于构建连接器的更多细节已经超
出了本章的讨论范围,不过可以参考官方文档学习如何构建连接器(htp:y
docs. confluent.io/3.0.l/ connect/devguide. html)。我们也建议将已有的连接器
作为入门参考,或者从使用 maven archtype(htp/ github. com/jcustenborder/
kafka-connect-archtype)开始。另外,可以在Kaka的社区邮件组( users@
kafka. apache. org)寻求帮助,或者在邮件组里展示自己的连接器。
7.3.4深入理解 Connect
要理解 Connect的工作原理,需要先知道3个基本概念,以及它们之间是如何进行交互
的。我们已经在之前的示例里演示了如何运行 worker进程集群以及如何启动和关闭连接
器。不过我们并没有深入解释转化器是如何处理数据的转换器把 MySQL的数据行转
成JSON记录,然后由连接器将它们写人 Kafka
现在让我们深入理解每一个组件,以及它们之间是如何进行交互的。
1.连接器和任务
连接器插件实现了 Connector APl,API包含了两部分内容。
连接器
连接器负责以下3件事情。
决定需要运行多少个任务。
按照任务来拆分数据复制。
从 worker进程获取任务配置并将其传递下去。例如,JDBC连接器会连接到数据库,
统计需要复制的数据表,并确定需要执行多少个任务,然后在配置参数ax, tasks
和实际数据量之间选择数值较小的那个作为任务数。在确定了任务数之后,连接器
会为每个任务生成一个配置,配置里包含了连接器的配置项(比如 connection,ur)
和该任务需要复制的数据表。 task Configs()方法返回一个映射列表,这些映射包含
了任务的相关配置。 worker进程负责启动和配置任务,每个任务只复制配置项里指
定的数据表。如果通过 REST API启动连接器,有可能会启动任意节点上的连接器,
那么连接器的任务就会在该节点上执行。
任务
任务负责将数据移入或移出 Kafka。任务在初始化时会得到由 worker进程分配的一个
上下文:源系统上下文( Source context)包含了一个对象,可以将源系统记录的偏移
量保存在上下文里(例如,文件连接器的偏移量就是文件里的字节位置,JDBC连接器
114第7章的偏移量可以是数据表的主键⑩D)。目标系统连接器的上下文提供了一些方法,连接器
可以用它们操作从 Kafka接收到的数据,比如进行数据清理、错误重试,或者将偏移量
保存到外部系统以便实现仅一次传递。任务在完成初始化之后,就开始按照连接器指定
的配置(包含在一个 Properties对象里)启动工作。源系统任务对外部系统进行轮询,
并返回一些记录, worker进程将这些记录发送到Kaka。数据池任务通过 worker进程
接收来自Kaka的记录,并将它们写入外部系统。
2. worker进程
worker进程是连接器和任务的“容器”。它们负责处理HTTP请求,这些请求用于定义连
接器和连接器的配置。它们还负责保存连接器的配置、启动连接器和连接器任务,并把配
置信息传递给任务。如果一个 worker进程停止工作或者发生崩溃,集群里的其他 worker
进程会感知到( Kafka的消费者协议提供了心跳检测机制),并将崩溃进程的连接器和任务
重新分配给其他进程。如果有新的进程加入集群,其他进程也会感知到,并将自己的连接
器和任务分配给新的进程,确保工作负载的均衡。进程还负责提交偏移量,如果任务抛出
异常,可以基于这些偏移量进行重试。
为了更好地理解 worker进程,我们可以将其与连接器和任务进行简单的比较。连接器和任
务负责“数据的移动”,而 worker进程负责 REST API、配置管理、可靠性、高可用性、伸
缩性和负载均衡
这种关注点分离是 Connect API给我们带来的最大好处,而这种好处是普通客户端API所
不具备的。有经验的开发人员都知道,编写代码从 Kafka读取数据并将其插入数据库只需
要一到两天的时间,但是如果要处理好配置、异常、 REST API、监控、部署、伸缩、失效
等问题,可能需要几个月。如果你使用连接器来实现数据复制,连接器插件会为你处理掉
大堆复杂的问题。
3.转化器和 Connect的数据模型
数据模型和转化器是 Connect API需要讨论的最后一部分内容。 Connect提供了一组数据
APⅠ—它们包含了数据对象和用于描述数据的 schema。例如,JDBC连接器从数据库读
取了一个字段,并基于这个字段的数据类型创建了一个 Connect Schema对象。然后使用这
些 Schema对象创建一个包含了所有数据库字段的 Struct我们保存了每一个字段的名
字和它们的值。源连接器所做的事情都很相似——从源系统读取事件,并为每个事件生成
schema和值(值就是数据对象本身)。目标连接器正好相反,它们获取 schema和值,并使
用 schema来解析值,然后写入到目标系统。
源连接器只负责基于 Data aPl生成数据对象,那么 worker进程是如何将这些数据对象保
存到Kaka的?这个时候,转换器就派上用场了。用户在配置 worker进程(或连接器)时
可以选择使用合适的转化器,用于将数据保存到Kaka。目前可用的转化器有Avro、JSON
和 String。JSON转化器可以在转换结果里附带上 schema.,当然也可以不使用 schema,这
个是可配的。 Kafka系统因此可以支持结构化的数据和半结构化的数据。连接器通过Data
AP将数据返回给 worker进程, worker进程使用指定的转化器将数据转换成Avo对象
JSON对象或者字符串,然后将它们写入 Kafka
对于目标连接器来说,过程刚好相反—在从 Kafka读取数据时
, worker进程使用指定的
构建数据管道|115转换器将各种格式(Awro、JSON或 String)的数据转换成 Data APl格式的对象,然后将
它们传给目标连接器,目标连接器再将它们插入到目标系统。
Connect API因此可以支持多种类型的数据,数据类型与连接器的实现是相互独立的—
只要有可用的转换器,连接器和数据类型可以自由组合。
4.偏移量管理
worker进程的 REST API提供了部署和配置管理服务,除此之外, worker进程还提供了
偏移量管理服务。连接器只要知道哪些数据是已经被处理过的,就可以通过 Kafka提供的
API来维护偏移量。
源连接器返回给 worker进程的记录里包含了一个逻辑分区和一个逻辑偏移量。它们并非
Kaka的分区和偏移量,而是源系统的分区和偏移量。例如,对于文件源来说,分区可以
是一个文件,偏移量可以是文件里的一个行号或者字符号;而对于JDBC源来说,分区可
以是一个数据表,偏移量可以是一条记录的主键。在设计一个源连接器时,要着重考虑如
何对源系统的数据进行分区以及如何跟踪偏移量,这将影响连接器的并行能力,也决定了
连接器是否能够实现至少一次传递或者仅一次传递。
源连接器返回的记录里包含了源系统的分区和偏移量, worker进程将这些记录发送给
Kafka。如果 Kafka确认记录保存成功, worker进程就把偏移量保存下来。偏移量的存储
机制是可插拔的,一般会使用Kaka主题来保存。如果连接器发生崩溃并重启,它可以从
最近的偏移量继续处理数据。
目标连接器的处理过程恰好相反,不过也很相似。它们从Kaka上读取包含了主题、分区
和偏移量信息的记录,然后调用连接器的put()方法,该方法会将记录保存到目标系统里
如果保存成功,连接器会通过消费者客户端将偏移量提交到 Kafka上。
框架提供的偏移量跟踪机制简化了连接器的开发工作,并在使用多个连接器时保证了一定
程度的行为一致性。
7.4 Connect之外的选择
现在,我们对 Connect API有了更加深入的了解,不仅知道如何使用它们,还知道它们的
些工作原理。虽然 Connect API为我们提供了便利和可靠性,但它并非唯一的选择。下
面看看还有哪些可用的框架,以及在什么时候可以使用它们。
74.1用于其他数据存储的摄入框架
虽然我们很想说Kaka是至高无上的明星,但肯定会有人不同意这种说法。有些人
将 Hadoop或 Elastic Search作为他们数据架构的基础,这些系统都有自己的数据摄入工
具。 Hadoop使用了 Flume, Elastic Search使用了 Logstash或 Fluent。如果架构里包含了
Kaka,并且需要连接大量的源系统和目标系统,那么建议使用 Connect ap作为摄入工
具。如果构建的系统是以 Hadoop或 Elastic Search为中心的,Kaka只是数据的来源之一,
那么使用Fume或 Logstash会更合适。
116第7章74.2基于图形界面的ETL工具
从保守的 I Informatica到一些开源的替代方案,比如 Talend和 Pentaho,或者更新的 Apache
NFi和 Stream Sets-这些ETL解决方案都支持将Kaka作为数据源和数据池。如果你已
经使用了这些系统,比如 Pentaho,那么就可能不会为了Kaka而在系统里增加另一种集
成工具。如果你已经习惯了基于图形界面的ETL数据管道解决方案,那就继续使用它们。
不过,这些系统有一些不足的地方,那就是它们的工作流比较复杂,如果你只是希望从
Kaka里获取数据或者将数据写入Kaka,那么它们就显得有点笨重。我们在本章的开头部
分已经说过,在进行数据集成时,应该将注意力集中在消息的传输上。因此,对于我们来
说,大部分ETL工具都太过复杂了。
我们极力建议将 Kafka当成是一个支持数据集成(使用 Connect)、应用集成(使用生产者
和消费者)和流式处理的平台。Kaka完全可以成为ETL工具的替代品。
7.4.3流式处理框架
几乎所有的流式处理框架都具备从 Kafka读取数据并将数据写入外部系统的能力。如果你
的目标系统支持流式处理,并且你已经打算使用流式框架处理来自 Kafka的数据,那么使
用相同的框架进行数据集成看起来是很合理的。这样可以省掉一个处理步骤(不需要保存
来自 Kafka的数据,而是直接从Kaka读取数据然后写到其他系统),不过在发生数据丢失
或者出现脏数据时,诊断问题会变得很困难,因为这些框架并不知道数据是什么时候丢失
的,或者什么时候出现了脏数据。
7.5总结
本章讨论了如何使用 Kafka进行数据集成,从解释为什么要使用Kaka进行数据集成开始,
到说明数据集成方案的一般性考虑点。我们先解释了为什么Kaka和 Connect AP是一种
更好的选择,然后给出了一些例子,演示如何在不同的场景下使用 Connect,并深入了解
了 Connect的工作原理,最后介绍了一些 Connect之外的数据集成方案。
不管最终你选择了哪一种数据集成方案,都需要保证所有消息能够在各种恶劣条件下完成
传递。我们相信,在与 Kafka的可靠性特性结合起来之后, Connect具有了极高的可靠性
不过,我们仍然需要对它们进行严格的测试,以确保你选择的数据集成系统能够在发生进
程停止、机器崩溃、网络延迟和高负载的情况下不丢失消息。毕竟,数据集成系统应该只
做一件事情,那就是传递数据。
可靠性是数据集成系统唯一一个重要的需求。在选择数据系统时,首先要明确需求(可以
参考η.1节),并确保所选择的系统能够满足这些需求。除此之外,还要很好地了解数据集
成方案,确保知道怎么使用它们来满足需求。虽然Kaka支持至少一次传递的原语,但你
也要小心谨慎,避免在配置上出现偏差,破坏了可靠性。
构建数据管道117第8章
跨集群数据镜像
本书的大部分内容都是在讨论单个Kaka集群的配置、维护和使用。不过,在某些场景的
架构里,可能需要用到多个集群。
有时候,这些集群相互独立,属于不同的部门,或者有不同的用途,那么就没有必要在集
群间复制数据。有时候,因为对SLA有不同的要求,或者因为工作负载的不同,很难通过
调整单个集群来满足所有的需求。还有一些时候,对安全有各种不同的要求。这些问题其
实很容易解决,就是分别为它们创建不同的集群管理多个集群本质上就是重复多次运
行单独的集群。
在某些情况下,不同的集群之间相互依赖,管理员需要不停地在集群间复制数据。大部分
数据库都支持复制( replication),也就是持续地在数据库服务器之间复制数据。不过,因
为前面已经使用过“复制”这个词来描述在同一个集群的节点间移动数据,所以我们把集
群间的数据复制叫作镜像( mirroring)。Kaka内置的跨集群复制工具叫作 MirrorMaker。
在这一章,我们将讨论跨集群的数据镜像,它们既可以镜像所有数据,也可以镜像部分数
据。我们先从一些常见的跨集群镜像场景开始,然后介绍这些场景所使用的架构模式,以
及这些架构模式各自的优缺点。接下来我们会介绍 MirrorMaker以及如何使用它,然后说
明在进行部署和性能调优时需要注意的一些事项。最后我们会对 Mirror Maker的一些替代
方案进行比较。
8.1跨集群镜像的使用场景
下面列出了几个使用跨集群镜像的场景。
118区域集群和中心集群
有时候,一个公司会有多个数据中心,它们分布在不同的地理区域、不同的城市或不同
的大洲。这些数据中心都有自己的Kaka集群。有些应用程序只需要与本地的Kaka集
群通信,而有些则需要访问多个数据中心的数据(否则就没必要考虑跨数据中心的复制
方案了)。有很多情况需要跨数据中心,比如一个公司根据供需情况修改商品价格就是
个典型的场景。该公司在每个城市都有一个数据中心,它们收集所在城市的供需信
息,并调整商品价格。这些信息将会被镜像到一个中心集群上,业务分析员就可以在上
面生成整个公司的收益报告。
冗余(DR)
个 Kafka集群足以支撑所有的应用程序,不过你可能会担心集群因某些原因变得不可
用,所以你希望有第二个Kaka集群,它与第一个集群有相同的数据,如果发生了紧急
情况,可以将应用程序重定向到第二个集群上。
云迁移
现今有很多公司将它们的业务同时部署在本地数据中心和云端。为了实现冗余,应用
程序通常会运行在云供应商的多个服务区域里,或者使用多个云服务。本地和每个云
服务区域都会有一个Kaka集群。本地数据中心和云服务区域里的应用程序使用自己的
Kafka集群,当然也会在数据中心之间传输数据。例如,如果云端部署了一个新的应用
程序,它需要访问本地的数据。本地的应用程序负责更新数据,并把它们保存在本地的
数据库里。我们可以使用 Connect捕获这些数据库变更,并把它们保存到本地的Kaka
集群里,然后再镜像到云端的Kaka集群上。这样有助于控制跨数据中心的流量成本,
同时也有助于改进流量的监管和安全性。
8.2多集群架构
现在,我们已经知道有哪些场景需要用到多个 Kafka集群,接下来将介绍几种常见的架构
模式。我们之前已经成功地基于这些模式实现了上述的几种场景。在讲解这些架构模式之
前,先简单地介绍一下有关跨数据中心通信的现实情况。我们即将讨论的方案都是以特定
的网络条件为前提的。
821跨数据中心通信的一些现实情况
以下是在进行跨数据中心通信时需要考虑的一些问题。
高延迟
Kafka集群之间的通信延迟随着集群间距离的增长而增加。虽然光缆的速度是恒定的
但集群间的网络跳转所带来的缓冲和堵塞会增加通信延迟。
有限的带宽
单个数据中心的广域网带宽远比我们想象的要低得多,而且可用的带宽时刻在发生变
化。另外,高延迟让如何利用这些带宽变得更加困难。
跨集群数据镜像|119高成本
不管你是在本地还是在云端运行Kaka,集群之间的通信都需要更高的成本。部分原因
是因为带宽有限,而增加带宽是很昂贵的,当然,这个与供应商制定的在数据中心、区
域和云端之间传输数据的收费策略也有关系。
Kaka服务器和客户端是按照单个数据中心进行设计、开发、测试和调优的。我们假设服
务器和客户端之间具有很低的延迟和很高的带宽,在使用默认的超时时间和缓冲区大小时
也是基于这个前提。因此,我们不建议跨多个数据中心安装Kaka服务器(不过稍后会介
绍一些例外情况
大多数情况下,我们要避免向远程的数据中心生成数据,但如果这么做了,那么就要忍受
高延迟,并且需要通过增加重试次数( LinkedIn曾经为跨集群镜像设置了32000多次重试
次数)和增大缓冲区来解决潜在的网络分区问题(生产者和服务器之间临时断开连接)
如果有了跨集群复制的需求,同时又禁用了从 broker到 broker之间的通信以及从生产者到
broker之间的通信,那么我们必须允许从 broker到消费者之间的通信。事实上,这是最安
全的跨集群通信方式。在发生网络分区时,消费者无法从 Kafka读取数据,数据会驻留在
Kafka里,直到通信恢复正常。因此,网络分区不会造成任何数据丢失。不过,因为带宽
有限,如果一个数据中心的多个应用程序需要从另一个数据中心的Kaka服务器上读取数
据,我们倾向于为每一个数据中心安装一个Kaka集群,并在这些集群间复制数据,而不
是让不同的应用程序通过广域网访问数据。
在讨论更多有关跨数据中心通信的调优策略之前,我们需要先知道以下一些架构原则。
每个数据中心至少需要一个集群。
每两个数据中心之间的数据复制要做到毎个事件仅复制一次(除非出现错误需要重试)。
如果有可能,尽量从远程数据中心读取数据,而不是向远程数据中心写入数据。
822Hub和 Spoke架构
这种架构适用于一个中心Kaka集群对应多个本地Kaka集群的情况,如图8-1所示。
纽约
Kaka集群
中央度量
伦敦
指标 Kafka集群
Kaka集群
本地
整体
本地
应用
报表
应用
AWS
亚特兰大
Kaka集群
Kaka集群
本地
本地
应用
应用
图81:一个中心Kaka集群对应多个本地 Kafka集群
120
第8章这种架构有一个简单的变种,如果只有一个本地集群,那么整个架构里就只剩下两个集
群:一个首领和一个跟随者,如图8-2所示。
关键的生产
非关键的
环境 Kafka集群
报表Kaka集群
关键
应用
报表
图82:一个首领对应一个跟随者
当消费者需要访问的数据集分散在多个数据中心时,可以使用这种架构。如果每个数据中
心的应用程序只处理自己所在数据中心的数据,那么也可以使用这种架构,只不过它们无
法访问到全局的数据集。
这种架构的好处在于,数据只会在本地的数据中心生成,而且每个数据中心的数据只会被
镜像到中央数据中心一次。只处理单个数据中心数据的应用程序可以被部署在本地数据中
心里,而需要处理多个数据中心数据的应用程序则需要被部署在中央数据中心里。因为数
据复制是单向的,而且消费者总是从同一个集群读取数据,所以这种架构易于部署、配置
和监控。
不过这种架构的简单性也导致了一些不足。一个数据中心的应用程序无法访问另一个数据
中心的数据。为了更好地理解这种局限性,我们举一个例子来说明。
假设有一家银行,它在不同的城市有多家分行。每个城市的 Kafka集群上保存了用户的信
息和账号历史数据。我们把各个城市的数据复制到一个中心集群上,这样银行就可以利用
这些数据进行业务分析。在用户访问银行网站或去他们所属的分行办理业务时,他们的请
求被路由到本地集群上,同时从本地集群读取数据。假设一个用户去另一个城市的分行办
理业务,因为他的信息不在这个城市,所以这个分行需要与远程的集群发生交互(不建议
这么做),否则根本没有办法访问到这个用户的信息(很尴尬)。因此,这种架构模式在数
据访问方面有所局限,因为区域数据中心之间的数据是完全独立的
在采用这种架构时,毎个区域数据中心的数据都需要被镜像到中央数据中心上。镜像进程
会读取每一个区域数据中心的数据,并将它们重新生成到中心集群上。如果多个数据中心
出现了重名的主题,那么这些主题的数据可以被写到中心集群的单个主题上,也可以被写
到多个主题上。
8.2.3双活架构
当有两个或多个数据中心需要共享数据并且每个数据中心都可以生产和读取数据时,可以
使用双活( Active-Active)架构,如图8-3所示。
跨集群数据镜像‖121旧金山Kaka集群
休斯顿Kaka集群
所有
所有
应用
应用
西海岸用户
中南部用户
图8-3:两个数据中心需要共享数据
这种架构的主要好处在于,它可以为就近的用户提供服务,具有性能上的优势,而且不会
因为数据的可用性问题(在Hub和 Spoke架构中就有这种问题)在功能方面作出牺牲。第
二个好处是冗余和弹性。因为每个数据中心具备完整的功能,一旦一个数据中心发生失
效,就可以把用户重定向到另一个数据中心。这种重定向完全是网络的重定向,因此是
种最简单、最透明的失效备援方案。
这种架构的主要问题在于,如何在进行多个位置的数据异步读取和异步更新时避免冲突。
比如镜像技术方面的问题—如何确保同一个数据不会被无止境地来回镜像?而数据一致
性方面的问题则更为关键。下面是可能遇到的问题。
如果用户向一个数据中心发送数据,同时从第二个数据中心读取数据,那么在用户读取
数据之前,他发送的数据有可能还没有被镜像到第二个数据中心。对于用户来说,这就
好比把一本书加入到购物车,但是在他点开购物车时,书却不在里面。因此,在使用这
种架构时,开发人员经常会将用户“粘”在同一个数据中心上,以确保用户在大多数情
况下使用的是同一个数据中心的数据(除非他们从远程进行连接或者数据中心不可用)。
一个用户在一个数据中心订购了书A,而第二个数据中心几乎在同一时间收到了该用
户订购书B的订单,在经过数据镜像之后,每个数据中心都包含了这两个事件。两个
数据中心的应用程序需要知道如何处理这种情况。我们是否应该从中挑选一个作为“正
确”的事件?如果是这样,我们需要在两个数据中心之间定义一致的规则,用于确定
哪个事件才是正确的。又或者把两个都看成是正确的事件,将两本书都发给用户,然
后设立一个部门专门来处理退货问题? Amazon就是使用这种方式来处理冲突的,但
对于股票交易部门来说,这种方案是行不通的。如何最小化冲突以及如何处理冲突要
视具体情况而定。总之要记住,如果使用了这种架构,必然会遇到冲突问题,还要想
办法解决它们。
如果能够很好地处理在从多个位置异步读取数据和异步更新数据时发生的冲突问题,那么
我们强烈建议使用这种架构。这种架构是我们所知道的最具伸缩性、弹性、灵活性和成本
优势的解决方案。所以,它值得我们投入精力去寻找一些办法,用于避免循环复制、把相
同用户的请求粘在同一个数据中心,以及在发生冲突时解决冲突。
双活镜像(特别是当数据中心的数量超过两个)的挑战之处在于,每两个数据中心之间都
需要进行镜像,而且是双向的。如果有5个数据中心,那么就需要维护至少20个镜像进
程,还有可能达到40个,因为为了高可用,每个进程都需要冗余。
122第8章另外,我们还要避免循环镜像,相同的事件不能无止境地来回镜像。对于毎一个“逻辑
主题”,我们可以在每个数据中心里为它创建一个单独的主题,并确保不要从远程数据中
心复制同名的主题。例如,对于逻辑主题“ users”,我们在一个数据中心为其创建“SF
users”主题,在另一个数据中心为其创建“ NYC.users”主题。镜像进程将SF的“SF.
users”镜像到NYC,同时将NYC的“NYC. users”镜像到SF。这样一来,每一个事件只
会被镜像一次,不过在经过镜像之后,每个数据中心同时拥有了 SEusers和 NYC.users这
两个主题,也就是说,每个数据中心都拥有相同的用户数据。消费者如果要读取所有的用
户数据,就需要以“*. users”的方式订阅主题。我们也可以把这种方式理解为数据中心的
命名空间,比如在这个例子里,NYC和SF就是命名空间。
在不久的将来,Kaka将会增加记录头部信息。头部信息里可以包含源数据中心的信息,
我们可以使用这些信息来避免循环镜像,也可以用它们来单独处理来自不同数据中心的数
据。当然,你也可以通过使用结构化的数据格式(比如Avo)来实现这一特性,并用它在
数据里添加标签和头部信息。不过在进行镜像时,需要做一些额外的工作,因为现成的镜
像工具并不支持自定义的头部信息格式。
824主备架构
有时候,使用多个集群只是为了达到灾备的目的。你可能在同一个数据中心安装了两个
集群,它们包含相同的数据,平常只使用其中的一个。当提供服务的集群完全不可用时
就可以使用第二个集群。又或者你可能希望它们具备地理位置弹性,比如整体业务运行
在加利福尼亚州的数据中心上,但需要在德克萨斯州有第二个数据中心,第二个数据中
心平常不怎么用,但是一旦第一个数据中心发生地震,第二个数据中心就能派上用场。
德克萨斯州的数据中心可能拥有所有应用程序和数据的非活跃(“冷”)复制,在紧急情
况下,管理员可以启动它们,让第二个集群发挥作用。这种需求一般是合规性的,业务
不一定会将其纳入规划范畴,但还是要做好充分的准备。主备( Active-Standby)架构示
意图如图84所示。
生产环境的
失效备援
Kaka集群
Kafka集群
所有
活跃
应用
所有用户
图84:主备架构示意图
这种架构的好处是易于实现,而且可以被用于任何一种场景。你可以安装第二个集群,然
后使用镜像进程将第一个集群的数据完整镜像到第二个集群上,不需要担心数据的访问和
冲突问题,也不需要担心它会带来像其他架构那样的复杂性。
这种架构的不足在于,它浪费了一个集群。Kaka集群间的失效备援比我们想象的要难得
多。从目前的情况来看,要实现不丢失数据或无重复数据的 Kafka集群失效备援是不可能
跨集群数据镜像|123的。我们只能尽量减少这些问题的发生,但无法完全避免。
让一个集群什么事也不做,只是等待灾难的发生,这明显就是对资源的浪费。因为灾难是
(或者说应该是)很少见的,所以在大部分时间里,灾备集群什么事也不做。有些组织尝
试减小灾备集群的规模,让它远小于生产环境的集群规模。这种做法具有一定的凤险,因
为你无法保证这种小规模的集群能够在紧急情况下发挥应有的作用。有些组织则倾向于让
灾备集群在平常也能发挥作用,他们把一些只读的工作负载定向到灾备集群上,也就是
说,实际上运行的是Hb和 Spoke架构的一个简化版本,因为架构里只有一个 Spoke。
那么问题来了:如何实现Kaka集群的失效备援?
首先,不管选择哪一种失效备援方案,SRE(网站可靠性工程)团队都必须随时待命。今
天能够正常运行的计划,在系统升级之后可能就无法正常工作,又或者已有的工具无法满
足新场景的需求。每季度进行一次失效备援是最低限度的要求,一个高效的SRE团队会更
频繁地进行失效备援。 Chaos Monkey是 Netflix提供的一个著名的服务,它随机地制造灾
难,有可能让任何一天都成为失效备援日。
现在,让我们来看看失效备援都包括哪些内容。
1.数据丢失和不一致性
因为 Kafka的各种镜像解决方案都是异步的(8.2.5节将介绍一种同步的方案),所以灾
备集群总是无法及时地获取主集群的最新数据。我们要时刻注意灾备集群与主集群之间
拉开了多少距离,并保证不要出现太大的差距。不过,一个繁忙的系统可以允许灾备集
群与主集群之间有几百个甚至几千个消息的延迟。如果你的 Kafka集群每秒钟可以处理
100万个消息,而在主集群和灾备集群之间有5ms的延迟,那么在最好的情况下,灾备
集群每秒钟会有5000个消息的延迟。所以,不在计划内的失效备援会造成数据的丢失
在进行计划内的失效备援时,可以先停止主集群,等待镜像进程将剩余的数据镜像完毕,
然后切换到灾备集群,这样可以避免数据丢失。在发生非计划内的失效备援时,可能会
丢失数千个消息。目前 Kafka还不支持事务,也就是说,如果多个主题的数据(比如销
售数据和产品数据)之间有相关性,那么在失效备援过程中,一些数据可以及时到达灾
备集群,而有些则不能。那么在切换到灾备集群之后,应用程序需要知道该如何处理没有
相关销售信息的产品数据。
2.失效备援之后的起始偏移量
在切换到灾备集群的过程中,最具挑战性的事情莫过于如何让应用程序知道该从什么地
方开始继续处理数据。下面将介绍一些常用的方法,其中有些很简单,但有可能会造成
额外的数据丢失或数据重复;有些则比较复杂,但可以最小化丢失数据和出现重复数据的
可能性。
偏移量自动重置
Kaka消费者有一个配置选项,用于指定在没有上一个提交偏移量的情况下该作何处
理。消费者要么从分区的起始位置开始读取数据,要么从分区的末尾开始读取数据。如
果使用的是旧版本的消费者(偏移量保存在 Zookeeper上),而且因为某些原因,这些
偏移量没有被纳入灾备计划,那么就需要从上述两个选项中选择一个。要么从头开始读
124第8章取数据,并处理大量的重复数据,要么直接跳到末尾,放弃一些数据(希望只是少量的
数据)。如果重复处理数据或者丢失一些数据不会造成太大问题,那么重置偏移量是最
为简单的方案。不过直接从主题的未尾开始读取数据这种方式或许更为常见。
复制偏移量主题
如果使用新的Kaka消费者(0.9或以上版本),消費者会把偏移量提交到一个叫作
consumer offsets的主题上。如果对这个主题进行了镜像,那么当消费者开始读
取灾备集群的数据时,它们就可以从原先的偏移量位置开始处理数据。这个看起来
很简单,不过仍然有很多需要注意的事项。
首先,我们并不能保证主集群里的偏移量与灾备集群里的偏移量是完全匹配的。假设主集
群里的数据只保留3天,而你在一个星期之后才开始镜像,那么在这种情况下,主集群里
第一个可用的偏移量可能是57000000(前4天的旧数据已经被删除了),而灾备集群里的
第一个偏移量是0,那么当消费者尝试从57000003处(因为这是它要读取的下一个数据)
开始读取数据时,就会失败
其次,就算在主题创建之后立即开始镜像,让主集群和灾备集群的主题偏移量都从0开
始,生产者在后续进行重试时仍然会造成偏移量的偏离。简而言之,目前的 Kafka镜像解
决方案无法为主集群和灾备集群保留偏移量
最后,就算偏移量被完美地保留下来,因为主集群和灾备集群之间的延迟以及Kaka缺乏
对事务的支持,消费者提交的偏移量有可能会在记录之前或者记录之后到达。在发生失效
备援之后,消费者可能会发现偏移量与记录不匹配,或者灾备集群里最新的偏移量比主集
群里的最新偏移量小。如图8-5所示。
生产环境的Kaka集群
生产环境的Kaka集群
主题A,分区0
主题A,分区0
20222212x2
23456
主题A,分区0
主题A,分区0
主题A,分区0
主题A,分区0
群组C,主群组C主群组C1,主
群组C1,主群组C1,主
题A分区0题B分区0.题A分区0
题A,分区0,题B,分区0
偏移量23偏移量6偏移量26
偏移量23偏移量6
图8-5:灾备集群偏移量与主集群的最新偏移量不匹配的示例
在这些情况下,我们需要接受一定程度的重复数据。如果灾备集群最新的偏移量比主集群
的最新偏移量小,或者因为生产者进行重试导致灾备集群的记录偏移量比主集群的记录偏
移量大,都会造成数据重复。你还需要知道该怎么处理最新偏移量与记录不匹配的问题,
跨集群数据镜像|125此时要从主题的起始位置开始读取还是从末尾开始读取?
复制偏移量主题的方式可以用于减少数据重复或数据丢失,而且实现起来很简单,只要
及时地从θ开始镜像数据,并持续地镜像偏移量主题就可以了。不过一定要注意上述的
几个问题。
基于时间的失效备援
如果使用的是新版本(0.10.0及以上版本)的Kaka消费者,每个消息里都包含了一个
时间戳,这个时间戳指明了消息发送给Kaka的时间。在更新版本的Kaka(0.10.1.0
及以上版本)里, broker提供了一个索引和一个API,用于根据时间戳查找偏移量。于
是,假设你正在进行失效备援,并且知道失效事件发生在凌晨4:05,那么就可以让消费
者从4:03的位置开始处理数据。在两分钟的时间差里会存在一些重复数据,不过这种
方式仍然比其他方案要好得多,而且也很容易向其他人解释
我们将从凌晨4:03
的位置开始处理数据”这样的解释要比“我们从一个不知道是不是最新的位置开始处理
数据”要好得多。所以,这是一种更好的折中。问题是,如何让消费者从凌晨4:03的
位置开始处理数据呢?
可以让应用程序来完成这件事情。我们为用户提供一个配置参数,用于指定从什么时间
点开始处理数据。如果用户指定了时间,应用程序可以通过新的AP获取指定时间的
偏移量,然后从这个位置开始处理数据。
如果应用程序在一开始就是这么设计的,那么使用这种方案就再好不过了。但如果应用
程序在一开始不是这么设计的呢?开发一个这样的小工具也并不难—接收一个时间
戳,使用新的AP获取相应的偏移量,然后提交偏移量。我们希望在未来的Kaka版本
里添加这样的工具,不过你也可以自己写一个。在运行这个工具时,应该先关闭消费者
群组,在工具完成任务之后再启动它们。
该方案适用于那些使用了新版Kaka、对失效备援有明确要求并且喜欢自己开发工具
的人。
偏移量外部映射
我们知道,镜像偏移量主题的一个最大问题在于主集群和灾备集群的偏移量会发生偏
差。因此,一些组织选择使用外部数据存储(比如 Apache Cassandra)来保存集群之间
的偏移量映射。他们自己开发镜像工具,在一个数据被镜像到灾备集群之后,主集群
和灾备集群的偏移量被保存到外部数据存储上。或者只有当两边的偏移量差值发生变
化时,才保存这两个偏移量。比如,主集群的偏移量495被映射到灾备集群的偏移量
500,在外部存储上记录为(495,500)。如果之后因为消息重复导致差值发生变化,偏
移量5%6被映射为600,那么就保留新的映射(569,600)。他们没有必要保留495和
59%6之间的所有偏移量映射,他们假设差值都是一样的,所以主集群的偏移量550会映
射到灾备集群的偏移量555。那么在发生失效备援时,他们将主集群的偏移量与灾备集
群的偏移量映射起来,而不是在时间戳(通常会有点不准确)和偏移量之间做映射。他
们通过上述技术手段之一来强制消费者使用映射当中的偏移量。对于那些在数据记录之
前达到的偏移量或者没有及时被镜像到灾备集群的偏移量来说,仍然会有问题——不过
这至少已经满足了部分场景的需求
126第8章这种方案非常复杂,我认为并不值得投入额外的时间。在索引还没有出现之前,或许可
以考虑使用这种方案。但在今天,我倾向于将集群升级到新版本,并使用基于时间戳的
解决方案,而不是进行偏移量映射,更何况偏移量映射并不能覆盖所有的失效备援场景。
3.在失效备援之后
假设失效备援进行得很顺利,灾备集群也运行得很正常,现在需要对主集群做一些改动,
比如把它变成灾备集群。
如果能够通过简单地改变镜像进程的方向,让它将数据从新的主集群镜像到旧的主集群上
面,事情就完美了!不过,这里还存在两个问题。
怎么知道该从哪里开始镜像?我们同样需要解决与镜像程序里的消费者相关的问题。而
且不要忘了,所有的解决方案都有可能出现重复数据或者丢失数据,或者两者兼有。
之前讨论过,旧的主集群可能会有一些数据没有被镜像到灾备集群上,如果在这个时
候把新的数据镜像回来,那么历史遗留数据还会继续存在,两个集群的数据就会出现
不一致。
基于上述的考虑,最简单的解决方案是清理旧的主集群,删掉所有的数据和偏移量,然后
从新的主集群上把数据镜像回来,这样可以保证两个集群的数据是一致的。
4.关于集群发现
在设计灾备集群时,需要考虑一个很重要的问题,就是在发生失效备援之后,应用程序
需要知道如何与灾备集群发起通信。不建议把主集群的主机地址硬编码在生产者和消费
者的配置属性文件里。大多数组织为此创建了DNS别名,将其指向主集群,一旦发生紧
急情况,可以将其指向灾备集群。有些组织则使用服务发现工具,比如 Zookeeper、Etcd
或 Consul。这些服务发现工具(DNS或其他)没有必要将所有 broker的信息都包含在内,
Kaka客户端只需要连接到其中的一个 broker,就可以获取到整个集群的元数据,并发现
集群里的其他 broker。一般提供3个 broker的信息就可以了。除了服务发现之外,在大多
数情况下,需要重启消费者应用程序,这样它们才能找到新的可用偏移量,然后继续读取
数据。
825延展集群
在主备架构里,当Kaka集群发生失效时,可以将应用程序重定向到另一个集群上,以保
证业务的正常运行。而在整个数据中心发生故障时,可以使用延展集群( stretch cluster)
来避免Kaka集群失效。延展集群就是跨多个数据中心安装的单个 Kafka集群。
延展集群与其他类型的集群有本质上的区别。首先,延展集群并非多个集群,而是单个
集群,因此不需要对延展集群进行镜像。延展集群使用Kaka内置的复制机制在集群的
broker之间同步数据。我们可以通过配置打开延展集群的同步复制功能,生产者会在消息
成功写入到其他数据中心之后收到确认。同步复制功能要求使用机架信息,确保每个分区
在其他数据中心都存在副本,还需要配置nin.isr和acks=aL,确保每次写入消息时都可
以收到至少两个数据中心的确认。
同步复制是这种架构的最大优势。有些类型的业务要求灾备站点与主站点保持100%的同
跨集群数据镜像127步,这是一种合规性需求,可以应用在公司的任何一个数据存储上,包括Kaka本身。这
种架构的另一个好处是,数据中心及所有 broker都发挥了作用,不存在像主备架构那样的
资源浪费。
这种架构的不足之处在于,它所能应对的灾难类型很有限,只能应对数据中心的故障,无
法应对应用程序或者 Kafka故障。运维的复杂性是它的另一个不足之处,它所需要的物理
基础设施并不是所有公司都能够承担得起的。
如果能够在至少3个具有高带宽和低延迟的数据中心上安装Kaka(包括 Zookeeper),那
么就可以使用这种架构。如果你的公司有3栋大楼处于同一个街区,或者你的云供应商在
同一个地区有3个可用的区域,那么就可以考虑使用这种方案。
为什么是3个数据中心?主要是因为 Zookeeper Zookeeper要求集群里的节点个数是奇
数,而且只有当大多数节点可用时,整个集群才可用。如果只有两个数据中心和奇数个节
点,那么其中的一个数据中心将包含大多数节点,也就是说,如果这个数据中心不可用,
那么 Zookeeper和 Kafka也不可用。如果有3个数据中心,那么在分配节点时,可以做到
每个数据中心都不会包含大多数节点。如果其中的一个数据中心不可用,其他两个数据中
心包含了大多数节点,此时 Zookeeper和Kaka仍然可用。
从理论上说,在两个数据中心运行 Zookeeper和Kaka是可能的,只要将 Zookeeper的群
组配置成允许手动进行失效备援。不过在实际应用当中,这种做法并不常见。
83 Kafka的 MirrorMaker
Kaka提供了一个简单的工具,用于在两个数据中心之间镜像数据。这个工具叫
MirrorMaker,它包含了一组消费者(因为历史原因,它们在 MirrorMaker文档里被称为
流),这些消费者属于同一个群组,并从主题上读取数据。每个 MirrorMaker进程都有一个
单独的生产者。镜像过程很简单: MirrorMaker为每个消费者分配一个线程,消费者从源
集群的主题和分区上读取数据,然后通过公共生产者将数据发送到目标集群上。默认情况
下,消费者每60秒通知生产者发送所有的数据到Kaka,并等待Kaka的确认。然后消费
者再通知源集群提交这些事件相应的偏移量。这样可以保证不丟失数据(在源集群提交偏
移量之前, Kafka对消息进行了确认),而且如果 MirrorMaker进程发生崩溃,最多只会出
现60秒的重复数据。见图8-6。
源Kaka集群
MirrorMaker
目标Kaka集群
主题
消费者
主题A
主题B
消费者
生产者
主题
主题
iC I
消费者
主题C
图8-6: MirrorMaker的镜像过程
128第8章MirrorMaker相关信息
MirrorMaker看起来很简单,不过出于对效率的考虑,以及尽可能地做到仅
次传递,它的实现并不容易。截止到Kaka0.10.0.0版本, MirrorMaker已经被
重写了4次,而且在未来有可能会进行更多的重写。这里所描述的以及后续章
节将提及的 MirrorMaker相关细节都基于09.0.0到0.10.2.0之间的版本。
8.3.1如何配置
MirrorMaker是高度可配置的。首先,它使用了一个生产者和多个消费者,所以生产者和
消费者的相关配置参数都可以用于配置 MirrorMaker另外, MirrorMaker本身也有一些配
置参数,这些配置参数之间有时候会有比较复杂的依赖关系。下面将举一些例子,并着重
说明一些重要的配置参数。不过, MirrorMaker的详细文档不在本书的讨论范围之内。
先来看一个 MirrorMaker的例子:
bin/kafka-mirror-maker --consumer config etc/kafka/consumer properties
producer. config etc/kafka/producer properties --new consumer --num streams=2
whitelist
接下来分别说明 MirrorMaker的基本命令行参数。
consumer. conf ig
该参数用于指定消费者的配置文件。所有的消费者将共用这个配置,也就是说,只能配
置一个源集群和一个 group.d。所有的消费者属于同一个消费者群组,这正好与我们
的要求不谋而合。配置文件里有两个必选的参数: bootstrap servers(源集群的服务
器地址)和 group.id。除了这两个参数外,还可以为消费者指定其他任意的配置参数。
auto, commit, enable参数一般不需要修改,用默认值 false就行。 MirrorMaker会在消
息安全到达目标集群之后提交偏移量,所以不能使用自动提交。如果修改了这个参数,
可能会导致数据丢失。auto. offset, reset参数一般需要进行修改,默认值是 Latest,
也就是说, MirrorMaker只对那些在Mi
启动之后到达源集群的数据进行镜像
如果想要镜像之前的数据,需要把该参数设为 earliest。我们将在8.3.3节介绍更多的
配置属性。
producer. conf lg
该参数用于指定生产者的配置文件。配置文件里唯一必选的参数是 bootstrap servers
(目标集群的服务器地址)。我们将在833节介绍更多的配置属性。
new. consumer
MirrorMaker只能使用0.8版本或者0.9版本的消费者。建议使用0.9版本的消费者,因
为它更加稳定。
num streams
之前已经解释过,一个流就是一个消费者。所有的消费者共用一个生产者, MirrorMaker
将会使用这些流来填充同一个生产者。如果需要额外的吞吐量,就需要创建另一个
MirrorMaker进程。
跨集群数据镜像「129whitelist
这是一个正则表达式,代表了需要进行镜像的主题名字。所有与表达式匹配的主题都
将被镜像。在这个例子里,我们希望镜像所有的主题,不过在实际当中最好使用类似
prod.*”这样的表达式,避免镜像测试用的主题。在双活架构中, MirrorMaker将NYC
数据中心的数据镜像到SF,为其配置了 whitelist="NYC\*",这样就不会将SF的主题
重新镜像回来。
8.32在生产环境部署 MirrorMaker
在上面的例子里,我们是从命令行启动 MirrorMaker的。在生产环境, MirrorMaker一般是
作为后台服务运行的,而且是以 nohup的方式运行,并将控制台的输出重定向到一个日志
文件里。这个工具有一个- deamon命令行参数。理论上,只要使用这个参数就能实现后台
运行,不需要再做其他任何事情,但在实际当中,最近发布的一些版本并不能如我们所期
望的那样。
大部分使用 MirrorMaker的公司都有自己的启动脚本,他们一般会使用部署系统(比如
Ansible、 Puppet、Chef和Salt)实现自动化的部署和配置管理。
在 Docker容器里运行 MirrorMaker变得越来越流行。 Mirrormaker是完全无状态
的,也不需要磁盘存储(所有的数据和状态都保存在 Kafka上)。将 MirrorMaker安
装在 Docker里,就可以实现在单台主机上运行多个 MirrorMaker实例。因为单个
MirrorMaker实例的吞吐量受限于单个生产者,所以为了提升吞吐量,需要运行多个
MirrorMaker实例,而 Docker简化了这一过程。 Docker也让 Mirror maker的伸缩变
得更加容易,在流量高峰时,可以通过增加更多的容器来提升吞吐量,在流量低谷时,
则减少容器。如果在云端运行 MirrorMaker,根据吞吐量实际情况,可以通过增加额外
的服务器来运行 Docker容器。
如果有可能,尽量让 MirrorMaker运行在目标数据中心里。也就是说,如果要将NYC的
数据发送到SF, MirrorMaker应该运行在SF的数据中心里。因为长距离的外部网络比数
据中心的内部网络更加不可靠,如果发生了网络分区,数据中心之间断开了连接,那么
个无法连接到集群的消费者要比一个无法连接到集群的生产者要安全得多。如果消费
者无法连接到集群,最多也就是无法读取数据,数据仍然会在 Kafka集群里保留很长的
段时间,不会有丢失的风险。相反,在发生网络分区时,如果 MirrorMaker已经读取
了数据,但无法将数据生成到目标集群上,就会造成数据丢失。所以说,远程读取比远
程生成更加安全。
那么,什么情况下需要在夲地读取消息并将其生成到远程数据中心呢?如果需要加密传
输数据,但又不想在数据中心进行加密,就可以使用这种方式。消费者通过SSL连接
到 Kafka对性能有一定的影响,这个比生产者要严重得多,而且这种性能问题也会影响
broker。如果跨数据中心流量需要加密,那么最好把 MirrorMaker放在源数据中心,让它
读取本地的非加密数据,然后通过SSL连接将数据生成到远程的数据中心。这个时候,使
用SSL连接的是生产者,所以性能问题就不那么明显了。在使用这种方式时,需要确保
MirrorMaker在收到目标 broker副本的有效确认之前不要提交偏移量,并在重试次数超出
130
第8章限制或者生产者缓冲区溢出的情况下立即停止镜像。
如果希望减小源集群和目标集群之间的延迟,可以在不同的机器上运行至少两个
MirrorMaker实例,而且它们要使用相同的消费者群组。也就是说,如果关掉其中一台服务
器,另一个 Mirror Maker实例能够继续镜像数据。
在将 MirrorMaker部署到生产环境时,最好要对以下几项内容进行监控。
延迟监控
我们绝对有必要知道目标集群是否落后于源集群。延迟体现在源集群最新偏移量和目标
集群最新偏移量的差异上。见图8-7。
生产环境的Kaka集群
生产环境的 Kafka集群
主题A,分区0
主题A,分区0
23562
345
MirrorMaker
主题_消费者偏移量
群组 MirrorMaket
主题A.分区0
偏移量3
图8-7:监控不同偏移量之间的延迟
如图87所示,源集群的最后一个偏移量是7,而目标集群的最后一个偏移量是5,所以它
们之间有两个消息的延迟。
有两种方式可用于跟踪延迟,不过它们都不是完美的解决方案。
检査 MirrorMaker提交到源集群的最新偏移量。可以使用 kafka-consumer-groups工具检
查 MirrorMaker读取的每一个分区,查看分区的最新偏移量,也就是 Mirror Maker提交
的最新偏移量。不过这个偏移量并不会100%的准确,因为 MirrorMaker并不会每时每
刻都提交偏移量,默认情况下,它会毎分钟提交一次。所以,我们最多会看到一分钟的
延迟,然后延迟突然下降。图87中的延迟是2,但 kafka-consumer-groups会认为是4,
因为 MirrorMaker还没有提交最近的偏移量。 LinkedIn的 burrow也会监控这些信息,不
过它使用了更为复杂的方法来识别延迟的真实性,所以不会导致误报。
检査 MirrorMaker读取的最新偏移量(即使还未提交)。消费者通过JMX发布关键性度
量指标,其中有一个指标是指消费者的最大延迟(基于它所读取的所有分区计算得出的)。
这个延迟也不是100%的准确,因为它只反映了消费者读取的数据,并没有考虑生产者
是否成功地将数据发送到目标集群上。在图8-7的示例里, MirrorMaker消费者会认为
延迟是1,而不是2,因为它已经读取了消息6,尽管这个消息还没有被生成到目标集群上。
要注意,如果 MirrorMaker跳过或丢弃部分消息,上述的两种方法是无法检测到的,因为
它们只跟踪最新的偏移量。 Confluent的 Control center通过监控消息的数量和校验和来提
升监控的准确性。
跨集群数据镜像|131度量指标监控
Mirrormaker内嵌了生产者和消费者,它们都有很多可用的度量指标,所以建议对它们
进行监控。Kaka文档列出了所有可用的度量指标。下面列出了几个已经被证明能够提
升 MirrorMaker性能的度量指标
消费者
fetch-size-avg、 fetch-size-nax、 fetch-rate、 fetch- throttle-tine-avg以及
fetch-throttle-time-max
生产者
batch- sLze-avg、 batch-size-nax、 requests-in- flight以及 record- retry-rate。
同时适用于两者
Lo-ratio和io-wait- ratio。
canary
如果对所有东西都进行了监控,那么 canary就不是必需的,不过对于多层监控来说,
canary可能还是有必要的。我们可以每分钟往源集群的某个特定主题上发送一个事件,
然后尝试从目标集群读取这个事件。如果这个事件在给定的时间之后才到达,那么就发
出告警,说明 MirrorMaker出现了延迟或者已经不正常了。
8.33 Mirror Maker调优
Mirror Maker集群的大小取决于对吞吐量的需求和对延迟的接受程度。如果不允许有任何
延迟,那么 MirrorMaker集群的容量需要能够支撑吞吐量的上限。如果可以容忍一些延迟,
那么可以在95%-99%的时间里只使用75%~80%的容量。在吞吐量高峰时可以允许一些
延迟,高峰期结束时,因为 MirrorMaker有一些空余容量,可以很容易地消除延迟。
你可能想通过消费者的线程数(通过 num. streams参数配置)来衡量 MirrorMaker的吞吐
量。我们可以提供一些参考数据( LinkedIn使用8个消费者可以达到6MB/s的吞吐量,使
用16个则可以达到2MB/s),不过实际的吞吐量取决于具体的硬件、数据中心或云服务
提供商,所以需要自己进行测试。 Kafka提供了 kafka-performance-producer工具,用于在
源集群上制造负载,然后启动 MirrorMaker对这个负载进行镜像。分别为 MirrorMaker配
置1、2、4、8、16、24和32个消费者线程,并观察性能在哪个点开始下降,然后将num
streams的值设置为一个小于当前点的整数。如果数据经过压缩(因为网络带宽是跨集群
镜像的瓶颈,所以建议将数据压缩后再传输),那么 MirrorMaker还要负责解压并重新压缩
这些数据。这样会消耗很多的CPU资源,所以在增加线程数量时,要注意观察CPU的使
用情况。通过这种方式,可以得到单个 MirrorMaker实例的最大吞吐量。如果单个实例的
吞吐量还达不到要求,可以增加更多的 Mirror Maker实例和服务器
另外,你可能想要分离比较敏感的主题,它们要求很低的延迟,所以其镜像必须尽可能地
接近源集群和 MirrorMaker集群。这样可以避免主题过于臃肿,或者避免出现失控的生产
者拖慢数据管道。
我们能够对 MirrorMaker进行的调优也就是这些了。不过,我们仍然有其他办法可以增加
每个消费者和每个 MirrorMaker的吞吐量。
132第8章如果 MirrorMaker是跨数据中心运行的,可以在 Linux上对网络进行优化。
增加TCP的缓冲区大小(net.core. rmem default、net.core.rnen_nax、net.core.wme
default、net.core. wmem max、net.core. optmem_max)。
启用时间窗口自动伸缩( sysctL-wnet.tpv4. tcp window_scaling1或者把 net. Lpv4
tcp window_ scaling=1添加到etc/ sysctl. conf)。
减少TCP慢启动时间(将/proc/sys/net/ip4tcp. slow start after idle设为0)。
要注意,在 Linux上进行网络调优包含了太多复杂的内容。为了了解更多参数和细节,建
议阅读相关的网络调优指南。例如,由 Sandra JOhnson等人合著的 Performance tuning
for Linux servers
除此以外,你可能还想对 MirrorMaker里的生产者和消费者进行调优。首先,你想知道生
产者或消费者是不是瓶颈所在——生产者是否在等待消费者提供更多的数据,或者其他的
什么?通过查看生产者和消费者的度量指标就可以知道问题所在了,如果其中的一个进程
空闲,而另外一个很忙,那么就知道该对哪个进行调优了。另外一种办法是查看线程转储
( thread dump),可以使用 stack获得线程转储。如果 MirrorMaker的大部分时间用在轮询上,
那么说明消费者出现了瓶颈,如果大部分时间用在发送上,那么就是生产者出现了瓶颈。
如果需要对生产者进行调优,可以使用下列参数。
maxin flight. requests per connection
默认情况下, MirrorMaker只允许存在一个处理中的请求。也就是说,生产者在发送
下一个消息之前,当前发送的消息必须得到目标集群的确认。这样会对吞吐量造成限
制,特别是当 broker在对消息进行确认之前出现了严重的延迟。 MirrorMaker之所以要
限定请求的数量,是因为有些消息在得到成功确认之前需要进行重试,而这是唯一能
够保证消息次序的方法。如果不在乎消息的次序,那么可以通过增加max.in. flight
requests.per, connection的值来提升吞吐量。
Linger.ms和 batch size
如果在进行监控时发现生产者总是发送未填满的批次(比如,度量指标 batch-size-avg
和 batch- size-max的值总是比 batch size低),那么就可以通过增加一些延迟来提升吞
吐量。通过增加 Latency.ns可以让生产者在发送批次之前等待几毫秒,让批次填充更
多的数据。如果发送的数据都是满批次的,同时还有空余的内存,那么可以配置更大的
batch.size,以便发送更大的批次。
下面的配置用于提升消费者的吞吐量。
range。 MirrorMaker默认使用 range策略(用于确定将哪些分区分配给哪个消费者的
算法)进行分区分配。 range策略有一定的优势,这也就是为什么它会成为默认的策
略。不过 range策略会导致不公平现象。对于 MirrorMaker来说,最好可以把策略改为
round rob讠n,特别是在镜像大量的主题和分区的时候。要将策略改为 round robin算法,
需要在消费者配置属性文件里加上 partition. assignment strategy=org, pache kafk
clients. consumer. RoundRobinAssignor
fetch. max. bytes。如果度量指标显示 fetch-size-avg和 fetch-size-max的数值与 fetch
ηax. bytes很接近,说明消费者读取的数据已经接近上限。如果有更多的可用内存,可
以配置更大的 fetch, max bytes,消费者就可以在每个请求里读取更多的数据。
跨集群数据镜像|133fetch,nin. bytes和 fetch. max wait。如果度量指标 fetch-rate的值很高,说明消费者
发送的请求太多了,而且获取不到足够的数据。这个时候可以配置更大的 fetch. min.
bytes和 fetch,nax.wat,这样消费者的每个请求就可以获取到更多的数据, broker会
等到有足够多的可用数据时才将响应返回
8.4其他跨集群镜像方案
我们深入了解了 MirrorMaker,因为 MirrorMaker是Kaka的一部分。不过在实际使用当
中, MirrorMaker也存在一些不足。在 MirrorMaker之外,还有其他的一些替代方案,它们
解决了 MirrorMaker的局限性和复杂性问题
84.1优步的 rEplicator
优步在他们的Kaka集群上大规模地使用 MirrorMaker,不过,随着主题和分区的增加以
及集群吞吐量的增长,他们开始面临一些问题。
再均衡延迟
MirrorMaker中的消费者只是普通的消费者,在增加 MirrorMaker的线程和实例、重启
Mirror Maker实例或往白名单里添加新主题时,消费者都需要进行再均衡。正如在第4章
里所看到的那样,再均衡要求关闭所有的消费者,直到新的分区被分配给消费者。如果
主题和分区的数量很大,整个过程需要很长的时间。如果使用了旧版本的消费者则更是
如此,比如像优步那样。有时候,这会造成5-10分钟的不可用,导致镜像过程延后,堆
积大量的待镜像数据,需要更长的时间进行恢复,这将给其他消费者带来很大的延迟。
难以增加新主题
因为白名单使用了正则表达式进行主题匹配,每次新增主题时, Mirror Maker都需要进
行再均衡。我们已经看到优步在这方面所遭遇的痛苦。后来,为了避免意外的再均衡,
他们把毎一个需要镜像的主题都列了出来,这意味着他们需要手动往白名单里添加新主
题,不过这样最起码可以保证再均衡只会在进行维护时发生,而不是在每次添加新主题
时发生。不过不管怎样,经常性的维护是避免不了的。如果没有做好维护工作,不同的
实例可能拥有不同的主题列表, MirrorMaker就会无休止地进行再均衡,因为消费者无
法就它们所订阅的主题达成一致。
为了解决上述问题,优步开发了 rEplicator。他们使用 Apache Helix(以下简称 Helix)作
为中心控制器(具有高可用性),控制器管理着主题列表和分配给每个 rEplicator实例
的分区。管理员通过 REST API添加新主题, rEplicator负责将分区分配给不同的消费
者。优步使用自己开发的 Helix Consumer替换 Mirror Maker里的 Kafka Consumer。 Hel
Consumer接受由Helx控制器分配的分区,而不是在消费者间进行再均衡(更多细节参考
第4章),从而避免了再均衡,并改为监听来自Heix控制器的分配变更事件。
优步在他们的博客上分享了 rEplicator的架构细节及其所经历的改进过程。到目前为止,
我们并不知道是否还有其他公司在使用 rEplicator或许大部分公司都还达不到Uber那
样的规模,也没有遇到相同的问题,又或者新引入的 Helix对于他们来说需要进行额外的
学习和管理,增加了整个项目的复杂性。
134
第8章8.4.2 Confluent Replicator
在优步开发 rEplicator的同时, Confluent也开发了他们的 Replicator。除了名字有点相似
外,它们之间没有任何共同点,所要解决的问题也不一样。 Replicator为 Confluent的企业
用户解决了他们在使用 MirrorMaker进行多集群部署时所遇到的问题。
分散的集群配置
MirrorMaker只能做到源集群和目标集群之间的数据同步,而主题可以有不同的分区
不同的复制系数和不同的配置。如果将源集群的保留时间从1周改为3周,但忘记给灾
备集群也做同样的修改,一旦灾备集群发生了失效备援,就会丢失几周的数据。通过手
动的方式对所有配置进行同步很容易出错,而且如果系统出现了不同步,会导致下游的
应用或者镜像进程失效。
在集群管理方面所面临的挑战
MirrorMaker一般是以多实例的集群方式进行部署的,这意味着它本身也需要进行部署
监控和配置管理。两个配置文件和大量的配置参数让 MirrorMaker的配置管理变得极具
挑战性。如果集群超过了两个,而且集群间的复制是双向的,那么情况会更加严峻。如
果有3个双活集群,就有6个 MirrorMaker集群需要进行部署、监控和配置,而且毎个
集群至少需要3个实例。如果有5个双活集群,就需要20个 MirrorMaker集群。
为了减轻IT部门的负担, Confluent将 Replicator实现为 Connect的源连接器,从 Kafka
集群读取数据,而不是从数据库读取。在第7章介绍 Connect的架构时,我们知道,连
接器会将工作分配给多个任务。在 Replicator里,每个任务包含了一个消费者和一个生
产者。 Connect根据实际情况将不同的任务分配给不同的 worker节点,因此单个服务器
上可能会有多个任务,或者任务被分散在多个服务器上,这样就避免了手动去配置每个
MirrorMaker实例需要多少个线程以及每台服务器需要多少个 MirrorMaker实例。 Connect
还提供了 REST API,用于集中管理连接器和任务。假设大部分Kaka都部署了 Connect
(比如为了将数据库的变更事件写入 Kafka),那么通过在 Connect里运行 Replicator,就可
以减少需要管理的集群数量。另一个重大的改进在于, Replicator不仅会从Kaka主题复制
数据,它还会从 Zookeeper上复制主题的配置信息。
8.5总结
本章从解释为什么需要多个 Kafka集群开始,介绍了几种从简单到复杂的多集群架构,还
介绍了 Kafka失效备援的实现细节,并比较了当前几种可用的方案。接下来介绍了一些可
用的工具,从 MirrorMaker开始,说明了在生产环境中使用 MirrorMaker要注意的细节问
题,最后介绍了 Mirror Maker之外的两个替代方案,用于弥补 Mirror Maker本身的不足。
不管最终选择哪一种架构和工具,对多集群配置和镜像管道进行测试总是少不了的。因为
Kaka多集群管理比关系型数据库要简单得多,所以很多组织总是忽视了对它进行适当的
设计、规划、测试、自动化部署、监控和维护。重视多集群的管理问题,并把它作为组织
的全盘灾备计划或多区域计划的一部分,才有可能更好地管理好多个 Kafka集群
跨集群数据镜像|135第9章
管理 Kafka
Kafka提供了一些命令行工具,用于管理集群的变更。这些工具使用Java类实现,Kaka
提供了一些脚本来调用这些Java类。不过,它们只提供了一些基本的功能,无法完成那
些复杂的操作。本章将介绍一些工具,它们是Kaka开放源码项目的一部分。 Kafka社区
也开发了很多高级的工具,我们可以在 Apache Kafka网站上找到它们,不过它们并不属于
Kaka项目
管理操作授权
虽然 Kafka实现了操作主题的认证和授权控制,但还不支持集群的其他大部
分操作。也就是说,在没有认证的情况下也可以使用这些命令行工具,在没
有安全检査和审计的情况下也可以执行诸如主题变更之类的操作。不过这些
功能正在开发当中,应该很快就能发布。
9.1主题操作
使用 kafka-topics. sh工具可以执行主题的大部分操作(配置变更部分已经被弃用并被移动
到 kafka-configs.sh工具当中)。我们可以用它创建、修改、删除和査看集群里的主题。要
使用该工具的全部功能,需要通过…z0eepr参数提供 Zookeeper的连接字符串。在下
面的例子里,Zookeeper的连接字符串是z0o1.example.com:2181/kafka-cluster
检查版本
Kaka的大部分命令行工具直接操作 Zookeeper上的元数据,并不会连接到
broker上。因此,要确保所使用工具的版本与集群里的 broker版本相匹配
直接使用集群 broker自带的工具是最保险的。
1369.1.1创建主题
在集群里创建一个主题需要用到3个参数。这些参数是必须提供的,尽管有些已经有了
broker级别的默认值。
主题名字
想要创建的主题的名字。
复制系数
主题的副本数量。
分区
主题的分区数量。
指定主题配置
可以在创建主题时显式地指定复制系数或者对配置进行覆盖,不过我们不打
算在这里介绍如何做到这些。稍后会介绍如何进行配置覆盖,它们是通过向
kafka-topics.sh传递-cong参数来实现的。本章还会介绍分区的重分配。
主题名字可以包含字母、数字、下划线以及英文状态下的破折号和句号。
主题的命名
主题名字的开头部分包含两个下划线是合法的,但不建议这么做。具有这种
格式的主题一般是集群的内部主题(比如_ consumer_ offsets主题用于保存
消费者群组的偏移量)。也不建议在单个集群里使用英文状态下的句号和下
划线来命名,因为主题的名字会被用在度量指标上,句号会被替换成下划线
(比如“opic.1”会变成“ topic_1”)。
试着运行下面的命令:
kafka-topics. sh--zookeeper <zookeeper connect> --create--topic <string>
replication-factor <integer> --partitions <integer>
这个命令将会创建一个主题,主题的名字为指定的值,并包含了指定数量的分区。集群会
为每个分区创建指定数量的副本。如果为集群指定了基于机架信息的副本分配策略,那么
分区的副本会分布在不同的机架上。如果不需要基于机架信息的分配策略,可以指定参数
disable- rack-aware。
示例:使用以下命令创建一个叫作 my-topic的主题,主题包含8个分区,每个分区拥有两
个副本。
#t kafka-topics sh--zookeeper zoo1 example. com: 2181/kafka-cluster --create
topic my-topic --replication -factor 2--partitions 8
Created topic "my-topic
管理 Kafka|137忽略重复创建主题的错误
在自动化系统里调用这个脚本时,可以使用-if- not-exists参数,这样即
使主题已经存在,也不会抛出重复创建主题的错误。
9.1.2增加分区
有时候,我们需要为主题增加分区数量。主题基于分区进行伸缩和复制,增加分区主要是
为了扩展主题容量或者降低单个分区的吞吐量。如果要在单个消费者群组内运行更多的消
费者,那么主题数量也需要相应增加,因为一个分区只能由群组里的一个消费者读取。
调整基于键的主题
从消费者角度来看,为基于键的主题添加分区是很困难的。因为如果改变了
分区的数量,键到分区之间的映射也会发生变化。所以,对于基于键的主题
来说,建议在一开始就设置好分区数量,避免以后对其进行调整。
忽略主题不存在的错误
在使用…-ater命令修改主题时,如果指定了- if-exists参数,主题不存
在的错误就会被忽略。如果要修改的主题不存在,该命令并不会返回任何错
误。在主题不存在的时候本应该创建主题,但它却把错误隐藏起来,因此不
建议使用这个参数
示例:将 my-topic主题的分区数量增加到16。
kafka-topics sh--zookeeper zoo1 example. com: 2181/kafka-cluster
alter - topic my -topic
titions 16
WARNING: If partitions are increased for a topic that has a key
the partition logic or order ing of the messages will be affected
Adding partitions succeeded!
#
减少分区数量
我们无法减少主题的分区数量。因为如果删除了分区,分区里的数据也一并
被删除,导致数据不一致,我们也无法将这些数据分配给其他分区,因为这
样做很难,而且会出现消息乱序。所以,如果一定要减少分区数量,只能删
除整个主题,然后重新创建它。
9.1.3删除主题
如果一个主题不再被使用,只要它还存在于集群里,就会占用一定数量的磁盘空间和文件
句柄。把它删除就可以释放被占用的资源。为了能够删除主题, broker的 delete. topic
enable参数必须被设置为trve。如果该参数被设为 false,删除主题的请求会被忽略。
138第9章删除主题会丢弃主题里的所有数据。这是一个不可逆的操作,所以在执行时
要十分小心。
示例:删除 my-topic主题。
kafka-topics sh --zookeeper zoo1 example. com: 2181/kafka-cluster
-delete -- topic my -topic
Topic my-topic is marked for deletion
Note: This will have no impact if delete topic. enable is not set
to true
9.14列出集群里的所有主题
可以使用主题工具列出集群里的所有主题。每个主题占用一行输出,主题之间没有特
定的顺序。
示例:列出集群里的所有主题。
kafka-topics sh--zookeeper zoo1 example. com: 2181/kafka-cluster
-List
my-topic.marked for de letion
other -topic
9.1.5列出主题详细信息
主题工具还能用来获取主题的详细信息。信息里包含了分区数量、主题的覆盖配置以及
毎个分区的副本清单。如果通过- topic参数指定特定的主题,就可以只列出指定主题
的详细信息
示例:列出集群里所有主题的详细信息。
kafka-topics sh--zookeeper zoo1 example. com: 2181/ kafka-cluster --describe
Topic: other -topic
PartitionCount: 8
Replication Factor: 2 Configs:
Topic: other -topic
Partition: 0
Replicas: 1, 0
IS「:1,6
Topic: other-topic
Partition: 1
Replicas: 0, 1
Isr: 0. 1
Topic: other-topic
Partition: 2
Replicas: 1, 0
Isr: 1.0
Topic: other-topic
Partition: 3
Replicas: 0, 1
Isr: 0.1
Topic: other-topic
Partition: 4
Replicas: 1, 0
Is「:1,0
Topic: other-topic
Partition: 5
Replicas: 0, 1
Isr: 0. 1
Topic other-topic
Partition: 6
Replicas: 1, 0
Isr: 1.0
Topic: other -topic
Partition: 7
Replicas: 0, 1
Isr: 0.1
describe命令还提供了一些参数,用于过滤输出结果,这在诊断集群问题时会很有用。不
要为这些参数指定-topc参数(因为这些参数的目的是为了找出集群里所有满足条件的
主题和分区)。这些参数也无法与Lst命令一起使用(最后一部分会详细说明原因)。
管理 Kafka|139使用- topics-wtth- overrides参数可以找出所有包含覆盖配置的主题,它只会列出包含了
与集群不一样配置的主题。
有两个参数可用于找出有问题的分区。使用-- under- replicated- partitions参数可以列出
所有包含不同步副本的分区。使用- unavailable-partitions参数可以列出所有没有首领
的分区,这些分区已经处于离线状态,对于生产者和消费者来说是不可用的。
示例:列出包含不同步副本的分区。
kafka-topics sh --zookeeper zoo1 example. com: 2181/kafka-cluster
describe --under-replicated -partitions
Topic: other-topic
Partition: 2 Leader: o Replic
Isr: 0
Topic: other-topic
Partition: 4 Leader: 0 Replicas: 1,0
Isr: 0
9.2消费者群组
在Kaka里,有两个地方保存着消费者群组的信息。对于旧版本的消费者来说,它们的信
息保存在 Zookeeper上;对于新版本的消费者来说,它们的信息保存在 broker上。 kafka-
consumer- groups.sh工具可以用于列出上述两种消费者群组。它也可以用于删除消费者群
组和偏移量信息,不过这个功能仅限于旧版本的消费者群组(信息保存在 Zookeeper上)。
在对旧版本的消费者群组进行操作时,需要通过-z0 keeper参数指定 Zookeeper的地址;
在对新版本的消费者群组进行操作时,则需要使用-- bootstrap- server参数指定 broker的
主机名和端口。
92.1列出并描述群组
在使用旧版本的消费者客户端时,可以使用--z00keer和-1ist参数列出消费者群
组;在使用新版本的消费者客户端时,则要使用- bootstrap- server、-lst和--new
consumer
参数。
示例:列出旧版本的消费者群组。
kafka-consumer -groups. sh --zookeeper
zoo1. example. com: 2181/kafka-cluster --list
console-consumer-79697
consumer
示例:列出新版本的消费者群组。
#f kafka-consumer -groups. sh --new-consumer --bootstrap-server
kafka1. example. com: 9092/kafka-cluster --list
kafka- python-test
my-new-Consumer
140第9章对于列出的任意群组来说,使用-- describe代替-1ist,并通过- group指定特定的群组,
就可以获取该群组的详细信息。它会列出群组里所有主题的信息和每个分区的偏移量。
示例:获取旧版本消费者群组 testgroup的详细信息。
kafka-consumer-groups sh--zookeeper zoo1 examp le. com: 2181/ kafka-cluster
describe --group testgroup
GROUP
TOPIC
PARTITION
CURRENT-OFFSET LOG-END-OFFSET LAG
OWNER
consumer
my-topic
0
1688
1688
myconsumer_ host1. example. com-1478188622741-7dab5ca7-0
consumer
my-toplc
1418
1418
myconsumer host1 example. com-1478188622741-7dab5ca7-0
consumer
my -topic
1314
1315
1
myconsumer host1 example. com-1478188622741-7dab5ca7-o
consumer
my-topic
2012
myconsumer host1 example. com-1478188622741-7dab5ca7-0
consumer
y-topic
1089
1089
myconsumer host1 example. com- 1478188622741-7dab5ca7-0
consumer
my-toplc
1429
1432
myconsumer host1 example. com- 1478188622741-7dab5ca7-0
consumer
my -topic
6
1634
1634
myconsumer_host1. example. com-1478188622741-7dab5ca7-0
consumer
my-toplc
2261
2261
myconsumer_ host1.example. com-1478188622741-7dab5ca7-0
输出结果里包含了如表9-1所示的字段。
表9-1:输出结果中的字段
字段描述
GROUP
消费者群组的名字
TOPIC
正在被读取的主题名字
PARTITION
正在被读取的分区ID
CURRENT-OFFSET消费者群组最近提交的偏移量,也就是消费者在分区里读取的当前位置
LOG-END- OFFSET当前高水位偏移量,也就是最近一个被读取消息的偏移量,同时也是最近一个被提
交到集群的偏移量
LAG
消费者的 CURRENT-OFFSET和 broker的LOG- END-OFFSET之间的差距
OWNER
消费者群组里正在读取该分区的消费者。这是一个消费者的ID,不一定包含消费
者的主机名
管理 Kafka
14192.2删除群组
只有旧版本的消费者客户端才支持删除群组的操作。删除群组操作将从 Zookeeper上移除
整个群组,包括所有已保存的偏移量。在执行该操作之前,必须关闭所有的消费者。如
果不先执行这一步,可能会导致消费者出现不可预测的行为,因为群组的元数据已经从
keeper
上移除了。
示例:删除消费者群组 testgroup
kafka-consumer -groups. sh--zookeeper
zoo1. example. com: 2181/kafka-cluster --delete --group testgroup
De leted all consumer group information for group testgroup in
zookeeper
该命令也可以用于在不删除整个群组的情况下删除单个主题的偏移量。再次强调,在进行
删除操作之前,需要先关闭消费者,或者不要让它们读取即将被删除的主题。
示例:从消费者群组 testgroup里删除 my-topic主题的偏移量。
kafka-consumer -groups. sh--zookeeper
zoo1. example. com: 2181/kafka-cluster -delete --group testgroup
topic my- topic
De leted consumer group information for group testgroup topic
my-topic in zoo keeper
#
9.2.3偏移量管理
除了可以显示和删除消费者群组(使用了旧版本消费者客户端)的偏移量外,还可以获取
偏移量,并保存批次的最新偏移量,从而实现偏移量的重置。在需要重新读取消息或者因
消费者无法正常处理消息(比如包含了非法格式的消息)需要跳过偏移量时,需要进行偏
移量重置。
管理已经提交到 Kafka的偏移量
目前,还没有工具可以用于管理由消费者客户端提交到Kaka的偏移量,管理
功能只对提交到 Zookeeper的偏移量可用。另外,为了能够管理提交到Kaka
的消费者群组偏移量,需要在客户端使用相应的API来提交群组的偏移量。
1.导出偏移量
Kaka没有为导出偏移量提供现成的脚本,不过可以使用 kafka-run-class. sh脚本调用底层的
Java类来实现导出。在导出偏移量时,会生成一个文件,文件里包含了分区和偏移量的信
息。偏移量信息以一种导入工具能够识别的格式保存在文件里。每个分区在文件里占用
行,格式为:/ consumers/GROUPNAME/offsets/topic/TOPICNAME/PARTITIONID. OFFSET。
示例:将群组 testgroup的偏移量导出到 offsets文件里
142第9章kafka-run-class. sh kafka. too ls. ExportZkoffsets
zkconnect zoo1 example. com: 2181/kafka-cluster --group testgroup
output-file offset
cat offsets
/consumers/testgroup/offsets/my-topic/0: 8905
/consumers/testgroup/offsets/my-topic/1: 8915
/consumers/testgroup/offsets/ my -topic/2: 9845
/consumers/testgroup/offsets/my-topic/3: 8072
/consumers/testgroup/offsets /my-topic/4: 8008
/consumers/testgroup/offsets/my -topic/5: 8319
/consumers/testgroup/offsets/my-topic/6: 8102
/consumers/testgroup/offsets/ my-topic/7: 1273
#
2.导入偏移量
偏移量导入工具与导出工具做的事情刚好相反,它使用之前导岀的文件来重置消费者群组
的偏移量。一般情况下,我们会导出消费者群组的当前偏移量,并将导出的文件复制一份
(这样就有了一个备份),然后修改复制文件里的偏移量。这里要注意,在使用导入命令
时,不需要使用-- group参数,因为文件里已经包含了消费者群组的名字。
先关闭消费者
在导入偏移量之前,必须先关闭所有的消费者。如果消费者群组处于活跃状
态,它们不会读取新的偏移量,反而有可能将导人的偏移量覆盖掉
示例:从 offsets文件里将偏移量导入到消费者群组 testgroup
kafka-run-class. sh kafka. tools. ImportZkoffsets --zkconnect
zoo1. example. com: 2181/kafka-cluster --input -file offsets
9.3动态配置变更
我们可以在集群处于运行状态时覆盖主题配置和客户端的配额参数。我们打算在未来增加
更多的动态配置参数,这也是为什么这些参数被单独放进了 kafka-configs.sh。这样就可以
为特定的主题和客户端指定配置参数。一旦设置完毕,它们就成为集群的永久配置,被保
存在 Zookeeper上, broker在启动时会读取它们。不管是在工具里还是文档里,它们所说
的动态配置参数都是基于“主题”实例或者“客户端”实例的,都是可以被“覆盖”的。
与之前介绍的工具一样,这里也需要通过… zookeeper参数提供 Zookeeper集群的连接字符
串。在下面的例子里,Zookeeper的连接字符串是“zool.example.com:218l/kafka-cluster”
9.3.1覆盖主题的默认配置
为了满足不同的使用场景,主题的很多参数都可以进行单独的设置。它们大部分都有
broker级别的默认值,在没有被覆盖的情况下使用默认值。
更改主题配置的命令格式如下。
管理 Kafka|143kafka-configs. sh --zookeeper zoo1 example. com: 2181/ kafka-cluster
alter - -entity-type topics --entity -name <topic name>
add-config <key>=<value>[, <key>=<value>.]
可用的主题配置参数(键)如表9-2所示。
表92:可用的主题配置参数
配置项
描述
cleanup. policy
如果被设置为 compact,只有最新包含了指定key的消息会被保留下
来(压缩日志),其他的被丢弃掉
compression type
broker在将消息批次写入磁盘时所使用的压缩类型,目前支持
gzip”、“ snappy"和“lz4
delete retention. ms
被标识为待删除的数据能够保留多久,以ms为单位。该参数只对压
缩日志类型的主题有效
file delete delay. ms
从磁盘上删除日志片段和索引之前可以等待多长时间,以ms为单位
flush. messages
需要收到多少个消息才能将它们刷新到磁盘
ush. ms
在将消息刷新到磁盘之前可以等待多长时间,以ms为单位
index interval bytes
日志片段的两个索引之间能够容纳的消息字节数
max message bytes
最大消息字节数
message format. version
broker将消息写入磁盘时所使用的消息格式,必须是有效的API版
本号(比如“0.10.0”)
message timestamp difference. maxms消息自带的时间戳和 broker收到消息时的时间戳之间最大的差值,
以ms为单位。该参数只在 messsage. timestamp.type被设为 Create
Tine时有效
messagetimestamp type
在将消息写入磁盘时使用哪一种时间戳。目前支持两种值,其中
CreateTime指客户端指定的时间戳,而 Log AppendTime指消息被写
入分区时的时间戳
min cleanable. dirty ratio
日志压缩器压缩分区的频率,使用未压缩日志片段数与总日志分段
数之间的比例来表示。该参数只对压缩日志类型的主题有效
minInsync replicas
可用分区的最少同步副本
preallocate
如果被设为true,需要为新的日志片段预分配空间
retention bytes
主题能够保留的消息量,以字节为单位
retention. ms
主题需要保留消息多长时间,以ms为单位
ment bytes
日志片段的消息字节数
segment Index bytes
单个日志片段的最大索引字节数
segment JItter. ms
滚动日志片断时,在 segment,ns基础上增加的随机毫秒数
segment. ms
日志片段多长时间滚动一次,以ms为单位
unclean. leader election. enable
如果被设为true,不彻底的首领选择无效
示例:将主题 my-topic的消息保留时间设为1个小时(3600000)。
kafka-configs sh --zookeeper zoo1 example. com: 2181/ kafka-cluster
alter -.entity-type topics -entity-name my -topic --add-config
retention. ms=3600000
Updated config for topic: my-topic
144第9章9.3.2覆盖客户端的默认配置
对于 Kafka客户端来说,只能覆盖生产者配额和消费者配额参数。这两个配额都以字节每
秒为单位,表示客户端在毎个 broker上的生产速率或消费速率。也就是说,如果集群里有
5个 broker,生产者的配额是10MB/s,那么它可以以10MB/s的速率在单个 broker上生成
数据,总共的速率可以达到50MB/S。
客户端|D与消费者群组
客户端⑩可以与消费者群组的名字不一样。消费者可以有自己的ID,因此
不同群组里的消费者可能具有相同的ID。在为消费者客户端设置ID时,最
好使用能够表明它们所属群组的标识符,这样便于群组共享配额,从日志里
查找负责请求的群组也更容易一些。
更改客户端配置的命令格式如下:
kafka-configs. sh--zookeeper zoo1 example. com: 2181/kafka-cluster
alter - entity-type clients --entity-name <client ID>
add-config <key>=evalue>l, <key>=evalue>.]
可用的客户端配置参数(键)如表93所示。1
表93:可用的客户端配置参数
配置项
描述
producer_bytes_rate单个生产者每秒钟可以往单个 broker上生成的消息字节数
consumer_ bytes_rate单个消费者每秒钟可以从单个 broker读取的消息字节数
9.3.3列出被覆盖的配置
使用命令行工具可以列出所有被覆盖的配置,从而用于检查主题或客户端的配置。与其他
工具类似,这个功能通过- descr让be命令来实现。
示例:列出主题my-opic所有被覆盖的配置。
kafka-configs sh --zookeeper zoo1 example. com: 2181/ kafka-cluster
describe -.entity-type topics --entity-name my-topic
Configs for topics: my -topic are
retention. ms=3600000, segment. ms=3600000
只能显示主题的覆盖配置
这个命令只能用于显示被覆盖的配置,不包含集群的默认配置。目前还无
法通过 Zookeeper或 Kafka实现动态地获取 broker本身的配置。也就是
说,在进行自动化时,如果要使用这个工具来获得主题和客户端的配置信
息,必须同时为它提供集群的默认配置信息。
管理 Kafka|1459.34移除被覆盖的配置
动态的配置完全可以被移除,从而恢复到集群的默认配置。可以使用…-ater命令和
- delete-conf讠g参数来删除被覆盖的配置。
示例:删除主题 my-topic的 retention. ms覆盖配置。
kafka-configs sh --zookeeper zoo1 example. com: 2181/ kafka-cluster
alter - entity-type topics --entity-name my -topic
delete-config retentionms
Updated config for topic:my -topic
94分区管理
Kafka工具提供了两个脚本用于管理分区,一个用于重新选举首领,另一个用于将分区分
配给 broker。结合使用这两个工具,就可以实现集群流量的负载均衡。
941首选的首领选举
第6章提到,使用多个分区副本可以提升可靠性。不过,只有其中的一个副本可以成为
分区首领,而且只有首领所在的 broker可以进行生产和消费活动。 Kafka将副本清单里
的第一个同步副本选为首领,但在关闭并重启 broker之后,并不会自动恢复原先首领的
身份。
自动首领再均衡
broker有一个配置可以用于启用自动首领再均衡,不过到目前为止,并不建
议在生产环境使用该功能。自动均衡会带来严重的性能问题,在大型的集群
里,它会造成客户端流量的长时间停顿。
通过触发首选的副本选举,可以让 broker重新获得首领。当该事件被触发时,集群控制器
为分区重新选择理想的首领。选举过程一般不会造成负面的影响,因为客户端可以自动
跟踪首领的变化。也可以通过 kafka-preferred-replica-election.sh工具手动触发选举。
示例:在一个包含了1个主题和8个分区的集群里启动首选的副本选举
kafka-preferred-replica-election. sh--zookeeper
zoo1. example. com: 2181/ kafka-cluster
Successfully started preferred replica election for partitions
Set([my-topic, 5], [my -topic, o], [
7l, [my-topic, 4
Imy-topic, 6], [my-topic, 2], [my-topic, 3], [my-topic, 11)
因为集群包含了大量的分区,首选的副本选举有可能无法正常进行。在进行选举时,集
群的元数据必须被写到 Zookeeper的节点上,如果元数据超过了节点允许的大小、(默认是
IMB),那么选举就会失败。这个时候,需要将分区清单的信息写到一个JSON文件里,
并将请求分为多个步骤进行。JSON文件的格式如下:
146第9章partitions":
artition": 1
"topic:"foo
tition": 2
topic":foobar
示例:通过在 partitions. ]son文件里指定分区清单来启动副本选举。
#t kafka-preferred-replica-election sh--zookeeper
zoo1. example. com: 2181/kafka-cluster --path-to-json-file
partitions. json
Successfully started preferred replica election for partitions
Set([my-topic, 1], [my-topic, 2], [my-topic, 31)
942修改分区副本
在某些时候,可能需要修改分区的副本。以下是一些需要修改分区副本的场景。
主题分区在整个集群里的不均衡分布造成了集群负载的不均衡。
broker离线造成分区不同步。
·新加入的 broker需要从集群里获得负载。
可以使用 kafka-reassign-partitions. sh工具来修改分区。使用该工具需要经过两个步骤:第
步,根据 broker清单和主题清单生成一组迁移步骤;第二步,执行这些迁移步骤。第
个步骤是可选的,也就是可以使用生成的迁移步骤验证分区重分配的进度和完成情况。
为了生成迁移步骤,需要先创建一个包含了主题清单的JSON文件,文件格式如下(目前
的版本号都是1):
topics": L
topic" :"foo
topic":fc
version": 1
示例:为 topics. ]son文件里的主题生成迁移步骤,以便将这些主题迁移到 broker o和
broker1上
管理Kaka|147kafka-reassign -partitions. sh --zookeeper
zoo1. example. com: 2181/kafka-cluster
generate --topics-to-move-json-file topics json --broker-list 0,1
Current partition replica assignment
t"version":1,"partitions"[["topic": "my-topic","partition": 5, "replicas": [0, 1]1
[ topic:"my-topic","partition":10, "replicas": [1, 0],["topic": my
topic ,"partition:1,replicas": [0, 113,[topic":"my-topic","partition": 4, "repli
cas":[1,0],t topic":"my-topic","partition": 7, "replicas":[0, 11,[topic:"m
topic,"partition:6, replicas: [1,013, topic": "my-topic", "partition":
3, replicas": [0, 1]],"topic": my-topic","partition": 15, "replicas": [0, 111
ttopic: my-topic","partition": 0, "replicas":[1, 0],["topic": "my
topic",partition: 11, replicas":[0, 1],I "topic": "my-topic", "partition": 8, "repli
cas":[1,0], topic": " my-topic","partition": 12, "replicas":[1,0],"topic":"my-
topic ,"partition":2,"replicas: [1,0],I topic":"my-topic","partition":
13, replicas":[0, 1],[" topic":"my-topic","partition": 14,"replicas":[1,0J]
[topic:" my-topic","partition": 9,"replicas": [0, 11113
Proposed partition reassignment configuration
t"": 1, "partitions":[["topic": " my-topic", "partition":, "replicas": [0, 11)
t"":my-topic","partition": 10, "replicas": [1, 0],["topic": "my-
topic","partition": 1, replicas: [0, 1,[ topic": "my -topic","partition": 4, "repli
cas":[1,0]],[ topic":"my-topic""partition: 7,"replicas":[0,1],[topic":"my-
topic ,"partition:6, replicas":[1,0],["topic": " my-topic","partition"
15, replicas":[,11],"topic":"my-topic", "partition:0, "replicas": [1,0J]
my-topic",partition":3, replicas: [0, 1,[ topic":"my-
topic,"partition": 11, replicas":[0, 1],["topic: my-topic","partition": 8, "repli
Cas":[1,0]},"topic":"my-topic","partition":12,"repLicas":[1,0]},["topic":"ny-
topic ,"partition": 13, "replicas:[0, 1]],["topic": " my-topic","partition":
2,"replicas":[1,01,"topic": "my-topic", "partition": 14, "replicas": [1,0J]
t topic":"my-topic","partition 9, "replicas": [0, 11
broker的⑩D以逗号分隔,并作为参数提供给命令行工具。这个工具会在标准控制台上输出
两个JSON对象,分别描述了当前的分区分配情况以及建议的分区分配方案。这些JSON
对象的格式如下:{" partitions":[I" topic":"my- topic"," partition":0," replicas"
[1,2]}]," versLon":1}。
可以把第一个JSON对象保存起来,以便在必要的时候进行回滚。第二个JSON对象应该
被保存到另一个文件里,作为 kafka-reassign- partitions. sh工具的输入来执行第二个步骤。
示例:使用 reassign]son来执行建议的分区分配方案。
kafka-reassign-partitions. sh--zookeeper
zoo1. example. com: 2181/ kafka-cluster--execute
reassignment-json-file reassign json
Current partition replica assignment
"version":1,"partitions":[{"topic":"my-topic","partition":5,"replicas":[θ,1]},
topic:" my-topic",partition": 10, replicas": [1,0]],["topic":"my-
topic",partition": 1, replicas [0, 1]],[topic": "my-topic", "partition": 4, "repli
cas":[1,01,"topic":" my-topic","partition": 7, "replicas": [0, 1]3,["topic":"my-
topic ","partition: 6,replicas": [1,0, topic: my-topic","partition":
3,replicas":[0, 1]], "topic":my-topic", "partition": 15, "replicas":[0, 111
148
第9章topic":"my-topic","partition":O, " replicas": [1,0],[ topic":"my-
topic","partition":11, "replicas":[0, 11],["topic" :"my-topic","partition":8,"repli
cas":[1,0],["topic": "my-topic","partition 12, "replicas":[1,0),t"topic:"my-
topic","partition :2,"replicas": [1, 01,t
t topic": my-topic","partition
13, "replicas":[0, 1),["topic": "my-topic", "partition": 14, "replicas": [1,OJJ
["topic":"my-topic","partition": 9, replicas":[0, 111]
Save this to use as the --reassignment- json-file option during
rollback
Successfully started reassignment of partitions ["version":1,"partitions
[[topic:"my-topic","partition: 5, replicas: [0, 1,[ topic":"my
topic","partition":,"replicas": [1,o]],["topic": "my-topic","partition": 7,"repl
cas: [0, 1]], topic:"my-topic","partition":13, replicas":[0, 1]], topic":"my-
topic","partition 4, replicas [1,0, topic: my-topic","partition":
12, replicas:[1,0],f topic":"my-topic","partition: 6, "replicas": [1,0J]
"topic":"my-topic","partition": 11, " replicas": [0, 1]),t topic":"my-
topic","partition: 10, "replicas [1, 0]], topic: my-topic","partition": 9, "repli
cas":[0,1],[topic":"my-topic"," partition
"replicas":[1,0],t topic":"my
topic","partition : 14, "replicas":[1, 0], topic:"my-topic","partition"
3, replicas": [0,1]], topic: my-topic""partition: 1, replicas [0,1]]
partition":8, " replicas":[1, 0J17.replicas":[0, 1],[topic":"my-topic
["topic": my-topic","partition": 15,
该命令会将指定分区的副本重新分配到新的 broker上。集群控制器通过为每个分区添加
新副本实现重新分配(增加复制系数)。新的副本将从分区的首领那里复制所有数据。根
据分区大小的不同,复制过程可能需要花一些时间,因为数据是通过网络复制到新副本上
的。在复制完成之后,控制器将旧副本从副本清单里移除(恢复到原先的复制系数)。
为重新分配副本进行网络优化
如果要从单个 broker上移除多个分区,比如将 broker移出集群,那么在重新
分配副本之前最好先关闭或者重启 broker。这样,这个 broker就不再是任何
个分区的首领,它的分区就可以被分配给集群里的其他 broker(只要没有
启用自动首领选举)。这可以显著提升重分配的性能,并减少对集群的影响,
因为复制流量将会被分发给多个 broker
在重分配进行过程中或者完成之后,可以使用 kafka-reassign-partitions. sh工具验证重分配
的状态。它可以显示重分配的进度、已经完成重分配的分区以及错误信息(如果有的话)
为了做到这一点,需要在执行过程中使用JSON对象文件。
示例:验证 reassignjson文件里指定的分区重分配情况。
kafka-reassign- partitions. sh --zookeeper
zoo1. example. com: 2181/kafka-cluster --verify
reassignment- json-file reassign json
Status of partition reassignment
Reassignment of partition [my-topic, 5] completed successfully
Reassignment of partition [my-topic, 0] completed successfully
Reassignment of partition [my-topic, 7 completed successfull
Reassignment of partition [my -topic, 13 completed successfully
管理 Kafka|149Reassignment of partition [my-topic, 4] completed successfully
Reassignment of partition [my-topic, 12] completed successfully
Reassignment of partition [my-topic, 6] completed successfully
Reassignment of partition [my-topic, 11] completed successfully
Reassignment of partition [my-topic, 10] completed successfully
Reassignment of partition [my-topic, 9] completed successfully
Reassignment of partition [my-topic, 2] compl
leted successfully
Reassignment of partition [my -topic, 14
leted
fully
Reassignment of partition [my-topic, 3] completed successfully
Reassignment of partition [my-topic, 1
leted successfully
Reassignment of partition [my-topic, 15] completed successfully
Reassignment of partition [my-topic, 8] completed successfully
#
分批重分配
分区重分配对集群的性能有很大影响,因为它会引起内存页缓存发生变化,
并占用额外的网络和磁盘资源。将重分配过程拆分成多个小步骤可以将这种
影响降到最低。
94.3修改复制系数
分区重分配工具提供了一些特性,用于改变分区的复制系数,这些特性并没有在文档里
说明。如果在创建分区时指定了错误的复制系数(比如在创建主题时没有足够多可用的
broker),那么就有必要修改它们。这可以通过创建一个JSON对象来完成,该对象使用分
区重新分配的执行步骤中使用的格式,显式指定分区所需的副本数量。集群将完成重分配
过程,并使用新的复制系数。
例如,假设主题 my-topIc有一个分区,该分区的复制系数为1
partitions":
"topic":"my-toplc
partition": 0
replicas":[
version": 1
在分区重新分配的执行步骤中使用以下JSON可以将复制系数改为2
partitions":[
partition":0
replicas":[
150第9章topic: my-topic
version": 1
也可以通过类似的方式减小分区的复制系数。
944转储日志片段
如果需要查看某个特定消息的内容,比如一个消费者无法处理的“毒药”消息,可以使用工
具来解码分区的日志片段。该工具可以让你在不读取消息的情况下査看消息的内容。它接受
个以逗号分隔的日志片段文件清单作为参数,并打印出每个消息的概要信息和数据内容
示例:解码日志片段00000000368601log,显示消息的概要信息。
kafka-run-class sh kafka. tools. DumpLog Segments -files
0000000052368601.Log
Dumping000000052368601.log
Starting offset: 52368601
offset: 52368601 position: 0 NoTimestampType:-1 isvalid: true
pay loadsize: 661 magic: 0 compresscodec: GZIPCompression Codec crc
1194341321
offset: 52368603 position 687 NoTimestamp Type:-1 isvalid: true
pay loadsize: 895 magic: 0 compresscodec GZIPCompression Codec crc
278946641
offset: 52368604 position: 1608 NoTimestampType: -1 isvalid: true
payloadsize: 665 magic: 0 compresscodec: GZIPCompression Codec crc:
3767466431
offset: 52368606 position: 2299 NoTimestampType:-1 isvalid: true
payloadsize: 932 magic: 0 compresscodec: GZIPCompressionCodec crc
2444301359
示例:解码日志片段0000000368601log,显示消息的数据内容
kafka-run-class sh kafka. tools. DumpLog Segments --files
00000000000052368601.log --print-data-Log
offset: 52368601 position: 0 NoTimestampType: -1 isvalid: true
payloadsize: 661 magic: 0 compresscodec: GZIPCompression Codec crc
1194341321 pay load: test message 1
offset: 52368603 position: 687 NoTimestamp Type:-1 isvalid: true
pay loadsize: 895 magic: 0 compresscodec: GZIPCompression Codec crc:
278946641 pay load: test message 2
offset: 52368604 position: 1608 NoTimestampType: -1 isvalid: true
payloadsize: 665 magic: 0 compresscodec: GZIPCompression Codec crc
3767466431 pay load: test message 3
offset: 52368606 position: 2299 NoTimestampType: -1 isvalid: true
payloadsize: 932 magic: 0 compresscodec: GZIPCompression Codec crc
2444301359 pay load: test message 4
这个工具也可以用于验证日志片段的索引文件。索引用于在日志片段里査找消息,如果索
管理 Kafka|151引文件损坏,会导致消费者在读取消息时出现错误。 broker在不正常启动(比如之前没有
正常关闭)时会自动执行这个验证过程,不过也可以手动执行它。有两个参数可以用于指
定不同程度的验证,-- index- sanity- check将会检查无用的索引,而-- verify- index-only
将会检查索引的匹配度,但不会打印出所有的索引。
示例:验证日志片段000000368601log索引文件的正确性。
#f kafka-run-class sh kafka. tools. DumpLog Segments --files
60000652368601. index,000000052368601.Log
-index-sanity-check
Dumping00000000000052368601. index
00000000000052368601 index passed sanity check
Dumping 00000000000052368601 log
Starting offset: 52368601
offset: 52368601 position: 0 NoTimestampType: -1 isvalid: true
pay loadsize: 661 magic: 0 compresscodec: GZIPCompression Codec crc:
1194341321
offset: 52368603 position: 687 NoTimestampType: -1 isvalid: true
pay loadsize: 895 magic: 0 compresscodec: GZIPCompression Codec crc:
278946641
offset: 52368604 position: 1608 NoTimestampType: -1 isvalid: true
payloadsize: 665 magic: 0 compresscodec: GZIPCompression Codec crc:
3767466431
94.5副本验证
分区复制的工作原理与消费者客户端类似:跟随者 broker定期将上一个偏移量到当前偏移
量之间的数据复制到磁盘上。如果复制停止并重启,它会从上一个检査点继续复制。如果
之前复制的日志片段被删除,跟随者不会做任何补偿
可以使用kaka- replica- verification,sh工具来验证集群分区副本的一致性。它会从指定分区
的副本上获取消息,并检査所有副本是否具有相同的消息。我们必须使用正则表达式将待
验证主题的名字传给它。如果不提供这个参数,它会验证所有的主题。除此之外,还需要
显式地提供 broker的地址清单
副本验证对集群的影响
副本验证工具也会对集群造成影响,因为它需要读取所有的消息。另外,它
的读取过程是并行进行的,所以使用的时候要小心
示例:对 broker 1和 broker2上以my-开头的主题副本进行验证。
kafka-replica- verification. sh --broker -list
kafka. example. com: 9092, kafka2. example. com: 9092 --topic-white-list 'my-*'
2016-11-23 18: 42: 08, 838: verification process is started
2016-11-23 18: 42: 38, 789: max Lag is o for partition [my-topic, 71
at offset 53827844 among 10 partitions
2016-11-23 18: 43: 08, 790: max Lag is 0 for partition [my-topic, 7]
at offset 53827878 among 10 partitions
152第9章9.5消费和生产
在使用Kaka时,有时候为了验证应用程序,需要手动读取消息或手动生成消息。这个时
候可以借助kaka- console-consumer.sh和 kafka-console-producer.sh这两个工具,它们包装
了Java客户端,让用户不需要编写整个应用程序就可以与Kaka主题发生交互。
将结果输出到其他应用程序
有时候,我们可能需要编写应用程序将控制台消费者和控制台生产者包装起
A\来,用它读取消息,并把消息传给另一个应用程序去处理。这种应用程序太
过脆弱,应该尽量避免编写这类应用程序。我们无法保证控制台消费者不丢
失数据,也无法使用控制台生产者的所有特性,而且它发送数据的方式也很
奇怪。最好的方式是直接使用Java客户端,或者使用其他基于Kaka协议实
现的第三方客户端(可能是使用其他语言开发的)。
9.5.1控制台消费者
kafka-console-consumer. sh工具提供了一种从一个或多个主题上读取消息的方式。消息被打
印在标准输出上,消息之间以空行分隔。默认情况下,它会打印没有经过格式化的原始消
息字节(使用 DefaultFormatter)。它有很多可选参数,其中有一些基本的参数是必选的
检查工具版本
使用与 Kafka broker相同版本的消费者客户端,这一点是非常重要的。旧版
个本的控制台消费者与 Zookeeper之间不恰当的交互行为可能会影响到集群。
第一步要指定是否使用新版本的消费者,并指定 Kafka集群的地址。如果使用的是旧版本的
消费者,只需要提供-z0 keeper参数,后面跟上Kaka集群的连接字符串。对于上面的例子
来说,参数可能是-2kee6o01.example.com:2181/kafka-cluster。如果使用了新版本的消
费者,必须使用-new- consumer和- broker-list,- broker-list后面需要跟上以逗号相隔的
broker地址列表,比如-broker-listkafka1example.com:9092kafka2example.con:9092。
下一步要指定待读取的主题。这里有3个可用参数,分别是- topic、- whitelist
和- blacklist。此处允许只指定一个参数。- topic用于指定单个待读取的主题,
whitelist和-- blacklist后面跟着一个正则表达式(在命令行里可能需要转义)。与白
名单正则表达式匹配的主题将会被读取,与黑名单正则表达式匹配的主题不会被读取。
示例:使用旧版消费者读取单个主题 my-topic
kafka-console-consumer sh --zookeeper
zoo1. example. com: 2181/kafka-cluster --topic my-topic
ple message 1
sample message 2
ACProcessed a total of 2 messages
管理 Kafka153除了基本的命令行参数外,也可以把消费者的其他配置参数传给控制台消费者。可以通过
两种方式来达到这个目的,这取决于需要传递的参数个数以及个人喜好。第一种方式是将
配置参数写在一个文件里,然后通过
consumer con
fg CONfiG讠LE指定配置文件,其中
CONFIGFILE就是配置文件的全路径。另一种方式是直接在命令行以- consumer- property
KEY= VALUE的格式传递一个或多个参数,其中KEY指参数的名字, VALUE指参数的值。
这种方式在设置消费者属性时会很有用,比如设置群组的ID
容易混淆的命令行参数
控制台消费者和控制台生产者有一个共同的参数- property,千万不要将这
n个参数与 umer-property fH- producer-property i混,ppry参
数用于向消息格式化器传递配置信息,而不是给客户端本身传递配置信息。
控制台消费者的其他常用配置如下。
formatter classnaMe
指定消息格式化器的类名,用于解码消息,它的默认值是 kafka. tools. Default Formatter
from-beginning
指定从最旧的偏移量开始读取数据,否则就从最新的偏移量开始读取。
nax-messages NUM
指定在退出之前最多读取NUM个消息
partition NUM
指定只读取ID为NUM的分区(需要新版本的消费者)。
1.消息格式化器的选项
除了默认的消息格式化器之外,还有其他3种可用的格式化器。
kafka. tools. LoggingMessageFormatter
将消息输出到日志,而不是输出到标准的输出设备。日志级别为ⅠNFO,并且包含了时
间戳、键和值。
kafka. tools. ChecksumMessageFormatter
只打印消息的校验和。
kafka. too ls. NoOpMessage Formatter
读取消息但不打印消息。
kafka. tools. DefaultMessageFormatter有一些非常有用的配置选项,这些选项可以通过
property命令行参数传给它
print timestamp
如果被设为true,就会打印每个消息的时间戳。
154第9章prLI
nt. key
如果被设为true,除了打印消息的值之外,还会打印消息的键。
ey separator
指定打印消息的键和消息的值所使用的分隔符。
line separator
指定消息之间的分隔符。
key deserializer
指定打印消息的键所使用的反序列化器类名。
value, deserializer
指定打印消息的值所使用的反序列化器类名。
反序列化类必须实现
org. apache. kafka. common. serialization. Deserializer接口,控制
台消费者会调用它们的 tostring()方法获取输出结果。一般来说,在使用 kafka_ console
consumer.sh工具之前,需要通过环境变量 CLASSPATH将这些实现类添加到类路径里。
2.读取偏移量主题
有时候,我们需要知道提交的消费者群组偏移量是多少,比如某个特定的群组是否在提交
偏移量,或者偏移量提交的频度。这个可以通过让控制台消费者读取一个特殊的内部主题
onsumer_ offsets来实现。所有消费者的偏移量都以消息的形式写到这个主题上。为了
解码这个主题的消息,需要使用 kafka. coordinator. GroupMetadataManager$ soffsetsMessage
Formatter这个格式化器。
示例:从偏移量主题读取一个消息。
kafka-console-consumer sh --zookeeper
zoo1. example. com: 2181/kafka-cluster -. topic consumer_ offsets
formatter ' kafka. coordinator. GroupMetadataManager SOffsetsMessage
Formatter --max-messages 1
Imy-group-name, my-topic, 0]: [OffsetMetadata[481690879, NO METADATA
CommitTime 1479708539051, ExpirationTime 1480313339051
Processed a total of 1 messages
9.52控制台生产者
与控制台消费者类似, kafka-console-producer. sh工具可以用于向Kaka主题写入消息。默
认情况下,该工具将命令行输入的毎一行视为一个消息,消息的键和值以Tab字符分隔
(如果没有出现Tab字符,那么键就是nul)
改变命令行的读取行为
如果有必要,可以使用自定义类来读取命令行输入。自定义类必须继承
kafka. common. MessageReader类,并负责创建 Producer Record对象。然后在
命令行的 ine-reader参数后面指定这个类,并确保包含这个类的JAR包
已经加入到类路径里。
管理 Kafk
155控制台生产者有两个参数是必须指定的:- broker-list参数指定了一个或多个 broker,它
们以逗号分隔,格式为 hostname:port;另一个参数- topic指定了生成消息的目标主题。
在生成完消息之后,需要发送一个EOF字符来关闭客户端。
示例:向主题 my-topic生成两个消息、。
kafka-console-producer sh --broker-list
kafka1. example. com: 9092, kafka. example. com: 9092--topic my-topic
ample message 1
sample message 2
与控制台消费者一样,控制台生产者可以接受普通生产者的配置参数。这也可以通过两种
方式来实现,具体用哪一种取决于你想要传递的参数个数和个人喜好。第一种方式是通过
producer. config CONFIGFILE指定消费者配置文件,其中 CONFIGFILE是配置文件的全
路径。另一种方式是直接在命令行以- producer- property KEy= VALUE的格式传递一个或多
个参数,其中KEY指参数的名字, VALUE指参数的值。这种方式在设置生产者属性时会
很有用,比如消息批次的相关配置(如nger,ms或 batch size)。
控制台生产者有很多命令行参数可用于调整它的行为
key-serializer CLASSNAME
指定消息键的编码器类名,默认是 kafka. serializer. DefaultEncoder。
value -serializer ClasSNAME
指定消息值的编码器类名,默认是kaka. serializer. DefaultEncoder
compression - codec STrING
指定生成消息所使用的压缩类型,可以是none、gzip、 snappy或Lz4,默认值是gzip。
sync
指定以同步的方式生成消息,也就是说,在发送下一个消息之前会等待当前消息得到
确认。
创建自定义序列化器
自定义序列化器必须继承 kafka. serializer, Encode类,可以用于做一些转换
操作,比如将JSON格式的字符串转成其他格式,如Avro,让这些消息可以
被保存到主题上。
文本行读取器的配置参数
kafka. tools. LineMessageReader类负责读取标准输入,并创建消息记录。它也有一些非常
有用的配置参数,可以通过 property命令行参数把这些配置参数传给控制台生产者。
lgnore error
如果被设为 false,那么在 parse.key被设为true或者标准输入里没有包含键的分隔符
时就会抛出异常,默认为true。
156第9章parse key
如果被设为 false,那么生成消息的键总是nu,默认为true。
key separator
指定消息键和消息值之间的分隔符,默认是Tab字符。
在生成消息时, LineMessageReader使用第一个出现的 key separator作为分隔符来拆分输
入。如果在分隔符之后没有其他字符,那么消息的值为空。如果输入里没有包含分隔符
或者 parse.key被设为 false,那么消息的键就是nuLL。
9.6客户端ACL
命令行工具kaka-acls.sh可以用于处理与客户端访问控制相关的问题,它的文档可以在
Apache Kafka官方网站上找到。
97不安全的操作
有一些管理操作虽然在技术上是可行的,但如果不是非常有必要,就不应该尝试那么做
比如,你正在诊断一个问题,但已经没有其他可行的办法,或者发现了一个bug,需要
个临时解决方案。这些操作一般在文档里不会有相关的说明,而且未经证实,有可能会给
应用程序带来风险。
这里会列举一些常见的操作,在紧急情况下可以使用它们。不过,一般情况下不建议执行
这些操作,而且在执行之前要慎重考虑。
此处有危险
本节介绍的操作将涉及保存在 Zookeeper上的元数据。除了这里提到的内容
以外,不要直接修改 Zookeeper的其他任何信息,一定要小心谨慎,因为这
些操作都是很危险的。
9.7.1移动集群控制器
每个Kaka集群都有一个控制器,它是运行在集群某个 broker上的一个线程。控制器负责
看管集群的操作,有时候需要将控制器从一个 broker迁移到另一个 broker上。例如,因为
出现了某些异常,控制器虽然还在运行,但已无法提供正常的功能。这时候可以迁移控制
器,但毕竟这也不是一般性的操作,所以不应该经常迁移控制器。
当前控制器将自己注册到 Zookeeper的一个节点上,这个节点处于集群路径的最顶层,
名字叫作/ controller。手动删除这个节点会释放当前控制器,集群将会进行新的控制
器选举。
972取消分区重分配
分区重分配的一般流程如下。
管理 Kafka|157(1)发起重分配请求(创建 Zookeeper节点)。
(2)集群控制器将分区添加到 broker上
(3)新的 broker开始复制分区,直到副本达到同步状态
(4)集群控制器从分区副本清单里移除旧的 broker。。
因为分区重分配是并行进行的,所以一般情况下没有理由取消一个正在进行中的重分配
任务。不过有一个例外的情况,比如在重分配进行到一半时, broker发生了故障并且无
法立即重启,这会导致重分配过程无法结束,进而妨碍其他重分配任务的进行(比如将
故障 broker的分区分配给其他 broker)。如果发生了这种情况,可以让集群忽略这个重分
配任务。
移除一个进行中的分区重分配任务的步骤如下。
(1)从 Zookeeper上删除/ admin/ reassign_ partitions节点。
(2)重新选举控制器(参见97.1节)。
检查复制系数
在取消进行中的分区重分配任务时,对于任何一个未完成重分配的分区来
说,旧的 broker都不会从副本清单里移除。也就是说,有些分区的复制系数
会比正常的大。如果主题的分区包含不一致的复制系数,那么 broker是不允
许对其进行操作的(比如增加分区)。所以建议检査分区是否仍然可用,并
确保分区的复制系数是正确的。
9.7.3移除待删除的主题
在使用命令行工具删除主题时,命令行工具会在 Zookeeper上创建一个节点作为删除主题
的请求。在正常情况下,集群会立即执行这个请求。不过,命令行工具并不知道集群是否
启用了主题删除功能。因此,如果集群没有启用主题删除功能,那么命令行工具发起的请
求会一直被挂起。不过这种挂起请求是可以被移除的。
主题的删除是通过在/ admin/ de lete topic节点下创建一个以待删除主题名字命名的子节
点来实现的。所以,删除了这些节点(不过不要删除/admn/ delete topic这个父节点)
也就移除了被挂起的请求。
9.7.4手动删除主题
如果集群禁用了主题删除功能,或者需要通过非正式的途径删除某些主题,那么可以进行
手动删除。这要求在线下关闭集群里所有的 broker。
先关闭 broker
在集群还在运行的时候修改 Zookeeper里的元数据是很危险的,这会造成集
群不稳定。所以,不要在集群还在运行的时候删除或修改 Zookeeper里的主
题元数据。
158第9章从集群里手动删除主题的过程如下。
(1)关闭集群里所有的 broker
(2)删除 Zookeeper路径/ brokers/topics/TOPICNAME,注意要先删除节点下的子节点。
(3)删除每个 broker的分区目录,这些目录的名字可能是 TOPICNAME-NUM,其中NUM是指分
区的ID。
(4)重启所有的 broker
9.8总结
运行一个 Kafka集群需要付出很大的努力,为了让 Kafka保持巅峰状态,需要做大量的配
置和维护。我们在这一章里介绍了Kaka的很多日常操作,比如经常会用到的主题管理和
客户端配置;也介绍了一些用于诊断问题的复杂操作,比如检查日志片段;最后还介绍了
些不安全的操作,这些操作在特殊的情况下可以帮你解决冋题。通过执行这些操作,你
就可以更好地管理 Kafka集群。
当然,如果没有进行适当的监控,管理集群就是一个不可能完成的任务。第10章将会讨
论如何对 broker和集群的健康状况以及操作进行监控,这样就可以知道 Kafka的运行状态
了。我们也会提供一些有关客户端(包括生产者和消费者)监控的最佳实践。
管理 Kafka|159第10章
监控 Kafka
Kafka应用程序包含了大量的度量指标,以至于很多人搞不清楚哪些是重要的,哪些可以
置之不理。它们所涉及的范围,从简单的流量速率度量指标到各种请求类型的时间度量指
标,既有主题级别的,也有分区级别的。这些度量指标为 broker的每一种行为提供了详细
的信息,不过它们也成为了Kaka系统监控者的“噩梦”。
这一章将详细介绍一些常用的关键性度量指标,以及如何根据这些指标采取相应的行动
也会介绍一些用于调试问题的度量指标。不过,这不是一个完整的度量指标清单。度量指
标清单会经常发生变化,而且很多度量指标只对有经验的Kaka开发人员有参考价值。
10.1度量指标基础
在介绍 Kafka broker和客户端的度量指标之前,先来讨论有关监控Java应用程序的基础知
识,以及有关监控和告警的最佳实践。学完本章知识,读者将会对应用程序的监控有一个
基本的了解,同时能明白度量指标的重要性。
10.1.1度量指标在哪里
Kaka提供的所有度量指标都可以通过 Java Management Extensions(MX)接口来访
问。要在外部监控系统里使用这些度量指标,最简单的方式是将负责收集度量指标的代理
( agent)连接到Kaka上。代理作为一个单独的进程运行在监控系统里,并连接到Kaka
的JMX接口上,例如使用 Nagios XI check_jmx插件或 Mstrans来连接JMX接口;也可
以直接在Kaka进程里运行一个JMX代理,然后通过HTTP连接访问度量指标,比如
Jolokia或MX4J。
本章将不会深入讨论如何设置监控代理,每一种代理都有多种使用方式。如果所在的组织
160没有监控Java应用程序的经验,那么可以考虑使用第三方的监控服务。如果采用了这种方
式,那就需要从服务供应商那里购买监控服务,由他们提供监控代理、度量指标收集点、
存储、图形化和告警。服务供应商可以搭建监控代理,并提供后续的服务支持。
找到JMX端口
broker将JMX端口作为整个 broker配置信息的一部分保存在 Zookeeper
上。所以,如果要通过编程的方式访问Kaka的JMX,比如管理工具需要
在没有端口配置的情况下连接到JMX,那么可以从 Zookeeper上获取端口
信息。/ brokers/ids/<ID>节点包含了JSON格式的 broker信息,里面有
JMX对应的主机名( hostname)和端口(jmx_port)。
10.12内部或外部度量
JMX接口提供的是内部度量指标,它们由被监控的应用程序生成。对于很多内部度量来说
(比如各个请求阶段的时间),使用内部度量指标是最好的选择。没有什么能比应用程序更
加了解自己了。还有一些度量指标,比如请求的整体时间、某种请求类型的可用性,可以
在应用程序外部进行度量。也就是说,这些度量指标是由客户端或其他第三方应用(对于
我们来说就是指 Kafka broker)提供的。另外也有一些度量指标,比如可用性( broker是
否可达)或延迟(请求需要的时间)。这些度量指标为被监控的应用程序提供了很有用的
外部视图。
网站健康监控就是我们所熟知的一种外部度量。一个正常运行的web服务器能够正常处理
请求,它向监控系统发送度量指标,一切看起来都很好。不过,Web服务器本身的防火墙
或服务器所处网络的防火墙可能导致客户端无法连接到服务器上。负责检查网站可访问性
的外部监控系统将会检测到这个问题,并发送告警
10.1.3应用程序健康检测
不管通过哪一种方式从Kaka收集监控信息,都要确保能够通过简单的健康检测来监控应
用程序本身的健康状况,这可以通过两种方式来实现。
使用外部进程来报告 broker的运行状态(健康检测)
·在 broker停止发送度量指标时发出告警(也叫作 stale度量指标)。
虽然第二种方式也是可行的,但有时候很难区分是 broker出现了问题还是监控系统本身出
现了问题。
如果监控的是 broker,可以直接连接到它的外部端口(就是客户端连接到 broker所使用的
端口),看看是否可以得到响应。如果监控的是 Kafka客户端,就会比较复杂,需要检査
进程是否处于运行状态,或者通过内部提供的方法来确定应用程序的健康状况。
10.1.4度量指标的覆盖面
Kafka提供了很多度量指标,如何选择合适的度量指标非常关键,特别是在基于这些度量
监控 Kafka|161指标定义告警的时候。太多难以确定严重程度的告警很容易让人们陷入“告警疲劳”,我
们难以为每一个度量指标定义恰当的阈值并保持更新,告警的可信度也会因此而下降。
些大覆盖面的告警用处更大。也就是说,这类告警会告诉我们某处出现了问题,然后我
们去收集更多的信息,以便确定问题的细节。想象一下汽车的“检査引擎”告警灯,如果
仪表盘上有100个不同的指示器,比如空气过滤器、油箱、排气管等,那么就会让人感到
很困惑。相反,如果用一个指示器就能告诉我们汽车出现了问题,然后我们再去找出问题
的其他细节,事情就会变得简单很多。这一章将介绍具有大覆盖面的度量指标,它们可以
让告警变得更简单
10.2 broker的度量指标
broker提供了很多度量指标,它们大部分都是底层的度量,由 Kafka开发者出于诊断或调
试的目的而增加,也有为预测将来需要的信息而添加,另外还有由Kaka用户请求添加,
以供日常操作所需。这些度量指标提供的信息几乎涵盖了 broker的每一项功能。它们很容
易造成信息过载,不过也有一些度量指标为Kaka的日常运行提供了必要的信息。
示例:谁来看着 watcher
很多组织使用Kaka收集应用程序和系统的度量指标与日志,然后供中心
监控系统使用,这样可以很好地解耦应用程序和监控系统。不过,对于
Kaka本身来说却存在一个问题,如果使用这个监控系统来监控Kaka,那
么当Kaka崩溃时,我们很可能无法感知到,因为监控系统的数据流也随
着消失了。
有一些办法可以解决这个问题。一种方法是使用一个单独的监控系统来监
控Kaka,这个系统不依赖Kaka提供的数据。另一种方法是,如果有多个
数据中心,可以将数据中心A的Kaka集群度量指标生成到数据中心B的
Kaka集群上,反之亦然。不管怎样,要确保 Kafka的监控和告警不依赖
Kafka本身。
本节将从讨论非同步分区度量指标开始,介绍如何根据这些度量指标采取行动,然后讨论
其他的度量指标,以便对 broker的度量指标有一个全面的认识。这里不会列出 broker的所
有度量指标,但会列出那些在监控 broker和集群时必须用到的部分。在介绍客户端度量指
标之前,还会针对日志展开详细的讨论
102.1非同步分区
如果说 broker只有一个可监控的度量指标,那么它一定是指非同步分区的数量。该度量指
明了作为首领的 broker有多少个分区处于非同步状态。这个度量可以反映Kaka的很多内
部问题,从 broker的崩溃到资源的过度消耗。因为这个度量指标可以说明很多问题,所以
当它的值大于零时,就应该想办法采取相应的行动。本章稍后会介绍更多用于诊断这类问
题的度量指标。表10-1列出了非同步分区度量指标的详细信息。
162第10章表10-1:度量指标和对应的非同步分区
度量指标名称 Under- replicated partitions
JMX MBean kafka. server: type=ReplicaManager, name=Under ReplicatedPartitionis
值区间
非负整数
如果集群里多个 broker的非同步分区数量一直保持不变,那说明集群中的某个 broker已经
离线了。整个集群的非同步分区数量等于离线 broker的分区数量,而且离线 broker不会生
成任何度量指标。这个时候,需要检査这个 broker出了什么问题,并解决问题。通常有可
能是硬件问题,也有可能是操作系统问题或者Java问题,导致进程出现中断或挂起。
默认的副本选举
在诊断问题之前,尝试过运行默认的副本选举(参见第9章)吗? broker在
释放首领角色(发生崩溃或被关闭)之后不会自动恢复首领角色(除非启用
了首领自动再均衡,不过不建议启用这个功能)。也就是说,集群里的首领
副本很容易出现不均衡。运行默认的副本选举是很容易的,也很安全,所以
在出现这类问题时,建议先重新选举首领,看看能否解决问题。
如果非同步分区的数量是波动的,或者虽然数量稳定但并没有 broker离线,说明集群出
现了性能问题。这类问题繁复多样,难以诊断,不过可以通过一些步骤来缩小问题的范
围。第一步,先确认问题是与单个 broker有关还是与整个集群有关。不过有时候这个也难
有定论。如果非同步分区属于单个 broker,那么这个 broker就是问题的根源,表象是其他
broker无法从它那里复制消息。
如果多个 broker都出现了非同步分区,那么有可能是集群的问题,也有可能是单个 broker
的问题。这时候有可能是因为一个 broker无法从其他 broker那里复制数据。为了找出这个
broker,可以列出集群的所有非同步分区,并检查它们的共性。使用 kafka-topics. sh工具
(第9章已详细介绍过)可以获取非同步分区清单。
示例:列出集群的非同步分区。
kafka-topics sh --zookeeper zoo1 example. com: 2181/kafka-cluster --describe
- under-replicated
Topic: topicOne Partition: 5 Leader: 1 Replicas: 1, 2 Isr: 1
Topic: topicOne Partition: 6 Leader: 3 Replicas: 2, 3 Isr: 3
Topic: topicTwo Partition: 3 Leader: 4 Replicas: 2, 4 Isr: 4
Topic: topicTwo Partition: 7 Leader: 5 Replicas: 5,2 Isr: 5
Topic: topicSix Partition: 1 Leader: 3 Replicas: 2, 3 Isr:
Topic: topicSix Partition: 2 Leader: 1 Replicas: 1, 2 Isr: 1
Topic: topicSix Partition: 5 Leader: 6 Replicas: 2,6 Isr: 6
Topic: topicsix Partition: 7 Leader: 7 Replicas: 7, 2 Isr: 7
Topic: topicNine Partition: 1 Leader: 1 Replicas: 1, 2 Isr: 1
Topic: topicNine Partition: 3 Leader: 3 Replicas: 2, 3 Isr: 3
Topic: topicNine Partition: 4 Leader: 3 Replicas: 3, 2 Isr: 3
Topic: topicNine Partition: 7 Leader: 3
Replicas: 2, 3 Isr: 3
Topic: topicNine Partition: 0 Leader: 3 Replicas: 2, 3 Isr: 3
Topic: topicNine Partition: 5 Leader: 6 Replicas: 6, 2 Isr: 6
监控 Kafka163在这个示例中, broker2出现在所有的副本清单里,但没有出现在所有的同步副本(ISR)
清单里,所以要将注意力放在这个 broker上。如果没有发现这样的 broker,那么问题有可
能出在整个集群上。
1.集群级别的问题
集群问题一般分为以下两类。
不均衡的负载。
资源过度消耗。
分区或首领的不均衡问题虽然解决起来有点复杂,但问题的定位是很容易的。为了诊断这
个问题,需要用到 broker的以下度量指标
分区的数量。
首领分区的数量
主题流入字节速率。
主题流入消息速率。
检査指标。在一个均衡的集群里,这些度量指标的数值在整个集群范围内是均等的,如表
10-2所示。
表10-2:资源使用情况度量指标
Bokr分区首领。流入字节流出字节
10050
3.56MB/945MB/s
123
101
49
3.66MB/s9.25MB/s
100
50
3.23MB/s982MB/s
也就是说,所有的 broker几乎处理相同的流量。假设在运行了默认的副本选举之后,这
些度量指标出现了很大的偏差,那说明集群的流量出现了不均衡。要解决这个问题,需要
将负载较重的 broker分区移动到负载较轻的 broker上。这可以使用第9章介绍的kafa
reassign- partitions.sh工具来实现。
实现集群负载均衡的辅助工具
broker本身无法在整个集群里实现自动的分区重分配。也就是说, Kafka集
群的流量均衡是一个十分费劲的过程,需要手动检查一大串度量指标,然后
进行均衡的副本重分配。为了解决这个问题,有一些组织开发了自动化工具
来帮助完成这个任务。例如, LinkedIn发布了一个叫作 kafka-assigner的工
具,可以在 Github的开源代码仓库kaka- tools里找到。 Kafka提供的企业支
持服务也包含了这一功能
Kaka集群的另一个性能问题是容量瓶颈。有很多潜在的瓶颈会拖慢整个系统:CPU、磁
盘IoO和网络吞吐量是其中最为常见的。磁盘的使用并不在其列,因为当磁盘被填满时,
broker会在进行适当的操作之后直接崩溃。为了诊断容量问题,可以对如下一些操作系统
级别的度量指标进行监控。
164
第10章CPU使用。
网络输入吞吐量。
网络输出吞吐量。
磁盘平均等待时间。
磁盘使用百分比
上述任何一种资源出现过度消耗,都会表现为分区的不同步。要记住, broker的复制过
程使用的也是 Kafka客户端。如果集群的数据复制出现了问题,那么集群的用户在生产
消息或读取消息时也会出现问题。所以,有必要为这些度量指标定义一个基线,并设定
相应的阈值,以便在容量告急之前定位问题。随着集群流量的增长,对这些度量指标的
趋势走向进行检查也是很有必要的。其中, ALL Topics Bytes In Rate最适合用于显示
集群的使用情况。
2.主机级别的问题
如果性能问题不是出现在整个集群上,而是出现在一两个 broker里,那么就要检査 broker
所在的主机,看看是什么导致它与集群里的其他 broker不一样。主机级别的问题可以分为
以下几类。
硬件问题。
进程冲突
本地配置的不一致。
典型的服务器和典型的问题
当一个服务器及其操作系统承载了数千个组件时,会变得很复杂,任何一个
组件都可能出现问题,导致整体的崩溃或者部分的性能衰退。本书不可能覆
盖到所有的内容,关于这个话题的书已经有很多了,而且这种局面还会一直
持续下去。不过,本书可以讨论一些最为常见的问题,这一小节将着重讨论
运行 Linux操作系统的服务器。
硬件问题很容易被发现,因为服务器会直接停止工作。不过,引起性能衰退的硬件问题却
不那么明显。当出现这类问题时,系统仍旧保持运行,但会降低行为能力。比如内存出现
了坏点,系统检测到坏点,直接跳过这个片段(可用的内存因此减少了)。类似的问题也
会发生在CPU上。对于这类问题,可以使用硬件提供的工具来监控硬件的健康状况,比
如智能平台管理接口(IPM)。出现问题时,可以通过 dmesg查看输出到系统控制台的内
核缓冲区日志。
能够导致Kaka性能衰退的一个比较常见的硬件问题是磁盘故障。Kaka使用磁盘来存储
消息,生产者的性能与磁盘的写入速度有直接关系。这里出现的任何偏差都会表现为生产
者和复制消息者的性能问题,而后者会导致分区的不同步。因此,应该持续地监控磁盘,
并在出现问题时马上进行修复。
监控 Kafka|165一粒老鼠屎
个 broker的磁盘问题可能会影响到整个集群的性能。因为生产者客户端会
连接到所有的 broker上,如果操作得当,这些 broker的分区几乎能够均等
地分布在整个集群里。如果一个 broker性能出现衰退并拖慢了处理请求的速
度,就会导致生产者的回压,从而拖慢发给所有 broker的请求。
假设你正在通过IPMI或其他硬件管理接口来监控磁盘的状态,与此同时,在操作系统上
运行了 SMART( Self-monitoring, Analysis and Reporting Technology,自行监控、分析和报
告技术)工具来监控和测试磁盘。在故障即将发生时,它会发出告警。除此之外,还要注
意査看磁盘控制器,不管是否使用了RAID硬件都要注意査看。很多磁盘控制器都有板载
的缓存,这个缓存只在控制器和电池备份单元(BBU)正常工作时才会被使用。如果BBU
发生故障,缓存就会被禁用,磁盘的性能就会衰退。
在网络方面,局部的故障也会带来很大的问题。有些问题是硬件引起的,比如糟糕的光缆
或连接器。有些问题是配置不当造成的,比如更改了服务器或上游网络硬件的网络连接速
度和双工设置。网络配置问题还有可能出现在操作系统上,比如网络缓冲区太小,或者太
多的网络连接占用了大量的系统内存。在这方面,网络接口的错误数量是一个最为关键的
指标。如果这个数字一直在增长,说明网络连接出现了问题。
如果硬件没有问题,那么需要注意系统里的其他应用程序,它们也会消耗系统的资源,而
且有可能会给Kaka带来压力。它们有可能是没有被正常安装的软件,或者一个非正常
运行的进程,比如监控代理进程。对于这种情况,可以使用top工具来识别那些大量消耗
CPU或内存的进程。
如果经过上述的检査还是找不出主机的问题根源,那么有可能是 broker或者系统配置不
致造成的。一个服务器上运行着多个应用程序,每个应用程序有多个配置选项,要找出
它们的差别真是一项艰巨的任务。这就是为什么要使用配置管理工具(如Chef或 Puppet)
来维护操作系统和应用程序(包括 Kafka)的配置一致性。
10.22 broker度量指标
除了非同步分区数量外,还有其他很多 broker级别的度量指标需要监控。虽然不一定会为
所有的度量指标设定告警阈值,但它们的确提供了关于 broker和集群的有价值的信息。它
们都应该出现在监控仪表盘上。
1.活跃控制器数量
该指标表示 broker是否就是当前的集群控制器,其值可以是0或1。如果是1,表示
broker就是当前的控制器。任何时候,都应该只有一个 broker是控制器,而且这个 broker
必须一直是集群控制器。如果出现了两个控制器,说明有一个本该退出的控制器线程被阻
塞了,这会导致管理任务无法正常执行,比如移动分区。为了解决这个问题,需要将这两
个 broker重启,而且不能通过正常的方式重启,因为此时它们无法被正常关闭。表10-3
给出了活跃控制器数量度量指标的详细信息。
166第10章表10-3:活跃控制器数量度量指标
度量指标名称 Active controller count
JMX MBean
kafka. controller: type=Kafka Controller, name=Active Controller Count
值区间
0或1
如果集群里没有控制器,集群就无法对状态的变更作出恰当的响应,状态的变更包括主题
或分区的创建和 broker故障。这时候要注意检查为什么控制器线程没有正常运行,比如,
Zookeeper集群的网络分区就会造成这样的问题。解决底层的问题之后,重启集群里的所
有 broker,重置控制器线程的状态。
2.请求处理器空闲率
Kafka使用了两个线程池来处理客户端的请求:网络处理器线程池和请求处理器线程池
网络处理器线程池负责通过网络读入和写出数据。这里没有太多的工作要做,也就是说,
不用太过担心这些线程会出现问题。请求处理器线程池负责处理来自客户端的请求,包括
从磁盘读取消息和往磁盘写入消息。因此, broker负载的增长对这个线程池有很大的影响。
表104给出了请求处理器空闲率度量指标的详细信息。
表10-4:请求处理器空闲率
度量指标名称 Request handler average idle percentage
JMX MBean
kafka. server: type=KafkaRequestHandler Pool, name=RequestHandler AvgIdlePercen
值区间
从0到1的浮点数(包括1在内)
智能地使用线程
这样看来,好像需要数百个请求处理器线程,但实际上,请求处理器线程数
M没必要超过CPU的核数、Ka在使用请求处理器时是非常智能的,它会分
流需要很长时间来处理的请求。例如,当请求的配额被限定或每个生产请求
需要多个确认时,Kaka就会使用这个功能。
请求处理器平均空闲百分比这个度量指标表示请求处理器空闲时间的百分比。数值越低,
说明 broker的负载越高。经验表明,如果空闲百分比低于20%,说明存在潜在的问题,如
果低于10%,说明出现了性能问题。除了集群的规模太小之外,还有其他两个原因会增大
这个线程池的使用量。首先,线程池里没有足够的线程。一般来说,请求处理器线程的数
量应该与系统的处理器核数一样(包括多线程处理器)。
另一个常见的原因是线程做了不该做的事。在 Kafka0.10之前,请求处理器线程负责解压
传入的消息批次、验证消息、分配偏移量,并在写入磁盘之前重新压缩消息。糟糕的是
压缩方法使用了同步锁。 Kafka0.10版本引入了一种新的格式,偏移量可以直接附加在消
息批次里。也就是说,生产者在发送消息批次之前可以设置相对的偏移量,这样 broker就
可以避免解压缩和重新压缩。如果使用了支持010版本消息格式的生产者和消费者客户
端,并且把 broker的消息格式也升级到了0.10版本,可以发现性能有了显著的改进。这样
可以降低对请求处理器线程的消耗。
监控 Kafka|1673.主题流入字节
主题流入字节速率使用b来表示,在对 broker接收的生产者客户端消息流量进行度量时,
这个度量指标很有用。该指标可以用于确定何时该对集群进行扩展或开展其他与规模增长
相关的工作。它也可以用于评估一个 broker是否比集群里的其他 broker接收了更多的流
量,如果出现了这种情况,就需要对分区进行再均衡。表10-5给出了详细信息。
表10-5:主题流入字节度量指标
度量指标名称 Bytes in per second
JMX MBean
kafka. server: type=Broker TopicMetrics, name=BytesInPer Sec
值区间
速率为双精度浮点数,计数为整数
因为这是第一个速率度量指标,所以有必要对它的属性进行简短的描述。所有的速率指标
都提供了7个属性,使用哪些属性完全取决于实际的需求。它们可能是具体的数字,也有
可能是基于某些时间段的平均值。如果没能恰当地使用这些指标,就无法看到 broker真实
的状况。
前两个属性与度量无关,但有助于更好地理解相应的度量指标。
EventType
这是度量的单位,在这里是“字节”。
RateUnit
这是速率的时间段,在这里是“秒”。
这两个属性表明,速率是通过b/s来表示的,不管它的值是基于多长的时间段算出的平均
值。速率还有其他4个不同粒度的属性
OneMinuteRate
前1分钟的平均值。
iveMinuteRate
前5分钟的平均值。
FifteenMinuteRate
前15分钟的平均值。
MeanRate
从 broker启动到目前为止的平均值。
OneMinuteRate波动很快,它提供了“时间点”粒度的度量,适用于查看短期的流量走势。
№ eanRate一般不会有太大变化,它提供的是整体的流量走势。虽然有一定的用途,但一般
不需要对它设置告警。F讠 veMinuterate和 FifteenMinuteRate提供了中间粒度的度量。
除了速率属性外,速率指标还有一个 Count属性,会在 broker启动之后保持增长。对于这
个指标来说, Count代表了从 broker启动以来接收到流量的字节总数。将该属性用在一个
支持计数度量指标的监控系统里,就可以提供度量的完整视图,而不仅仅是平均速率。
168第10章4.主题流出字节
主题流出字节速率与流入字节速率类似,是另一个与规模增长有关的度量指标。流出字节速
率显示的是消费者从 broker读取消息的速率。流出速率与流入速率的伸缩方式是不一样的,
这要归功于 Kafka对多消费者客户端的支持。很多 Kafka的流出速率可以达到流入速率的6
倍!所以,单独对流出速率进行观察和走势分析是非常重要的。表106给出了详细信息。
表10-6:主题流出字节度量指标
度量指标名称 Bytes out per second
JMX MBean kafka. server: type=Broker TopicMetrics, name=BytesoutPersec
值区间
速率为双精度浮点数,计数为整数
把复制消费者包括在内
流出速率也包括副本流量,也就是说,如果所有主题都设置了复制系数2
A\那么在没有消费者客户端的情况下,流出速率与流入速率是一样的。如果有
个消费者客户端从集群读取所有的消息,那么流出速率会是流入速率的2
倍。如果不知道这一点,光是看着这些指标就会感到很疑惑。
5.主题流入的消息
之前介绍的字节速率以字节的方式来表示 broker的流量,而消息速率则以每秒生成消息个
数的方式来表示流量,而且不考虑消息的大小。这也是一个很有用的生产者流量增长规模
度量指标。它也可以与字节速率一起用于计算消息的平均大小。与字节速率一样,该指标
也能反映集群的不均衡情况。表10-7给出了详细信息。
表10-7:主题流入消息度量指标
度量指标名称 Message in per second
JMX MBean
kafka. server: type=Broker TopicMetrics, name=MessagesInPerSec
值区间
速率为双精度浮点数,计数为整数
为什么没有消息的流出速率
经常会有人问,为什么没有 broker的“流出消息”度量指标?因为在消息被
A读取时,bker将整个消息批次发送给消费者,并没有展开批次,也就不会
去计算每个批次包含了多少个消息,所以, broker也就不知道发送了多少个
消息。 broker为此提供了一个度量指标叫作每秒获取次数,它指的是请求速
率,而不是消息个数。
6.分区数量
broker的分区数量一般不会经常发生改变,它是指分配给 broker的分区总数。它包括
broker的每一个分区副本,不管是首领还是跟随者。如果一个集群启用了自动创建主题的
功能,那么监控这个度量指标会变得很有意思,因为你会发现,这样可以让主题的创建游
离于控制之外。表10-8给出了详细信息。
监控 Kafka|169表10-8:分区数量度量指标
度量指标名称 Partition count世
JMX MBean
a server: type=ReplicaManager, name=PartitionCount
值区间
非负整数
7.首领数量
该度量指标表示 broker拥有的首领分区数量。与 broker的其他度量一样,该度量指标也应
该在整个集群的 broker上保持均等。我们需要对该指标进行周期性地检查,并适时地发出
告警,即使在副本的数量和大小看起来都很完美的时候,它仍然能够显示出集群的不均衡
问题。因为 broker有可能出于各种原因释放掉一个分区的首领身份,比如 Zookeeper会话
过期,而在会话恢复之后,这个分区并不会自动拿回首领身份(除非启用了自动首领再均
衡功能)。在这些情况下,该度量指标会显示较少的首领分区数,或者直接显示为零。这
个时候需要运行一个默认的副本选举,重新均衡集群的首领。表109给出了详细信息。
表10-9:首领数量度量指标
度量指标名称 Leader count
JMX MBean
kafka. server: type=ReplicaManager, name=LeaderCount
值区间
非负整数
可以使用该指标与分区数量一起计算出 broker首领分区的百分比。一个均衡的集群,如果
它的复制系数为2,那么所有的 broker都应该差不多是它们的50%分区的首领。如果复制
系数是3,这个百分比应该降到33%。
8.离线分区
与非同步分区数量一样,离线分区数量也是一个关键的度量指标(表10-10)。该度量只能
由集群控制器提供(对于其他 broker来说,该指标的值为零),它显示了集群里没有首领
的分区数量。发生这种情况主要有两个原因。
包含分区副本的所有 broker都关闭了。
由于消息数量不匹配,没有同步副本能够拿到首领身份(并且禁用了不完全首领选举)。
表10-10:离线分区数量度量指标
度量指标名称 Offline partitions count
JMX MBear
kafka. controller: type=KafkaController, name=offlinePartiionsCount
值区间
非负整数
在一个生产环境Kaka集群里,离线分区会影响生产者客户端,导致消息丢失,或者造成
回压。这属于“站点宕机”问题,需要立即解决。
9.请求度量指标
第5章描述了Kaka协议,它有多种不同的请求,每种请求都有相应的度量指标。以下的
请求类型都有相应的度量指标。
ApiVersions
Controlledshutdown
170
第10章CreateTopics
De lete Topics
DescribeGroups
Fetch
FetchConsumer
FetchFollower
Group Coordinator
Heartbeat
JoinGroup
LeaderAndIsr
Leave Group
Listgroups
Metadata
offsetCommit
OffsetFetch
offsets
Produce
SastHandshake
StopReplica
SyncGroup
UpdateMetadata
毎一种请求类型都有8个度量指标,它们分别体现了不同请求处理阶段的细节。例如,
Fetch请求有如表10-11所示的度量指标。
表10-11:请求度量指标
名字
Ete JMX MBean
Total Time
kafka. network: type=RequestMetrics, name=TotalTimeMs, request=Fetch
Request Queue Time kafka. network: type=RequestMetrics, name=RequestQueueTimeMs, request=Fetch
Local time
kafka. network: type=RequestMetrics, name=LocalTimeMs, request=Fetch
Remote Time
kafka. network: type=RequestMetrics, name=RemoteTimeMs, request=Fetch
Throttle Time
kafka. network: type=RequestMetrics, name=ThrottleTimeMs, request=Fetch
Response Queue Time kafka. network: type=RequestMetrics, name=Response QueueTimeMs, request=Fetch
Response Send Time kafka. network: type=RequestMetrics, name=Response SendTimeMs, request=Fetch
Requests Per Second kafka. network: type=RequestMetrics, name=RequestsPer Sec, request=Fetch
Request Per Second是一个速率指标,前面已经介绍过它的属性,这些属性显示了在单位时
间内收到并处理的请求个数。该度量提供了毎种请求类型的频度,尽管很多请求类型都不
是很频繁发生,比如 StopReplica和ψ pdateMetadata。
表10-11中的7个time指标分别为请求提供了一组百分比数值以及一个离散的 Count属
性,类似于速率度量指标。这些指标都是自 broker启动以来开始计算的,所以在查看那些
长时间没有变化的度量指标时,请记住: broker代理运行的时间越长,数据就越稳定。它
监控 Kafka|171们所代表的请求处理的部分如下。
Total Time
表示 broker花在处理请求上的时间,从收到请求开始计算,直到将响应返回给请求者。
Request Queue Time
表示请求停留在队列里的时间,从收到请求开始计算,直到开始处理请求。
Local Time
表示首领分区花在处理请求上的时间,包括把消息写入磁盘(但不一定要冲刷)。
Remote Time
表示在请求处理完毕之前,用于等待跟随者的时间。
Throttle Time
表示暂时搁置响应的时间,以便拖慢请求者,把它们限定在客户端的配额范围内。
Response queue Time
表示响应被发送给请求者之前停留在队列里的时间
Response Send Time
表示实际用于发送响应的时间。
每个度量指标的属性如下。
百分位
5 thPercentile、75 thPercentile、95 thPercentile、98 thPercentile、99 thPercentile、
999 thPercentile。
Count
从 broker启动至今处理的请求数量
Min
所有请求的最小值
Max
所有请求的最大值。
Mean
所有请求的平均值。
StdDev
整体的请求时间标准偏差。
什么是百分位
百分位是一种常见的时间度量方式,尽管它们容易被人误解。一个99百分位
A\度量表示,整组取样(这里指请求时间)里有9%的值小于度量指标的值,
也就是说,有1%的值大于指标的值。一般情况下需要查看平均值为99%或
99%的数值,这样就可以分辨出哪些是平均的请求,哪些是异样的请求。
172第10章在这些指标和属性中,哪些对于监控来说是比较重要的?我们至少要收集 Total Time和
Requests Per Second的平均值及较高的百分位(9%或99%),这样就可以获知发送请求
的整体性能。如果有可能,尽量为毎一种请求类型收集其他6种时间度量指标,这样就可
以将性能问题细分到请求的各个阶段。
为时间度量指标设定告警阈值有一定的难度。例如, fetch请求时间受各种因素的影响,
包括客户端等待消息的时间、主题的繁忙程度,以及客户端和 broker之间的网络连接速
度。不过,对于 Produce请求来说,可以为 Total Time设定99.9%百分位度量基线值。
与非同步分区类似, Produce请求的999%百分位数值快速增长说明出现了大规模的性
能问题。
102.3主题和分区的度量指标
broker的度量指标描述了 broker的一般行为,除此之外,还有很多主题实例和分区实例的
度量指标。在大型的集群里,这样的度量指标会有很多,一般情况下,不太可能将它们
全部收集到一个度量指标系统里。不过,我们可以用它们来调试与客户端相关的问题。例
如,主题的度量指标可以用于识别出造成集群流量大量增长的主题。我们需要提供这些度
量指标,这样Kaka的生产者和消费者就可以使用它们。不管是否会收集这些度量指标,
都有必要知道它们的用处。
表10-12中所列的度量指标将使用主题名称“ TOPICNAME”,分区⑩为0。在实际中,
要把主题名称和分区I替换成自己的名称和ID。
1.主题实例的度量指标
主题实例的度量指标与之前描述的 broker度量指标非常相似。事实上,它们之间唯一的区
别在于这里指定了主题名称,也就是说,这些度量指标属于某个指定的主题。主题实例的
度量指标数量取决于集群主题的数量,而且用户极有可能不会监控这些度量指标或设置告
警。它们一般提供给客户端使用,客户端依此评估它们对 Kafka的使用情况,并进行问题
调试。
表10-12:主题实例的度量指标
名字
JMX MBean
Bytes in rate
kafka. server: type=BrokerTopicMetrics, name=BytesInPer Sec, topic=TOPICNAME
Bytes out rate
kafka. server: type=Broker TopicMetrics, name=BytesoutPer Sec, topic=TOPICNAME
Failed fetch rate kafka. server: type=Broker TopicMetrics, name=Failed FetchRequestsPerSec, topic=TOPICNAME
Failed produce rate kafka. server: type=Broker TopicMetrics, name=FailedProduceRequests Sec, topic=TOPICNAME
Messages in rate
kafka. server: type=Broker TopicMetrics, name=MessagesInPer Sec, topic=TOPICNAME
Fetch request rate kafka. server: type=BrokerTopicMetrics, name=TotalFetchRequestsPer Sec, topic=TOPICNAME
Produce request rate kafka. server: type-BrokerTopicMetrics, name=TotaLProduceRequestsPer Sec, topic=TOPICNAME
2.分区实例的度量指标
分区实例的度量指标不如主题实例的度量指标那样有用。另外,它们的数量会更加庞大,
因为几百个主题就可能包含数千个分区。不过不管怎样,在某些情况下,它们还是有
监控
173定用处的。 Partition size度量指标表示分区当前在磁盘上保留的数据量(见表10-13)。如
果把它们组合在一起,就可以表示单个主题保留的数据量,作为客户端配额的依据。同一
个主题的两个不同分区之间的数据量如果存在差异,说明消息并没有按照生产消息的键
进行均匀分布。 Log segment count指标表示保存在磁盘上的日志片段的文件数量,可以与
Partition size指标结合起来,用于跟踪资源的使用情况。
表10-13:分区实例的度量指标
名称
JMX MBean
Partition size
kafka. log: type=Log, name=Size, topic=TOPICNAME, partition=o
Log segment count kafka. log: type=Log, name=NumLogSegments, topic=TOPICNAME, partition=o
Log end offset kafka. Log: type=Log, name=LogEndoffset, topic=TOPICNAME, partition=0
Log start offset kafka. log: type=Log, name=LogStartoffset, topic=TOPICNAME, partition=0
Log end offset和 Log start offset这两个度量指标分别表示消息的最大偏移量和最小偏移量。
不过需要注意的是,不能用这两个指标来推算分区的消息数量,因为日志压缩会导致偏移
量“丢失”,比如包含相同键的新消息会覆盖掉旧消息。不过它们在某些情况下还是很有
用的,比如在进行时间戳和偏移量映射时,就可以得到更精确的映射,消费者客户端因此
可以很容易地回滚到与某个时间点相对应的偏移量(尽管可能没有Kaka0.10.1里引入的
基于时间的索引搜索那么重要)。
非同步分区度量指标
在分区实例的度量指标中,有一个指标用于表示分区是否处于非同步状态。
A一般情况下,该指标对于日常的运维起不到太大作用,因为这类指标太多
了。可以直接监控 broker的非同步分区数量,然后使用命令行工具(参见第
9章)确定哪些分区处于非同步状态。
10.2.4Java虚拟机监控
除了 broker的度量指标外,还应该对服务器提供的一些标准度量进行监控,包括Java虚拟
机(JVM)。如果JVM频繁发生垃圾回收,就会影响 broker的性能,在这种情况下,就应
该得到告警。JVM的度量指标还能告诉我们为什么 broker下游的度量指标会发生变化。
1.垃圾回收
对于JVM来说,最需要监控的是垃圾回收(GC)的状态。需要监控哪些 MBean取决于具
体的Java运行时(JRE)和垃圾回收器的设置。如果JRE使用了 Oracle java1.8,并使用
了G1垃圾回收器,那么需要监控的 MBean如表10-14所示。
表10-14:G1垃圾回收器度量指标
名称
JMX MBean
Full GC cycles
java. Lang: type=Garbage Collector, name=G1 old Generation
Yong GC cycles java. Lang: type=Garbage Collector, name=G1 Young Generation
174第10章在垃圾回收语义里,Old和Full的意思是一样的。我们需要监控这两个指标的 Collection
Count和 ColLection time属性。 CollectionCount表示从JVM启动开始算起的垃圾回收次
数, Collectiontime表示从JVM启动开始算起的垃圾回收时间,以ms为单位。因为这些
属性是计数器,所以它们可以告诉我们发生GC的次数和花费在GC上的时间,还可以用
于计算平均每次GC花费的时间,不过在通常的运维中,这样做并没有多大用处。
这些指标还有一个 LastgcInfo属性。这是一个由5个字段组成的组合值,用于提供最后
次GC的信息。其中最重要的一个值是 duration值,它以ms为单位,展示最后一次GC
花费的时间。其他几个值( GcThread Count、id、 startTime和 endtime)虽然也会提供一些
信息,但用处不大。不过要注意的是,我们无法通过该属性查看到每一次GC的信息,因
为年轻代GC发生得很频繁
2.Java操作系统监控
JVM通过java.lang:type= Operating System提供了一些操作系统的信息,不过这些指标
的信息量很有限,并不能告知操作系统的完整状况。这些指标有两个比较有用但在操
作系统里难以收集到的属性— MaxFileDescriptor Count和0 pen FileDescriptor Count。
№ axFileDescriptor Count展示JVM能够打开的文件描述符(FD)数量的最大值,open
FileDescriptor Count展示目前已经打开的文件描述符数量。每个日志片段和网络连接都会
打开一个文件描述符,所以它们的数量增长得很快。如果网络连接不能被正常关闭,那么
broker很快就会把文件描述符用完。
10.2.5操作系统监控
JM并不能告诉用户所有与操作系统有关的信息,因此,用户不仅要收集 broker的度量指
标,也需要收集操作系统的度量指标。大多数监控系统都会提供代理,用于收集更多有关
操作系统的信息。用户需要监控CPU的使用、内存的使用、磁盘的使用、磁盘IO和网络
的使用情况。
在CPU方面,需要监控平均系统负载。系统负载是一个独立的数值,它展示处理器的相
对使用情况。另外,根据类型来捕捉CPU的使用百分比也是很有用的。根据收集方法和
操作系统的不同,可以使用如下列出的CPU百分比数值(使用了缩写),既可以使用其中
的一部分,也可以使用全部。
用户空间使用的时间。
内核空间使用的时间。
低优先级进程使用的时间。
id
空闲时间
磁盘等待时间
监控 Kafka|175处理硬件中断的时间
SL
处理软件中断的时间
st
等待管理程序的时间。
什么是系统负载
尽管很多人都知道系统负载是对CPU使用情况的度量,但他们并不知道度
量是如何进行的。平均负载是指等待处理器执行的线程数。在 Linux系统
里,它还包括处于不可中断睡眠状态的线程,比如磁盘等待。负载使用3个
数值来表示,分别是前1分钟的平均值,前5分钟的平均值和前15分钟的
平均值。在单CPU系统里,数值1表示系统负载达到了100%,此时总会存
在一个等待执行的线程。而在多CPU系统里,如果负载达到100%,那么负
载的数值与CPU的核数相等。例如,如果系统里有24个处理器,那么负载
100%表示负载数值为24。
对于 broker来说,跟踪CPU的使用情况是很有必要的,因为它们在处理请求时使用了大
量的CPU时间。而内存使用情况的跟踪就显得没有那么重要了,因为运行Kaka并不需
要太大的内存。它会使用堆外的一小部分内存来实现压缩功能,其余大部分内存则用于缓
存。不过,我们还是要对内存使用情况进行跟踪,确保其他的应用不会影响到 broker。可
以通过监控总内存空间和可用交换内存空间来确保内存交换空间不会被占用。
对于 Kafka来说,磁盘是最重要的子系统。所有的消息都保存在磁盘上,所以 Kafka的性
能严重依赖磁盘的性能。我们需要对磁盘空间和索引节点进行监控,确保磁盘空间不会被
用光。对于保存数据的分区来说就更是如此。对磁盘IO进行监控也是很有必要的,它们
揭示了磁盘的运行效率。我们需要监控磁盘的每秒种读写速度、读写平均队列大小、平均
等待时间和磁盘的使用百分比
最后,还需要监控 broker的网络使用情况。简单地说,就是指流入和流出的网络流量,
般使用bs来表示。要记住,在没有消费者时,1个流入比特对应1个或多个流出比特,
这个数字与主题的复制系数相等。根据消费者的实际数量,流λ流量很容易比输出流量高
出一个数量级。在设置告警阈值时要切记这一点。
10.2.6日志
在讨论监控时,如果不涉及日志,那么这个监控就是不完整的。与其他应用程序一样,
Kafka的 broker也可以被配置成定期向磁盘写入日志。为了能够从日志里获得有用的信
息,选择合适的日志和日志级别是很重要的。通过记录INFO级别的日志,可以捕捉到
很多有关 broker运行状态的重要信息。为了获得一系列清晰的日志文件,很有必要对日
志进行分类。
176第10章可以考虑使用两种日志。第一种是 kafka. controller,可以将它设置为INFO级别。这个日志
用于记录集群控制器的信息。在任何时候,集群里都只有一个控制器,因此只有一个 broker
会使用这个日志。日志里包含了主题的创建和修改操作、 broker状态的变更,以及集群的活
动,比如默认的副本选举和分区的移动。另一个日志是 kafka. server, ClientQuotaManager,
也可以将它设置为INF0级别。这个日志用于记录与生产和消费配额活动相关的信息。因为
这些信息很有用,所以最好不要把它们记录在 broker的主日志文件里。
在调试问题时,还有一些日志也很有用,比如 kafka. request. logger,可以将它设置为
DEBUG或 TRACE级别。这个日志包含了发送给 broker的每一个请求的详细信息。如果日志
级别被设置为 DEBUG,那么它将包含连接端点、请求时间和概要信息。如果日志级别被设
置为 TRACE,那么它将包含主题和分区的信息,以及除消息体之外的所有与请求相关的信
息。不管被设置成什么级别,这个日志会产生大量的数据,所以如果不是出于调试的目
的,不建议启用这个日志。
日志压缩线程的运行状态也是一个有用的信息。不过,这些线程并没有单独的度量指标,
个分区的压缩失败有可能造成压缩线程的整体崩溃,而且是悄然发生的。启用 kafka
log. LogCleaner、 kafka.Log. CLeaner和 kafka.log. LogCleaner Manager这些日志,并把它们
设置为 DEBUG级别,就可以输出日志压缩线程的运行状态。这些日志包含了每个被压缩分
区的大小和消息个数。一般情况下,这些日志的数量不会很大。也就是说,默认启用这些
日志并不会带来什么麻烦。
10.3客户端监控
所有的应用程序都需要监控,包括那些使用了Kaka客户端的应用程序。不管是生产者还
是消费者,它们都有特定的度量指标需要监控。本节将主要介绍如何监控官方的Java客户
端,其他第三方的客户端应该也有自己的度量指标。
10.3.1生产者度量指标
新版本 Kafka生产者客户端的度量指标经过调整变得更加简洁,只用了少量的 MBean。相
反,之前版本的客户端(不再受支持的版本)使用了大量的 MBean,而且度量指标包含了
大量的细节(提供了大量的百分位和各种移动平均数)。这些度量指标提供了很大的覆盖
面,但这样会让跟踪异常情况变得更加困难。
生产者度量指标的 MBean名字里都包含了生产者的客户端I。在下面的示例里,客户端
ID使用 CLIENTID表示, broker Id使用BR0 KERIO表示,主题的名字使用 TOPICNAME表示,
如表10-15所示。
表10-15: Kafka生产者度量指标 MBean
名称
JMX MBean
Overall producer kafka. producer: type=producer-metrics, client-id=CLIENTID
Per-broker
kafka. producer: type=producer-node-metrics, client-id=CLIENTID, node-id=node-BROKERID
Per-topic
kafka. producer: type=producer-topic-metrics, client-id=CLIENTID, topic=TOPICNAME
监控 Kafka177上表中列出的每一个 MBean都有多个属性用于描述生产者的状态。下面列出了用处最大
的几个属性。在继续了解这些属性之前,建议先通过阅读第3章了解生产者的工作原理。
1.生产者整体度量指标
生产者整体度量指标提供的属性描述了生产者各个方面的信息,从消息批次的大小到内
存缓冲区的使用情况。虽然这些度量在调试的时候很有用,不过在实际应用中只会用到
少数的几个,而在这几个度量里,也只有一部分需要进行监控和告警。下面将讨论一些
平均数度量指标(以-avg结尾),它们都有相应的最大值(以-nax结尾),不过这些最
大值的用处很有限。
record- error-rate是一个完全有必要对其设置告警的属性。这个指标的值一般情况下都
是零,如果它的值比零大,说明生产者正在丢弃无法发送的消息。生产者配置了重试次
数和回退策略,如果重试次数达到了上限,消息(记录)就会被丢弃。我们可以跟踪另
小一个属性 record- retry-rate,不过该属性不如 record- error-rate那么重要,因为重
试是很正常的行为。
我们可以对 request- Latency-avg设置告警,它表示发送一个生产者请求到 broker所需要
的平均时间。在运维过程中,应该为该指标建立一个基线,并设定告警阈值。请求延迟的
增加说明生产者请求正在变慢。有可能是网络出现了问题,也有可能是 broker出现了问
题。不管是哪一种情况,都会导致生产者端发生回压,并引发应用程序的其他问题。
除了这些关键性的度量指标外,如果能够知道生产者发送的消息流量就更好了。有3个属
性提供了3个不同的视图: outgoing-byte-rate表示每秒钟消息的字节数, record-send
rate表示每秒钟消息的数量, request-rate表示每秒钟生产者发送给 broker的请求数。单
个请求可以包含一个或多个批次,单个批次可以包含一个或多个消息,每个消息由多个字
节组成。把这些指标显示在仪表盘上,可以跟踪到生产者是如何使用Kaka的
为什么不使用 Producer RequestMetrics
Producer RequestMetrics MBean提供了请求延迟的百分位和请求速率的移动
平均数,但为什么不建议使用呢?问题在于,这个度量指标是为每个生产者
线程提供的。如果应用程序出于性能方面的考虑,使用了多个生产者线程,
那么就很难对这些指标进行聚合。所以,使用生产者整体指标所提供的属性
是一种更为有效的方式。
可以借助一些度量指标来更好地理解字节、记录、请求和批次之间的关系,这些指标描述
了这些实体的大小。 request-size-avg表示生产者发送请求的平均字节数, batch-size
avg表示单个消息批次(根据定义,批次包含了属于同一个分区的多个消息)的平均字节
数, record-size-avg表示单个消息的平均字节数。对于单主题的生产者来说,它们提供了
有关生产消息的有用信息。而对于多主题的生产者来说,比如 MirrorMaker,它们提供的
信息就不是很有用。除了这3个指标外,还有另外一个指标 records-per-request-avg,它
表示在生产者的单个请求里所包含的消息平均个数
最后一个值得推荐的指标是 record- queue- time-avg,它表示消息在发送给Kaka之前在生
178第10章产者客户端等待的平均毫秒数。应用程序在使用生产者客户端发送消息(调用send方法)
之后,生产者会一直等待,直到以下任一情况发生。
生产者客户端有足够多的消息来填充批次(根据 max. partition bytes的配置)。
距离上一次发送批次已经有足够长的时间(根据 linger. ms的配置)。
这两种情况都会促使生产者客户端关闭当前批次,然后把它发送给 broker。对于繁忙的
主题来说,一般会发生第一种情况,而对于比较慢的主题来说,一般会发生第二种情况
record- queue·tine-avg用于度量生产消息所使用的时间,因此,可以通过调优上述两个参
数来满足应用程序的延迟需求。
2.Per- broker和 Per-topic度量指标
除了生产者整体的度量指标外,还有一些 MBean为每个 broker连接和每个主题提供了有
限的属性集合。在调试问题时,这些度量会很有用,但不建议对它们进行常规的监控。这
些 MBean属性的命名与整体度量指标的命名是一样的,所表示的含义也是一样的(只不
过它们是作用在特定的 broker或特定的主题上)。
在 broker实例的度量指标里, request- Latency-avg是比较有用的一个,因为它一般比较稳
定(在消息批次比较稳定的情况下),而且能够显示 broker的连接问题。其他的属性,比
如 outgoing-byte-rate和 request- latency-avg,它们会因为 broker所包含分区的不同而有
所不同。也就是说,这些度量会随着时间发生变化,完全取决于集群的状态。
主题实例的度量指标比 broker实例的度量指标要有趣得多,不过只有在使用多主题的生
产者时,它们才能派上用场。当然,也只有在生产者不会消费过多主题的情况下,才有必
要对这些指标进行常规的监控。例如, MirrorMaker有可能会向成百上千的主题生成消息。
我们无法逐个检査这些指标,也几乎不可能为它们设置合理的告警阈值。与 broker实例
的度量指标一样,主题实例的度量指标一般用于诊断问题。例如,可以通过 record-send
rate和 record- error-rate这两个属性将丢弃的消息隔离到特定的主题上。另外,byte
rate表示主题整体的每秒消息字节数
10.3.2消费者度量指标
与新版本的生产者客户端类似,新版本的消费者客户端将大量的度量指标属性塞进了少数
的几个 MBean里,如表10-6所示。消费者客户端也作出了权衡,去掉了延迟百分位和速
率移动平均数。因为消费者读取消息的逻辑比生产者发送消息的逻辑复杂,所以消费者的
度量指标也会更多。具体请参考表10-16。
表10-16:Kaka消费者度量指标 MBean
名称
JMX MBean
Overall consumer kafka. consumer: type=consumer-metrics, client-id=CLIENTID
Fetch manager
kafka. consumer: type=consumer-fetch-manager-metrics, client-id=CLIENTID
Per-topic
kafka. consumer: type=-consumer-fetch-manager-metrics, client-id=CLIENTID, topic=TOPICNAME
Per-broker
kafka. consumer: type=consumer-node-metrics, client-id=CLIENTID, node-id=node-BROKERID
Coordinator
kafka. consumer: type=consumer-coordinator-metrics, client-id=CLIENTID
监控 Kafka
1791. Fetch Manager度量指标
在消费者客户端,消费者整体度量指标 MBean的用处不是很大,因为有用的指标都聚集
在 Fetch Manager MBean里。消费者 MBean包含了与网络底层运行情况有关的指标,而
Fetch Manager MBean则包含了与字节、请求和消息速率有关的指标。与生产者客户端不
同的是,用户可以查看消费者所提供的度量指标,但对它们设置告警是没有意义的。
对于 Fetch Manager来说, fetch- Latency-avg可能是一个需要对其进行监控和设置告警的
指标。与生产者的 request- Latency-=avg类似,该指标表示从消费者向 broker发送请求所
需要的时间。请求的延迟通过消费者的 fetch min bytes和 fetch, max, wait.ns这两个参数
进行控制。一个缓慢的主题会出现不稳定的延迟,有时候 broker响应很快(有可用消息的
时候),有时候甚至无法在 fetch. max, wait.ns规定的时间内完成响应(没有可用消息的时
候)。如果主题有相对稳定和足够的消息流量,那么查看这个指标或许会更有意义。
为什么不用延时(Lag)
我们建议对所有消费者的延时情况进行监控,但为什么不建议对 Fetch
Manager的 records1ag-nax属性进行监控呢?该属性表示落后最多的分区
的延时情况(落后 broker的消息数量)。
这里有两方面的原因:一方面是因为它只显示了单个分区的延时,另一方面
是因为它依赖消费者的某些特定功能。如果没有其他的选择,那么可以考虑
将该属性作为度量延时的指标,并为其设置告警。不过,最佳实践应该是使
用外部的延时监控,稍后会介绍更多的内容。
要想知道消费者客户端处理了多少消息流量,可以使用 bytes- consumed-rate或 records
consumed -rate,或者同时使用它们两个。它们分别表示客户端每秒读取的消息字节数和每
秒读取的消息个数。有些人为它们设置了最小值告警阈值,当消费者工作负载不足时,他
们就会收到告警。不过需要注意的是,消费者和生产者之间是没有耦合的,因此不能从消
费者端去推测生产者端的行为模式。
Fetch Manager也提供了一些度量指标,有助于理解字节、消息和请求之间的关系。 fetch
rate表示消费者每秒发出请求的数量, fetch-sze-avg表示这些请求的平均字节数,
records-per-request-avg表示每个请求的平均消息个数。要注意的是,消费者并没有提供
与生产者相对应的 record- sLze-avg,所以无法知道消息的平均大小。如果这个很重要,那
么可以参考其他指标,或者在应用程序接收到消息之后计算出消息的平均大小
2. Per-broker和 Per-topic度量指标
与生产者客户端类似,消费者客户端也为每个 broker连接和每个主题提供了很多度量指
标,它们可以用于诊断问题,但不建议对它们进行常规的监控。与 Fetch Manager一样,
broker的 request- latency-avg属性用处也很有限,它取决于主题的消息流量。 incoming
byte-rate和 request-rate分别表示 broker每秒读取的消息字节数和每秒请求数。它们可
以用于诊断消费者与 broker之间的连接问题。
180第10章在读取多个主题时,消费者客户端所提供的主题实例的度量指标会很有用。如果只是读
取单个主题,那么这些指标就与 Fetch Manager的指标一样,没有必要去收集它们。但
如果客户端读取了大量的主题,比如 MirrorMaker,那么査看这些指标就会变得很困难。
如果打算收集这些指标,可以考虑收集最重要的3个: bytes-consumed-rate、 records
consumed-rate和 fetch-size-avg。 bytes- consumed-rate表示从某个主题上每秒读取的消
息字节数, records- consumed-rate表示每秒读取的消息个数, fetch- stze-avg表示每个请
求的消息平均字节数。
3. Coordinator度量指标
正如第4章所述,消费者客户端以群组的方式运行。群组里会发生一些需要协调的活动,
比如新成员的加入或者向 broker发送心跳来保持群组的成员关系。消费者协调器负责处理
这些协调工作,作为消费者客户端的组成部分,它也维护着自己的一组度量指标。与其他
度量指标一样, Coordinator度量指标也提供了很多数字,但是其中只有关键的一小部分需
要进行常规的监控。
消费者群组在进行同步时,可能会造成消费者的停顿。当群组里的消费者在协商哪些分区
应该由哪些消费者来读取时,就会发生这种情况。停顿的时间长短取决于分区的数量。协
调器提供了sync-time-avg属性,用于表示同步活动所使用的平均亳秒数。 sync-rate属性
也很有用,表示每秒钟群组发生的同步次数。对于一个稳定的消费者群组来说,这个数字
在多数时候都是零。
消费者需要通过提交偏移量来作为读取进度的检查点,消费者可以基于固定时间间隔自动
提交偏移量,也可以通过应用程序代码手动提交偏移量。提交偏移量也是一种生成消息的
请求(不过它们有自己的请求类型),提交的偏移量就是消息,并被生成到一个特定的主
题上。协调器提供了 commit-latency-av属性,表示提交偏移量所需要的平均时间。与生
产者里的请求延时一样,我们也需要监控该指标,为它设定一个预期的基线,并设置合理
的告警阈值。
Coordinator度量指标的最后一个属性是 assigned- partitions,表示分配给消费者客户端
(群组里的单个实例)的分区数量。该属性之所以有用,是因为可以通过在整个群组内比
较各个实例的值,从而知道群组的负载是否均衡。我们可以使用该属性来识别因协调器的
分区分配算法所导致的负载不均衡问题。
10.33配额
Kaka可以对客户端的请求进行限流,防止客户端拖垮整个集群。对于消费者和生产者客
户端来说,这都是可配的,可以使用每秒钟允许单个客户端访问单个 broker的流量字节数
来表示。它有一个 broker级别的默认值,客户端可以对其进行覆盖。当 broker发现客户端
的流量已经超出配额时,它就会暂缓向客户端返回响应,等待足够长的时间,直到客户端
流量降到配额以下。
broker并不会在响应消息里提供客户端被限流的错误码,也就是说,对于应用程序来说,
如果不监控这些指标,可能就不知道发生了限流。需要监控的指标如表10-17所示
监控 Kafka|181表10-17:需要监控的度量指标
客户端 MBean
消费者 kafka. consumer: type=consumer-fetch-manager- metrics, client-id=CLIENTID的属性 fetch- throttle- time-avg
生产者 kafka. producer:type= producer- metrics, client- id=clIenTID的属性 produce-throttle-time-avg
默认情况下, broker不会开启配额功能。不过不管有没有使用配额,监控这些指标总是没
有问题的。况且,它们有可能在未来某个时刻被启用,而且从一开始就监控它们要比在后
期添加更加容易。
10.4延时监控
对于消费者来说,最需要被监控的指标是消费者的延时。它表示分区最后一个消息和消费
者最后读取的消息之间相差的消息个数。在之前的小节里已经说过,在这里,使用外部监
控要比使用客户端自己的监控好得多。虽然消费者提供了延时指标,但该指标存在一个问
题,它只表示单个分区的延时,也就是具有最大延时的那个分区,所以它不能准确地表示
消费者的延时。另外,它需要消费者做一些额外的操作,因为该指标是由消费者对每个发
出的请求进行计算得出的。如果消费者发生崩溃或者离线,那么该指标要么不准确,要么
不可用。
监控消费者延时最好的办法是使用外部进程,它能够观察 broker的分区状态,跟踪最近消
息的偏移量,也能观察消费者的状态,跟踪消费者提交的最新偏移量。这种方式提供了
种持续更新的客观视图,而且不依赖消费者的状态。我们需要在每一个分区上进行这种检
查。对于大型消费者来说,比如 Mirror Maker,可能意味着要监控成千上万个分区。
第9章介绍过如何使用命令行工具获取消费者群组的信息,包括提交的偏移量和延时。如
果直接监控由工具提供的延时信息,会存在一些问题。首先,需要为每个分区定义合理的
延时。每小时接收100个消息的主题和每秒钟接收10万个消息的主题,它们的阈值是不
一样的。其次,必须能够将延时指标导入到监控系统,并为它们设置告警。如果有一个消
费者群组读取了1500个主题,这些主题的分区数量超过了10万个,那将是一项令人望而
生畏的任务。
可以使用 Burrow来完成这项工作。 Burrow是一个开源的应用程序,最初由 LinkedIn开
发。它收集集群消费者群组的信息,并为毎个群组计算出一个单独的状态,告诉我们群组
是否运行正常、是否落后、速度是否变慢或者是否已经停止工作,以此来完成对消费者状
态的监控。它不需要通过监控群组的进度来获得阈值,不过用户仍然可以从中获得消息的
延时数量。 LinkedIn工程博客上记录了有关Buow工作原理的讨论。 Burrow可以用于监
控集群所有的消费者,也可以用于监控多个集群,而且可以很容易被集成到现有的监控和
告警系统里。
如果没有其他选择,消费者客户端的 records-1ag-max指标提供了有关消费者状态的部分
视图。不过,仍然强烈建议使用像 Burrow这样的外部监控系统。
182第10章10.5端到端监控
我们推荐使用的另一种外部监控系统是端到端的监控系统,它为Kaka集群的健康状态提
供了一种客户端视图。消费者和生产者客户端有一些度量指标能够说明集群可能出现了
问题,但这里有猜想的成分,因为延时的增加有可能是由客户端、网络或Kaka本身引起
的。另外,用户原本的工作可能只是管理Kaka集群,但现在也需要监控客户端。现在只
需要回答以下两个简单的问题。
可以向 Kafka集群生成消息吗?
可以从Kaka集群读取消息吗?
理想情况下,用户可能会希望每一个主题都允许这些操作,但要为此向每一个主题注入人
为的流量是不合理的。所以,可以考虑是否每个 broker都允许这些操作,而这正是Kafa
Monitor要做的事情。该工具由 LinkedIn的Kaka团队开发并开源,它持续地向一个横跨
集群所有 broker的主题生成消息,并读取这些消息。它对每个 broker的生产请求和读取请
求的可用性进行度量,包括从生产到读取的整体延时。这种类型的监控对于验证 Kafka集
群的运行状态来说是非常有价值的,因为 Kafka broker本身无法告知客户端是否能够正常
使用集群。
10.6总结
监控是运行Kaka的一个重要组成部分,这也是为什么有那么多的团队在这上面花费了那
么多时间。很多组织使用Kaka处理千万亿字节级别的数据流。确保数据的持续性和不丟
失消息是一个关键性的业务需求。作为Kaka集群的运维人员,我们的目标是成为最了解
集群状态的人。同时,要协助用户监控他们的应用程序。
本章介绍了如何监控Java应用程序,特别是Kaka应用程序。首先介绍了 broker的一些度
量指标,接着介绍了Java和操作系统的监控以及日志,然后详细地介绍了Kaka客户端的
监控,包括配额的监控,最后讨论了如何使用外部监控系统进行消费者延时监控以及如何
进行端到端的集群可用性监控。本章虽然没有列出所有可用的度量指标,但已经涵盖了最
为关键的部分。
监控 Kafka|183第11章
流式处理
Kaka一般被认为是一个强大的消息总线,可以传递事件流,但没有处理和转换事件的
能力。 Kafka可靠的传递能力让它成为流式处理系统完美的数据来源。很多基于Kaka构
建的流式处理系统都将Kaka作为唯一可靠的数据来源,如 Apache Storm、 Apache Spark
Streaming、 Apache Flink、 Apache Samza等。
有时候,行业分析师声称这些流式处理系统与那些已经存在了二十多年的复杂事件处理
(CEP)系统没有什么两样。但是流式处理系统却很成功,而只有寥宴可数的系统在采用
CEP,他们为此感到很惊讶。笔者认为,CEP的主要问题在于缺少事件流的处理能力。
随着 Kafka越来越流行,最初作为简单的消息总线,后来成为一种数据集成系统。很多
公司的系统里都包含了大量有价值的数据流,它们井然有序地存在了很长时间,好像在
等待一个流式处理框架的出现。换句话说,在出现数据库之前,数据的处理是一项艰巨
的任务。类似地,因为缺少能够提供可靠存储和集成的流式平台,流式处理的发展也受
到了阻碍。
从0.100版本开始, Kafka不仅为每一个流行的流式处理框架提供了可靠的数据来源,还
提供了一个强大的流式处理类库,并将其作为客户端类库的一部分。这样,开发人员就可
以在应用程序里读取、处理和生成事件,而不需要再依赖外部的处理框架
本章将以解释什么是流式处理作为开头(因为这个术语经常被人误解),然后讨论流式处
理的一些基本概念和流式处理系统常用的设计模式,然后深入介绍 Kafka的流式处理类
库,包括它的目标和架构,接着将会给出一个示例,介绍如何使用 Kafka streams(以下简
称 Streams)来计算股价移动平均数,最后讨论流式处理的其他使用场景,并在结束部分
列出为Kaka选择流式处理框架(如果有的话)的参考标准。本章主要是简单地介绍流式
处理,并不会涵盖 Streams的每一个特性,也不会对每一个现有的流式处理框架进行比较,
因为光这些内容就可以单独写成一本甚至好几本书了。
18411.1什么是流式处理
人们对流式处理的理解非常混乱。因为有太多关于流式处理的定义,它们混淆了实现细
节、性能需求、数据模型和软件工程的各个方面。笔者亲眼目睹了发生在关系型数据库上
的类似窘境,关系模型的抽象定文总是夹杂了数据库引擎的实现细节和特定局限性
流式处理领域还处在发展阶段,有一些流行的实现方案,其处理方式可能很特别,或者有
特定的局限,但这并不能说明它们的实现细节就是流式处理固有的组成部分。
先来看看什么是数据流(也被称为“事件流”或“流数捃”)。首先,数据流是无边界数据
集的抽象表示。无边界意味着无限和持续增长。无边界数据集之所以是无限的,是因为随
着时间的推移,新的记录会不断加入进来。这个定义已经被包括Ggle和 Amazon在内的
大部分公司所采纳。
这个简单的模型(事件流)可以表示很多业务活动,比如信用卡交易、股票交易、包裹
递送、流经交换机的网络事件、制造商设备传感器发出的事件、发送出去的邮件、游戏
里物体的移动,等等。这个清单是无穷无尽的,因为几乎每一件事情都可以被看成事件
的序列。
除了没有边界外,事件流模型还有其他一些属性。
事件流是有序的
事件的发生总是有个先后顺序。以金融活动事件为例,先将钱存进账户后再花钱,这与
先花钱再还钱的次序是完全不一样的。后者会出现透支,而前者不会。这是事件流与数
据库表的不同点之一。数据库表里的记录是无序的,而SQL语法中的 order by并不是
关系模型的组成部分,它是为了报表查询而添加的。
不可变的数据记录
事件一旦发生,就不能被改变。一个金融交易被取消,并不是说它就消失了,相反,这
需要往事件流里添加一个额外的事件,表示前一个交易的取消操作。顾客的一次退货并
不意味着之前的销售记录被删除,相反,退货行为被当成一个额外的事件记录下来。这
是数据流与数据表之间的另一个不同点——可以删除和修改数据表里的记录,但这些操
作只不过是发生在数据库里的事务,这些事务可以被看成事件流。假设你对数据库的二
进制日志( bin log)、预写式日志(WAL)和重做日志( (redo log)的概念都很熟悉,那
么就会知道,如果往数据库表插入一条记录,然后将其删除,表里就不会再有这条记
录。但重做日志里包含了两个事务:插入事务和删除事务。
事件流是可重播的
这是事件流非常有价值的一个属性。用户可以很容易地找出那些不可重播的流(流经
套接字的TCP数据包就是不可重播的),但对于大多数业务来说,重播发生在几个月
前(甚至几年前)的原始事件流是一个很重要的需求。可能是为了尝试使用新的分析方
法纠正过去的错误,或是为了进行审计。这也就是为什么我们相信Kaka能够让现代业
务领域的流式处理大获成功——可以借助Kaka来捕捉和重播事件流。如果没有这项能
力,流式处理充其量只是数据科学实验室里的一个玩具而已。
流式处理185如果事件流的定义里没有提到事件所包含的数据和每秒钟的事件数量,那么它就变得毫无
意义。不同系统之间的数据是不一样的,事件可以很小(有时候只有几个字节),也可以
很大(包含很多消息头的XML消息),它们可以是完全非结构化的键值对,可以是半结构
化的JSON,也可以是结构化的Avo或 Protobuf。虽然数据流经常被视为“大数据”,并
且包含了每秒钟数百万的事件,不过这里所讨论的技术同样适用(通常是更加适用)于小
点的事件流,可能每秒钟甚至每分钟只有几个事件。
知道什么是事件流以后,是时候了解“流式处理”的真正含义了。流式处理是指实时地处
理一个或多个事件流。流式处理是一种编程范式,就像请求与响应范式和批处理范式那
样。下面将对这3种范式进行比较,以便更好地理解如何在软件架构中应用流式处理。
请求与响应
这是延迟最小的一种范式,响应时间处于亚毫秒到亳秒之间,而且响应时间一般非常稳
定。这种处理模式一般是阻塞的,应用程序向处理系统发出请求,然后等待响应。在数
据库领域,这种范式就是线上交易处理(OLTP)。销售点(POS)系统、信用卡处理系
统和基于时间的追踪系统一般都使用这种范式。
批处理
这种范式具有高延迟和高吞吐量的特点。处理系统按照设定的时间启动处理进程,比如
每天的下午两点开始启动,每小时启动一次等。它读取所有的输入数据(从上一次执行
之后的所有可用数据,或者从月初开始的所有数据等),输出结果,然后等待下一次启
动。处理时间从几分钟到几小时不等,并且用户从结果里读到的都是旧数据。在数据库
领域,它们就是数据仓库(DWH)或商业智能(BI)系统。它们每天加载巨大批次的
数据,并生成报表,用户在下一次加载数据之前看到的都是相同的报表。从规模上来
说,这种范式既高效又经济。但在近几年,为了能够更及时、高效地作出决策,业务要
求在更短的时间内能提供可用的数据,这就给那些为探索规模经济而开发却无法提供低
延迟报表的系统带来了巨大的压力
流式处理
这种范式介于上述两者之间。大部分的业务不要求亚毫秒级的响应,不过也接受不了要
等到第二天才知道结果。大部分业务流程都是持续进行的,只要业务报告保持更新,业
务产品线能够持续响应,那么业务流程就可以进行下去,而无需等待特定的响应,也不
要求在几毫秒内得到响应。一些业务流程具有持续性和非阻塞的特点,比如针对可疑信
用卡交易的警告、网络警告、根据供应关系实时调整价格、跟踪包裹。
流的定义不依赖任何一个特定的框架、API或特性。只要持续地从一个无边界的数据集读
取数据,然后对它们进行处理并生成结果,那就是在进行流式处理。重点是,整个处理过
程必须是持续的。一个在每天凌晨两点启动的流程,从流里读取500条记录,生成结果,
然后结束,这样的流程不是流式处理
11.2流式处理的一些概念
流式处理的很多方面与普通的数据处理是很相似的:写一些代码来接收数据,对数据进行
处理,可能做一些转换、聚合和增强的操作,然后把生成的结果输出到某个地方。不过流
186第11章式处理有一些特有的概念,对于那些有数据处理经验但是首次尝试开发流式处理应用程序
的人来说,很容易造成混淆。下面将试着澄清这些概念。
112.1时间
时间或许就是流式处理最为重要的概念,也是最让人感到困惑的。在讨论分布式系统时,
该如何理解复杂的时间概念?推荐阅读 Justin Sheehy的论文“ There is No now”。在流式
处理里,时间是一个非常重要的概念,因为大部分流式应用的操作都是基于时间窗口的。
例如,流式应用可能会计算股价的5分钟移动平均数。如果生产者因为网络问题离线了2
小时,然后带着2小时的数据重新连线,我们需要知道该如何处理这些数据。这些数据大
部分都已经超过了5分钟,而且没有参与之前的计算
流式处理系统一般包含如下几个时间概念。
事件时间
事件时间是指所追踪事件的发生时间和记录的创建时间。例如,度量的获取时间、商店
里商品的出售时间、网站用户访问网页的时间,等等。在 Kafka0.10.0和更高版本里
生产者会自动在记录中添加记录的创建时间。如果这个时间戳与应用程序对“事件时
间”的定义不一样,例如,Kaka的记录是基于事件发生后的数据库记录创建的,那就
需要自己设置这个时间戳字段。在处理数据流时,事件时间是很重要的。
日志追加时间
日志追加时间是指事件保存到 broker的时间。在 Kafka0.10.0和更高版本里,如果启用
了自动添加时间戳的功能,或者记录是使用旧版本的生产者客户端生成的,而且没有包
含时间戳,那么 broker会在接收这些记录时自动添加时间戳。这个时间戳一般与流式
处理没有太大关系,因为用户一般只对事件的发生时间感兴趣。例如,如果要计算每天
生产了多少台设备,就需要计算在那一天实际生产的设备数量,尽管这些事件有可能因
为网络问题到了第二天才进入Kaka。不过,如果真实的事件时间没有被记录下来,那
么就可以使用日志追加时间,在记录创建之后,这个时间就不会发生改变。
处理时间
处理时间是指应用程序在收到事件之后要对其进行处理的时间。这个时间可以是在事件
发生之后的几毫秒、几小时或几天。同一个事件可能会被分配不同的时间戳,这取决于
应用程序何时读取这个事件。如果应用程序使用了两个线程来读取同一个事件,这个时
间戳也会不一样!所以这个时间戳非常不可靠,应该避免使用它
注意时区问题
在处理与时间有关的问题时,需要注意时区问题。整个数据管道应该使用同
个时区,否则操作的结果就会出现混淆,变得毫无意义。如果时区问题不
可避免,那么在处理事件之前需要将它们转换到同一个时区,这就要求记录
里同时包含时区信息。
流式处理18711.22状态
如果只是单独处理每一个事件,那么流式处理就很简单。例如,如果想从Kaka读取在线
购物交易事件流,找出金额超过10000美元的交易,并将结果通过邮件发送给销售人员,
那么可以使用 Katka消费者客户端和SMTP库,几行代码就可以搞定。
如果操作里包含了多个事件,流式处理就会变得很有意思,比如根据类型计算事件的数
量、移动平均数、合并两个流以便生成更丰富的信息流。在这些情况下,光处理单个事件
是不够的,用户需要跟踪更多的信息,比如这个小时内看到的每种类型事件的个数、需要
合并的事件、将每种类型的事件值相加,等等。事件与事件之间的信息被称为“状态”。
这些状态一般被保存在应用程序的本地变量里。例如,使用散列表来保存移动计数器。事
实上,本书的很多例子就是这么做的。不过,这不是一种可靠的方法,因为如果应用程序
关闭,状态就会丢失,结果就会发生变化,而这并不是用户希望看到的。所以,要小心地
持久化最近的状态,如果应用程序重启,要将其恢复。
流式处理包含以下几种类型的状态。
本地状态或内部状态
这种状态只能被单个应用程序实例访问,它们一般使用内嵌在应用程序里的数据库进行
维护和管理。本地状态的优势在于它的速度,不足之处在于它受到内存大小的限制。所
以,流式处理的很多设计模式都将数据拆分到多个子流,这样就可以使用有限的本地状
态来处理它们。
外部状态
这种状态使用外部的数据存储来维护,一般使用 NOSQL系统,比如 Cassandra。使用外
部存储的优势在于,它没有大小的限制,而且可以被应用程序的多个实例访问,甚至被
不同的应用程序访问。不足之处在于,引入额外的系统会造成更大的延迟和复杂性。大
部分流式处理应用尽量避免使用外部存储,或者将信息缓存在本地,减少与外部存储发
生交互,以此来降低延迟,而这就引入了如何维护内部和外部状态一致性的问题。
11.23流和表的二元性
大家都熟悉数据库表,表就是记录的集合,每个表都有一个主键,并包含了一系列由
schema定义的属性。表的记录是可变的(可以在表上面执行更新和删除操作)。我们可以
通过查询表数据获知某一时刻的数据状态。例如,通过查询 CUSTOMERS CONTACTS这
个表,就可以获取所有客户的联系信息。如果表被设计成不包含历史信息,那么就找不到
客户过去的联系信息了。
在将表与流进行对比时,可以这么想:流包含了变更—流是一系列事件,每个事件就是
个变更。表包含了当前的状态,是多个变更所产生的结果。所以说,表和流是同一个硬
币的两面
世界总是在发生变化,用户有时候关注变更事件,有时候则关注世界的当前
状态。如果一个系统允许使用这两种方式来查看数据,那么它就比只支持一种方式的系统
强大
为了将表转化成流,需要捕捉到在表上所发生的变更,将“ insert”、“ update”和“ delete
188第11章事件保存到流里。大部分数据库提供了用于捕捉变更的“ Change Data Capture”(CDC)解
决方案,Kaka连接器将这些变更发送到 Kafka,用于后续的流式处理
为了将流转化成表,需要“应用”流里所包含的所有变更,这也叫作流的“物化”。首
先在内存里、内部状态存储或外部数据库里创建一个表,然后从头到尾遍历流里的所
有事件,逐个地改变状态。在完成这个过程之后,得到了一个表,它代表了某个时间
点的状态。
假设有一个鞋店,某零售活动可以使用一个事件流来表示
红色、蓝色和绿色鞋子到货”
“蓝色鞋子卖出”
“红色鞋子卖出”
“蓝色鞋子退货
“绿色鞋子卖出”
如果想知道现在仓库里还有哪些库存,或者到目前为止赚了多少钱,需要对视图进行物
化。图11告诉我们,目前还有蓝色和黄色鞋子,账户上有170美元。如果想知道鞋店的
繁忙程度,可以査看整个事件流,会发现总共发生了5个交易,还可以査出为什么蓝色鞋
子被退货。
仓库变更事件流
蓝色鞋子300
代表仓库最新状态
〈红色鞋子30
的表或物化视图
绿色鞋子300
红色鞋子29
地〕蓝色鞋子209
蓝色鞋子」30
红色鞋子299
绿色鞋子299
y蓝色鞋子30
)绿色鞋子29
图11-1:物化仓库变更事件流
1124时间窗口
大部分针对流的操作都是基于时间窗口的,比如移动平均数、一周内销量最好的产品、系
统的99百分位等。两个流的合并操作也是基于时间窗口的,我们会合并发生在相同时间
片段上的事件。不过,很少人会停下来仔细想想时间窗口的类型。例如,在计算移动平均
数时,需要知道以下几个问题。
流式处理189窗口的大小。是基于5分钟进行平均,还是15分钟,或者一天?窗口越小,就能越快
地发现变更,不过噪声也越多。窗口越大,变更就越平滑,不过延迟也越严重,如果价
格涨了,需要更长的时间才能看出来。
窗口移动的频率(“移动间隔”)。5分钟的平均数可以每分钟变化一次,或者每秒钟变
化一次,或者每当有新事件到达时发生变化。如果“移动间隔”与窗口大小相等,这种
情况被称为“滚动窗口( tumbling window)”。如果窗口随着毎一条记录移动,这种情况
被称为“滑动窗口( sliding window)”。
窗口的可更新时间多长。假设计算了000到00:05之间的移动平均数,一个小时之后
又得到了一些“事件时间”是002的事件,那么需要更新00.00到00.05这个窗口的
结果吗?或者就这么算了?理想情况下,可以定义一个时间段,在这个时间段内,事件
可以被添加到与它们相应的时间片段里。如果事件处于4个小时以内,那么就更新它们,
否则就忽略它们。
窗口可以与时间对齐,比如5分钟的窗口如果每分钟移动一次,那么第一个分片可以是
00:00~00:.05,第二个就是00:01~00:06。它也可以不与时间对齐,应用可以在任何时候启
动,那么第一个分片有可能是03:17~03:22滑动窗口永远不会与时间对齐,因为只要有新
记录到达,它们就会发生移动。图112展示了这两种时间窗口的不同之处。
滚动窗口—5分钟的时间窗口,每5分钟滚动一次
时间窗口1时间窗口2时间窗口3
10
时间
跳跃窗口—5分钟的时间窗口,每分钟跳跃一次
窗口重叠,事件属于多个时间窗口
时间窗口3
10
时间
图11-2:滚动窗囗和跳跃窗口的区别
11.3流式处理的设计模式
每一个流式处理系统都不一样,从基本的消费者、处理逻辑和生产者的组合,到使用了
Spark Streaming和机器学习软件包的复杂集群,以及其他很多处于中间位置的组件。不过
有一些基本的设计模式和解决方案可以满足流式处理架构的常见需求。下面将介绍一些这
样的模式,并举例说明如何使用这种模式。
190第11章11.3.1单个事件处理
处理单个事件是流式处理最基本的模式。这个模式也叫map或 filter模式,因为它经常被
用于过滤无用的事件或者用于转换事件(map这个术语是从 Map-Reduce模式中来的,map
阶段转换事件, reduce阶段聚合转换过的事件)。
在这种模式下,应用程序读取流中的事件,修改它们,然后把事件生成到另一个流上。比
如,一个应用程序从一个流中读取日志消息,并把 ERROR级别的消息写到高优先级的流
中,同时把其他消息写到低优先级的流中。再如,一个应用程序从流中读取事件,并把事
件从JSON格式改为Avro格式。这类应用程序不需要在程序内部维护状态,因为每一个
事件都是独立处理的。这也意味着,从错误中恢复或进行负载均衡会非常容易,因为不需
要进行恢复状态的操作,只需要将事件交给应用程序的另一个实例去处理。
这种模式可以使用一个生产者和一个消费者来实现,如图11-3所示。
主题日志事分人错误事件,先级
主题
其他事件
低优
转换成
Ay
VrO
先级
主题
Avro
日志
图11-3:单事件处理拓扑
11.32使用本地状态
大部分流式处理应用程序关心的是如何聚合信息,特别是基于时间窗口进行聚合。例如,
找出每天最低和最高的股票交易价格并计算移动平均数。
要实现这些聚合操作,需要维护流的状态。在本例中,为了计算毎天的最小价格和平均价
格,需要将最小值和最大值保存下来,并将它们与每一个新值进行对比。
这些操作可以通过本地状态(而不是共享状态)来实现,因为本例中的每一个操作都是基
于组的聚合操作,如图114所示。例如,基于各个股票代码进行聚合,而不是基于整个股
票市场。我们使用了一个Kaka分区器来确保具有相同股票代码的事件总是被写入相同的
分区。应用程序的每个实例从分配给它们的分区上获取事件(这是Kaka的消费者保证)。
也就是说,应用程序的每一个实例都可以维护一个股票代码子集的状态。
流式处理191处理器
本地
交易主题
聚合min,avg
交易聚合主题
A-n
A-N
分区
处理器
分区
交易
0-Z
0-Z
分区
分区
聚合min,avg
图114:使用本地状态的事件拓扑
如果流式处理应用程序包含了本地状态,情况就会变得非常复杂,而且还需要解决下列的
些问题
内存使用
应用实例必须有可用的内存来保存本地状态。
持久化
要确保在应用程序关闭时不会丢失状态,并且在应用程序重启后或者切换到另一个应用
实例时可以恢复状态。 Streams可以很好地处理这些问题,它使用内嵌的 RocksDB将本
地状态保存在内存里,同时持久化到磁盘上,以便在重启后可以恢复。本地状态的变更
也会被发送到 Kafka主题上。如果 Streams节点崩溃,本地状态并不会丢失,可以通过
重新读取Kaka主题上的事件来重建本地状态。例如,如果本地状态包含“IBM当前
最小价格是167.9”,并且已经保存到了 Kafka上,那么稍后就可以通过读取这些数据
来重建本地缓存。这些Kaka主题使用了压缩日志,以确保它们不会无限量地增长,方
便重建状态
再均衡
有时候,分区会被重新分配给不同的消费者。在这种情况下,失去分区的实例必须把最
后的状态保存起来,同时获得分区的实例必须知道如何恢复到正确的状态。
不同的流式处理框架为开发者提供了不同的本地状态支持。如果应用程序需要维护本地状
态,那么就要知道框架是否提供了支持。本章的末尾将会对一些框架进行简要的对比,不
过软件发展变化太快,而流式处理框架更是如此。
192第11章11.3.3多阶段处理和重分区
本地状态对按组聚合操作起到很大的作用。但如果需要使用所有可用的信息来获得一个结
果呢?例如,假设要发布每天的“前10支”股票,这10支股票需要从每天的交易股票中
挑选出来。很显然,如果只是在每个应用实例上进行处理是不够的,因为10支股票分布
在多个实例上,如图11-5所示。我们需要一个两阶段解决方案。首先,计算每支股票当天
的涨跌,这个可以在每个实例上进行。然后将结果写到一个包含了单个分区的新主题上。
另一个单独的应用实例读取这个分区,找出当天的前10支股票。新主题只包含了每支股
票的概要信息,比其他包含交易信息的主题要小很多,所以流量很小,使用单个应用实例
就足以应付。不过,有时候需要更多的步骤才能生成结果。
处理器
本地
状态
每日获利
交易主题
或损失
获利或损失主题
处理器
前10个主题
A-N
本地
分区
处理器
个
分区
分区
0-Z
本地
分区
Top 10
每日获利
或损失
图11-5:包含本地状态和重分区步骤的拓扑
这种多阶段处理对于写过 Map-Reduce代码的人来说应该很熟悉,因为他们经常要使用多
个 reduce步骤。如果写过 Map-Reduce代码,就应该知道,处理每个 reduce步骤的应用需
要被隔离开来。与 Map-Reduce不同的是,大多数流式处理框架可以将多个步骤放在同
个应用里,框架会负责调配每一步需要运行哪一个应用实例(或 worker)。
11.34使用外部查找—一流和表的连接
有时候,流式处理需要将外部数据和流集成在一起,比如使用保存在外部数据库里的规则
来验证事务,或者将用户信息填充到点击事件当中。
很明显,为了使用外部查找来实现数据填充,可以这样做:对于事件流里的每一个点击事
件,从用户信息表里查找相关的用户信息,从中抽取用户的年龄和性别信息,把它们包含
在点击事件里,然后将事件发布到另一个主题上,如图116所示。
流式处理193点击事
件主题
填充
瞋充的点击
事件主题
用户信息
数据库
图11-6:使用外部数据源的流式处理
这种方式最大的问题在于,外部查找会带来严重的延迟,一般在5~15ms之间。这在很多
情况下是不可行的。另外,外部数据存储也无法接受这种额外的负载——流式处理系统毎
秒钟可以处理10~50万个事件,而数据库正常情况下每秒钟只能处理1万个事件,所以需
要伸缩性更强的解决方案。
为了获得更好的性能和更强的伸缩性,需要将数据库的信息缓存到流式处理应用程序里
不过,要管理好这个缓存也是一个挑战。比如,如何保证缓存里的数据是最新的?如果刷
新太频繁,那么仍然会对数据库造成压力,缓存也就失去了作用。如果刷新不及时,那么
流式处理中所用的数据就会过时。
如果能够捕捉数据库的变更事件,并形成事件流,流式处理作业就可以监听事件流,并及
时更新缓存。捕捉数据库的变更事件并形成事件流,这个过程被称为CDC——变更数据捕
捉( Change Data Capture)。如果使用了 Connect,就会发现,有一些连接器可以用于执行
CDC任务,把数据库表转成变更事件流。这样就拥有了数据库表的私有副本,一旦数据库
发生变更,用户会收到通知,并根据变更事件更新私有副本里的数据,如图117所示。
处理器
用户信息
的本地
缓存
点击事
件主题
连接
填充的点击
事件主题
「用户信
息主题
变更捕获
用户信息
数据库
图11-7:连接流和表的拓扑,不需要外部数据源
这样一来,当收到点击事件时,可以从本地的缓存里查找 user id,并将其填充到点击事件
里。因为使用的是本地缓存,它具有更强的伸缩性,而且不会影响数据库和其他使用数据
库的应用程序。
194第11章之所以将这种方案叫作流和表的连接,是因为其中的一个流代表了本地缓存表的变更。
11.3.5流与流的连接
有时候需要连接两个真实的事件流。什么是“真实”的流?本章开始的时候曾经说过,流
是无边界的。如果使用一个流来表示一个表,那么就可以忽略流的大部分历史事件,因为
你只关心表的当前状态。不过,如果要连接两个流,那么就是在连接所有的历史事件
将两个流里具有相同键和发生在相同时间窗口内的事件匹配起来。这就是为什么流和流的
连接也叫作基于时间窗口的连接( windowed-join)。
假设有一个由网站用户输入的搜索事件流和一个由用户对搜索结果进行点击的事件流。对用
户的搜索和用户对搜索结果的点击进行匹配,就可以知道哪一个搜索的热度更高。很显然,
我们需要基于搜索关键词进行匹配,而且每个关键词只能与一定时间窗口内的事件进行匹
配——假设用户在输入搜索关键词后几秒钟就会点击搜索结果。因此,我们为每一个流维护
了以几秒钟为单位的时间窗口,并对这些时间窗口事件结果进行匹配,如图118所示。
点击
U:5019|U:230:12U30:43U23U:5|0:18
5秒钟的时间窗口
本地状态
连接\—→连接过的事件
5秒钟的时间窗口本地状态
搜索
U:170: 290: 250:33 0: 4u. 17 u. 55 0:43 U:9 0:48
图11-8:连接两个流,通常包含一个移动时间窗
在 Streams中,上述的两个流都是通过相同的键来进行分区的,这个键也是用于连接两个
流的键。这样一来, user id:42的点击事件就被保存在点击主题的分区5上,而所有user
d:42的搜索事件被保存在搜索主题的分区5上。 Streams可以确保这两个主题的分区5的
事件被分配给同一个任务,这个任务就会得到所有与 user id42相关的事件。 Streams在内
嵌的 ROckSDB里维护了两个主题的连接时间窗口,所以能够执行连接操作。
11.3.6乱序的事件
不管是对于流式处理还是传统的ETL系统来说,处理乱序事件都是一个挑战。物联网领域
经常发生乱序事件:一个移动设备断开WiFi连接几个小时,在重新连上WiFi之后将几个
小时累积的事件一起发送岀去,如图11-9所示。这在监控网络设备(故障交换机被修复之
前不会发送任何诊断数据)或进行生产(装置间的网络连接非常不可靠)时也时有发生。
流式处理19550|502|5046454551057
迟到的旧事件
图119:乱序事件
要让流处理应用程序处理好这些场景,需要做到以下几点。
识别乱序的事件。应用程序需要检查事件的时间,并将其与当前时间进行比较
规定一个时间段用于重排乱序的事件。比如3个小时以内的事件可以重排,但3周以外
的事件就可以直接扔掉。
具有在一定时间段内重排乱序事件的能力。这是流式处理应用与批处理作业的一个主要
不同点。假设有一个每天运行的作业,一些事件在作业结束之后才到达,那么可以重新
运行昨天的作业来更新事件。而在流式处理中,“重新运行昨天的作业”这种情况是不
存在的,乱序事件和新到达的事件必须一起处理。
具备更新结果的能力。如果处理的结果保存到数据库里,那么可以通过put或 update对
结果进行更新。如果流应用程序通过邮件发送结果,那么要对结果进行更新,就需要很
巧妙的手段。
有一些流式处理框架,比如 Google的 Dataflow和Kaka的 Streams,都支持独立于处理时
间发生的事件,并且能够处理比当前处理时间更晚或更早的事件。它们在本地状态里维护
了多个聚合时间窗口,用于更新事件,并为开发者提供配置时间窗口大小的能力。当然,
时间窗口越大,维护本地状态需要的内存也越大。
Streams APl通常将聚合结果写到主题上。这些主题一般是压缩日志主题,也就是说,它
们只保留每个键的最新值。如果一个聚合时间窗口的结果需要被更新为晚到事件的结果,
Streams会直接为这个聚合时间窗口写入一个新的结果,将前一个结果覆盖掉。
11.37重新处理
最后一个很重要的模式是重新处理事件,该模式有两个变种。
我们对流式处理应用进行了改进,使用新版本应用处理同一个事件流,生成新的结果,
并比较两种版本的结果,然后在某个时间点将客户端切换到新的结果流上。
现有的流式处理应用出现了缺陷,修复缺陷之后,重新处理事件流并重新计算结果。
对于第一种情况, Kafka将事件流长时间地保存在可伸缩的数据存储里。也就是说,要使
用两个版本的流式处理应用来生成结果,只需要满足如下条件:
将新版本的应用作为一个新的消费者群组;
·让它从输入主题的第一个偏移量开始读取数据(这样它就拥有了属于自己的输入
流事件副本);
检查结果流,在新版本的处理作业赶上进度时,将客户端应用程序切换到新的结果流上
第二种情况有一定的挑战性。它要求“重置”应用,让应用回到输入流的起始位置开始处
理,同时重置本地状态(这样就不会将两个版本应用的处理结果混淆起来了),而且还可
196第11章能需要清理之前的输出流。虽然 Streams提供了一个工具用于重置应用的状态,不过如果
有条件运行两个应用程序并生成两个结果流,还是建议使用第一种方案。第一种方案更加
安全,多个版本可以来回切换,可以比较不同版本的结果,而且不会造成数据的丢失,也
不会在清理过程中引入错误。
114 Streams示例
为了演示如何在实际中实现这些模式,下面将给出一些使用 Streams API的例子。之所以
使用这个AP,是因为它相对简单,而且它是与Kaka一起发布的,用户可以直接使用它。
不过要记住一点,我们可以使用任意的流式处理框架和软件包来实现这些模式,这些模式
具有通用性。
Kafka有两个基于流的AP,一个是底层的 Processor apl,一个是高级的 Streams dsL。下
面的例子中将使用 StreamS dSL。通过为事件流定义转换链可以实现流式处理。转换可以
是简单的过滤器,也可以是复杂的流与流的连接。我们可以通过底层的API实现自己的转
换,不过没必要这么做。
在使用 DSL API时,一般会先用 Stream Builder创建一个拓扑( topology)。拓扑是一个有
向图(DAG),包含了各个转换过程,将会被应用在流的事件上。在创建好拓扑后,使用
拓扑创建一个 Kafkastreams执行对象。多个线程会随着 Kafkastreams对象启动,将拓扑应
用到流的事件上。在关闭 Kafkastreams对象时,处理也随之结束。
下面将展示一些使用 Streams来实现上述模式的例子。字数统计这个例子用于演示map与fer
模式以及简单的聚合,另一个例子是计算股票交易市场的各种统计信息,用于演示基于时间窗
口的聚合,最后使用填充点击事件流( Click Stream Enrichment)的例子来演示流的连接。
114.1字数统计
先看一个使用了Streams的字数统计示例。完整的示例代码可以在GitHub(httpsgithub
com/ gwenshap/kaka- streams- wordcount)上找到。
要创建一个流式处理应用,首先需要配置 Kafka streams引擎。 Katk streams有很多配
置参数,这里就不展开讨论了,感兴趣的读者可以在官方文档里查看。另外,也可以将
生产者和消费者内嵌到 Kafka streams引擎里,只要把生产者或消费者的配置信息添加到
Properties对象里即可
public class WordCount Example t
public static void main(String[] args) throws Exception
Properties props new Properties ();
props. put(Streams Config. APPLICATION_ID_CONFIG
count");1
props. put(StreamsConfig BOOTSTRAP SERVERS CONFIG
localhost:9092");②
props. put(StreamsConfig KEY_SERDE_CLASS_CONFIG
Serdes String(). getClass(). getName ());3
流式处理197props.put(Streams Config. VALUE_ SERDE CLASS CONFIG
Serdes String(). getClass(). getNameO)
每个 Streams应用程序必须要有一个应用I。这个ID用于协调应用实例,也用于命名
内部的本地存储和相关主题。对于同一个Kaka集群里的每一个 Streams应用来说,这
个名字必须是唯一的。
2 Streams应用程序从Kaka主题上读取数据,并将结果写到Kaka主题上,所以我们要
告诉应用程序如何找到Kaka。稍后我们将介绍, Streams应用程序也使用Kaka作为
协调工具。
③在读写数据时,应用程序需要对消息进行序列化和反序列化,因此提供了默认的序列化
类和反序列化类。如果有必要,可以在稍后创建拓扑时覆盖默认的类。
做好配置之后,下面开始创建拓扑。
<StreamBuilder bui lder new KStreamBuilder(;1
STream<String, String> source
builder. stream("wordcount-input")
final Pattern pattern Pattern compile("\\W+");
STream counts = source. flatMapValues(value->
Arrays. asList(pattern. split(value toLower Case))))2
map((key, value)-> new KeyValuecObject
object>(value, value))
filter((key, value)->(! value equals("the )))3
group ByKey ()4
count("CountStore"). mapValues(value->
Long toString (value)). tostream(; 5
counts to("wordcount-output "):6
①创建一个 KStreamBuilder对象,并定义了一个流,将它指向输入主题。
2从主题上读取的每一个事件就是一行文字,首先使用正则表达式将它拆分为一系列单
词,然后将每个单词(事件的值)作为事件的键,这样就可以执行 group by操作了。
③将单词the过滤掉,过滤操作看起来很简单。
④根据键执行 group by操作,这样就得到了一个不重复的单词集合。
⑤计算每个集合里的事件数。计算的结果是Long类型,将它转成 String类型,方便从
Kafka上读取结果。
6最后把结果写回 Kafka。
定义好转换的流程后,应用程序将会运行这个流程,接下来要做的就是运行它。
KafkaStreams streams= new KafkaStreams(builder, props);0
streams. start();②
//一般情况下, Streams应用程序会一直运行下去
//本例中,只让它运行一段时间,然后停掉它,因为数据是有限的
Thread. sleep(5000L);
streams. close();③
198第11章o基于拓扑和配置属性定义一个 KafkaStreams对象
2启动 Kafka Streams引擎。
8过一段时间后将它停掉
就是这么简单!本例只用了几行代码,就演示了如何实现单事件处理模式(在事件上使用
了map与fler),然后通过 group by操作对数据进行重新分区,并为统计记录个数维护了
一个简单的本地状态。
建议运行完整的示例, Github库的 README文件包含了如何运行示例的说明。
也许你会注意到,除了Kaka外,不需要在机器上安装任何软件,就可以运行完整的示
例。这类似于在“本地模式”下使用 Spark。主要的不同之处在于,如果主题包含了多个
分区,就可以运行多个字数统计应用实例(在不同的命令行终端运行),而这也就是第
个 Streams集群。几个字数统计应用实例之间互相交互,协调处理任务。 Spark的本地模
式非常简单,但要在生产环境运行集群,需要安装YARN或者Meos,并在所有的机器上
安装 Spark,然后将你的应用提交到集群上,所以 Spark有较高的准入门槛。而如果使用
Streams API,只需要启动几个应用实例就可以拥有一个集群。在开发机上运行和在生产环
境中运行几乎是一样的。
11.42股票市场统计
接下来的这个例子会复杂一些,下面将从一个股票交易事件流里读取事件,这些事件包含
了股票代码、沽盘价和要价规模。在股票交易里,沽盘价是指卖方的出价,买入价是指买
方建议支付的价格,要价规模是指卖方愿意在相应价格基础上出售的股数。为了简单起
见,这里直接忽略竞标过程。数据里不会包含时间戳,相反,我们会使用由Kaka生产者
计算得出的“事件时间”。
下面将创建一个包含了若干时间窗口统计信息的输出流:
每5秒钟内最好的(最低的)沽盘价;
每5秒钟内交易的股数;
每5秒钟内平均沽盘价
这些统计信息每秒钟会更新一次。
为了简单起见,假设交易所只有10支不同的股票。应用的参数设置与字数统计示例
很相似。
Properties props new Propertieso
props. put(StreamsConfig APPLICATION_ID_CONFIG, "stockstat")
props. put(Streams Config. BOOTSTRAP SERVERS CONFIG
Constants. BROKER);
props. put(Streams Config. KEY SERDE CLASS CONFIG
Serdes String(). getClass().getName()
props. put(Streams Config. VALUE SERDE CLASS CONFIG
流式处理199TradeSerde. class getName ());
主要的不同在于这次使用的 Serde类是不一样的。在字数统计应用里,键和值的类型都是
String,所以使用了 Serdes. String()类作为序列化器和反序列化器。而在这个例子里,键
仍然是一个字符串,但值是一个 Trade对象,它包含了股票代码、沽盘价和要价规模。为
了序列化和反序列化这个对象(还包括应用里用到的其他几种对象),使用了 Google的
GsOn类库。借助这个类库,可以生成JSon序列化器和反序列化器,然后创建一个包装类,
用于生成 Serde对象。
static public final class Trade Serde extends Wrapper Serde<Trade> i
public Trade Serdeo)t
super (new Json Serializer<Trade>o
new JsonDeserializer<Trade>(Trade. class));
这里没有什么蹊跷的,只是要记得为存储在Kaka里的每一个对象提供一个 Serde对
象—输入、输出和中间结果。为了简化这个过程,建议使用GSon、Awro、 Protobuf等框
架来生成 Serde。
在配置好以后,下面开始构建拓扑
KStreamsTickerWindow, TradeStats> stats source groupByKey()
aggregate(TradeStats: new, 2
(k, v, tradestats)-> tradestats add(v),3
TimeWindows of(5000). advanceBy (1000),4
new TradeStats Serde(,5
"trade-stats-store )6
toStream((key, value)-> new TickerWindow(key key()
key window(). start())
mapValues((trade)-> trade. computeAvgPrice());8
stats to(new TickerWindowSerde(), new TradeStatsSerde(
stockstatsoutput");9
0本例从输入主题上读取事件并执行一个 groupByKey()操作开始。这个方法虚有其名,
实际上并不会执行任何分组操作。不过,它会确保事件流按照记录的键进行分区。因为
在写数据时使用了键,而且在调用 groupByKey()方法之前不会对键进行修改,数据仍
然是按照它们的键进行分区的,所以说这个方法不会做任何事情。
②在确保正确的分区之后,开始进行基于时间窗口的聚合。 aggregate方法将流拆分成相
互叠加的时间窗口(毎秒钟出现一个5秒钟的时间窗口),然后在时间窗口内的所有事
件上应用聚合方法。这个方法的第一个参数是一个新的对象,用于存放聚合的结果,也
就是 Tradestats。我们创建这个对象,并用它存放每个时间窗口的统计信息—最低价
格、平均价格和交易数量。
③提供了一个方法对记录进行聚合, Tradestats的add方法用于更新窗口内的最低价格、
交易数量和总价。
4定义了5s(500ms)的时间窗口,并且每秒钟都会向前滑动。
⑤提供了一个5erde对象,用于序列化和反序列化聚合结果( Tradestats对象)。
G在介绍模式时曾经说过,基于时间窗口的聚合需要维护本地状态。聚合方法的最后一个
200第11章参数就是本地状态存储的名字,它可以是任意具有唯一性的名字
⑦聚合结果是一个表,包含了股票信息,并使用时间窗口作为主键、聚合结果作为值。它
表示一条记录,以及从变更流中计算得出的特定状态(参考“概念”一节有关流和表二
元性的讨论)。这里想将表重新转成事件流,不过不再使用整个时间窗口作为键,而是
使用一个包含了股票信息和时间窗口起始时间的对象。 tostream方法将表转成一个流
并将键转成 Tickerwindow对象。
⑧最后一步是更新平均价格。现在,聚合结果里包含了总价和交易数量。遍历所有的记
录,并使用现有的统计信息计算平均价格,然后把它写到输出流里
⑨最后将结果写到 stockstats- output流里。
定义好流程之后,用它生成 Kafkastreams对象,并运行它,就像之前的“字数统计”那个
例子一样。
这个例子展示了如何在一个流上面进行基于时间窗的聚合,这也许就是最为流行的流式处
理使用场景。大家可以发现,为聚合维护一个本地状态是一件多么简单的事情,只需要提
供一个 Serde对象和状态存储的名字。不仅如此,这个应用还能扩展到多个实例,如果有
实例失效,它的分区将会被分配给其他存活的实例,从而实现自动的故障恢复。在11.5节
将介绍更多有关这种机制的实现原理
完整的例子和运行说明可以在GitHub(htps:/github.com/gwenshap/kafka-streams-stockstats)
上找到。
11.4.3填充点击事件流
最后一个例子将通过填充网站点击事件流来演示如何进行流的连接。本例首先生成一个模
拟点击的事件流,一个虚拟用户信息数据库表的更新事件流和一个网站搜索事件流,然后
将这3个流连接起来,从而得到用户活动的360°视图,比如用户每分钟的搜索内容、点
击的内容以及感兴趣的内容。这些连接操作为数据分析提供了丰富的数据集。产品推荐
般就是基于这些信息进行的,比如用户搜索了自行车,点击了“Trek”的链接,并且爱好
旅游,那么就可以向用户推荐Trek自行车、头盔和具有异国情调的自行车骑行活动(比如
去内布拉斯加州)。
应用的配置与前一个例子很相似,所以这里跳过这一步,直接进入到构建连接流要使用到
的拓扑。
STream<Integer, PageView> views
builder. stream( Serdes Integer()
new PageViewSerde(), Constants. PAGE_ VIEW_ TOPIC); O
KStream<Integer, Search> searches
builder. stream( Serdes Integer(), new SearchSerdeo)
Constants. SEARCH TOPIC)
KTable<Integer, UserProfile> profiles
builder. table(Serdes Integer () new Pro-fileSerde(
Constants. USER_PROFILE_TOPIC,"profile-store");(2
STream<Integer, UserActivity> viewsWithProfile views leftJoin(profiles, 3
(page, profile)-> new UserActivity(profile. getUserIDO
profile getUser Name o), profile. getzipcodeo)
profile. getInterestso),, page getPage ());4
流式处理20KStream<Integer, UserActivity> userActivitykstream
viewswithProfile. leftJoin(searches,
userActivity, search)->
userActivity. updateSearch(search. getSearchTerms()),6
Joinwindows of(1000), Serdes Integer()
new UserActivitySerde(), new Search Serde());7
①首先为点击事件和搜索事件创建流对象。
2为用户信息定义一个 KTable。 KTable是本地缓存,可以通过变更流来对其进行更新。
3将点击事件流与信息表连接起来,将用户信息填充到点击事件里。在一个流和表的连接
操作里,每个事件都会收到来自信息表缓存副本里的信息。这是一个左连接操作,所以
如果有的点击事件没有匹配的用户信息,这些事件仍然会被保留下来。
④这是连接方法,它接受两个参数,一个来自事件流,一个来自表记录,并返回一个值。
如果是在数据库里,必须知道如何将两个值合并成一个结果。这里创建了一个 activity
对象,它包含了用户的详细信息和用户浏览过的页面。
⑤接下来要将点击信息和用户的搜索事件连接起来。这也是一个左连接操作,不过现在连
接的是两个流,而不是流和表。
⑥这是连接方法,这里只是简单地将搜索关键词添加到匹配的页面视图。
⑦这是最有意思的部分,流和流之间的连接是基于相同的时间窗口。如果只是把每个用户
所有的点击事件和所有的搜索事件连接起来,并没有什么意义,我们要把具有相关性的
搜索事件和点击事件连接起来。具有相关性的点击事件应该发生在搜索之后的一小段时
间内。所以这里定义了一个一秒钟的连接时间窗口。在搜索之后的一秒钟内发生的点击
事件才被认为是具有相关性的,而且搜索关键词也会被放进包含了点击信息和用户信息
的活动记录里,这样有助于对搜索和搜索结果进行全面的分析。
定义了流程之后,用它生成 Kafkastreams对象,并运行它,就像之前的“字数统计”那个
例子一样。
这个例子演示了两种不同类型的连接模式,一个是连接流和表,用于将表里的信息填充到
流的事件里。这个与在数据仓库里运行查询时加入一个维度的事实表有点相似。第二个是
连接基于时间窗口的两个流。这种操作只会在流式处理中出现。
完整的例子和运行说明可以在Gtub(htps:/github.com/wenshan/kafka-clickstream-enrich/
tree/master)上找到。
11.5 Kafka Streams的架构概览
114.3节的例子演示了如何使用 Streams APl实现流式处理的设计模式。为了更好地理解
Streams的工作原理和它的伸缩性,下面深入了解AP背后的设计原则。
11.5.1构建拓扌
毎个流式应用程序至少会实现和执行一个拓扑。拓扑(在其他流式处理框架里叫作DAG,
即有向无环图)是一个操作和变换的集合,每个事件从输入到输出都会流经它。在之前的
字数统计示例里,拓扑结构如图11-10所示。
202第11章输入主题:(拆分成单同
键一单词
过滤掉the
本地
按照键分组
输出主题
单词数量
(计数器
重分区主题
图11-10:字数统计示例的拓扑结构
哪怕是一个很简单的应用,都需要一个拓扑。拓扑是由处理器组成的,这些处理器是拓扑
图里的节点(用椭圆表示)。大部分处理器都实现了一个数据操作—过滤、映射、聚合
等。数据源处理器从主题上读取数据,并传给其他组件,而数据池处理器从上一个处理器
接收数据,并将它们生成到主题上。拓扑总是从一个或多个数据源处理器开始,并以一个
或多个数据池处理器结束。
11.52对拓扑进行伸缩
Streams通过在单个实例里运行多个线程和在分布式应用实例间进行负载均衡来实现伸缩。
用户可以在一台机器上运行 Streams应用,并开启多个线程,也可以在多台机器上运行
Streams应用。不管采用何种方式,所有的活动线程将会均衡地处理工作负载。
Streams引擎将拓扑拆分成多个子任务来并行执行。拆分成多少个任务取决于 Streams引
擎,同时也取决于主题的分区数量。每个任务负责一些分区:任务会订阅这些分区,并
从分区读取事件数据,在将结果写到数据池之前,在每个事件上执行所有的处理步骤
这些任务是 Streams引擎最基本的并行单元,因为每个任务可以彼此独立地执行,如图
11-11所示。
任务1
主题
分区1
输出主题
分区2
任务2
图11-11:运行相同拓扑的两个任务—每个读取主题的一个分区
流式处理203开发人员可以选择每个应用程序使用的线程数。如果使用了多个线程,每个线程将会执行
部分任务。如果有多个应用实例运行在多个服务器上,每个服务器上的每一个线程都会
执行不同的任务。这就是流式应用的伸缩方式:主题里有多少分区,就会有多少任务。如
果想要处理得更快,就添加更多的线程。如果一台服务器的资源被用光了,就在另一台服
务器上启动应用实例。Kaka会自动地协调工作,它为每个任务分配属于它们的分区,每
个任务独自处理自己的分区,并维护与聚合相关的本地状态,如图112所示。
服务器1
线程1
任务1
主题
任务2
分区1
分区2
分区3
服务器2
线程2
分区4
任务3
任务4
图11-12:处理任务可以运行在多个线程和多个服务器上
大家或许已经注意到,有时候一个步骤需要处理来自多个分区的结果,这样就会在任务之
间形成依赖。例如,在点击事件流的例子里对两个流进行了连接,在生成结果之前,需要
从每一个流的分区里获取数据。 Streams将连接操作所涉及的分区全部分配给相同的任务,
这样,这个任务就可以从相关的分区读取数据,并独立执行连接操作。这也就是为什
Streams要求同一个连接操作所涉及的主题必须要有相同数目的分区,而且要基于连接所
使用的键进行分区。
如果应用程序需要进行重新分区,也会在任务之间形成依赖。例如,在点击事件流的例子
里,所有的事件使用用户⑩D作为键。如果想要基于页面或者邮政编码生成统计信息该怎
么办?此时就需要使用邮政编码对数据进行重新分区,并在新分区上运行聚合操作。如果
任务1处理来自分区1的数据,这些数据到达另一个处理器,这个处理器对数据进行重新
分区( groupBy操作),它需要对数据进行 shuffle,也就是把数据发送给其他任务进行处
204第11章理。与其他流式处理框架不一样的是, Streams通过使用新的键和分区将事件写到新的主
题来实现重新分区,并启动新的任务从新主题上读取和处理事件。重新分区的步骤是将拓
扑拆分成两个子拓扑,每个子拓扑都有自己的任务集,如图11-13所示。第二个任务集依
赖第一个任务集,因为它们处理的是第一个子拓扑的结果。不过,它们仍然可以独立地并
行执行,因为第一个任务集以自己的速率将数据写到一个主题上,而第二个任务集也以自
己的速率从这个主题读取和处理事件。两个任务集之间不需要通信,也没有共享资源,而
且它们也不需要运行在相同的线程里或相同的服务器上。这是 Kafka提供的最有用的特性
之一减少管道各个部分之间的依赖
任务1
任务3
主题
重分区主题
分区1
分区1
分区2
任务2
分区
任务4
图11-13:处理主题分区事件的两组任务
11.5.3从故障中存活下来
Streams的伸缩模型不仅允许伸缩应用,还能优雅地处理故障。首先,包括本地状态在内
的所有数据被保存到有高可用性的Kaka上。如果应用程序出现故障需要重启,可以从
Kafka上找到上一次处理的数据在流中的位置,并从这个位置开始继续处理。如果本地状
态丢失(比如可能需要将服务器替换掉),应用程序可以从保存在Kaka上的变更日志重
新创建本地状态。
Streams还利用了消费者的协调机制来实现任务的高可用性。如果一个任务失败,只要还
有其他线程或者应用程序实例可用,就可以使用另一个线程来重启该任务。这类似于消费
者群组的故障处理,如果一个消费者失效,就把分区分配给其他活跃的消费者。
116流式处理使用场景
前面已经讲解了如何进行流式处理—从一般性的概念和模式说起,并列举了一些
Streams的例子。现在是时候让大家知道流式处理都有哪些常见的使用场景了。本章的开
头解释过,如果想快速处理事件,而不是为毎个批次等上几个小时,但又不是真的要求毫
秒级的响应,那么流式处理(或者说持续处理)就可以派上用场了。话是没错,不过听起
来仍然十分抽象。下面来看一些例子,它们都使用流式处理来解决实际的问题。
流式处理205客户服务
假设你向一个大型的连锁酒店预订了一个房间,并希望收到邮件确认和票据。在预订了
几分钟之后,仍然没有收到确认邮件,于是打电话向客服确认。客服的回复是:“我在
我们的系统里看不到订单,不过从预订系统加载数据的批次作业每天只运行一次,所以
请明天再打电话过来。你应该可以在2~3个工作日之后收到确认邮件。”这样的服务有
点糟糕,不过有人已经不止一次地在一家大型连锁酒店遭遇过类似的问题。我们真正需
要的是,连锁酒店的每一个系统在预订结束之后的几秒钟或者几分钟之内都能发出通
知,包括客服中心、酒店、发送确认邮件的系统、网站等。有的用户可能还希望客服中
心能够立即获知自己在这家连锁酒店的历史入住数据,前台能够知道他是一个忠实的客
户,从而提供更高级别的服务。如果使用流式处理应用来构建这些系统,就可以实现几
近实时的接收和处理这些事件,从而带来更好的用户体验。如果有这样的系统,就可以
在几分钟之内收到邮件确认,信用卡就可以及时扣款,然后发送票据,服务台就可以马
上回答有关预订房间的问题了。
物联网
物联网包含很多东西,从用于调节温度和自动添加洗衣剂的家居设备,到制药行业的实
时质量监控设备。流式处理在传感器和设备上应用,最为常见的是用于预测何时该进行
设备维护。这个与应用监控有点相似,不过这次是应用在硬件上,而且应用在很多不同
的行业——制造业、通信(识别故障基站)、有线电视(在用户投诉之前识别出故障机
顶盒)等。每一种场景都有自己的特点,不过目标是一样的处理大量来自设备的事
件,并识别出一些模式,这些模式预示着某些设备需要进行维护,比如交换机数据包的
下降、生产过程中需要更大的力气来拧紧螺丝,或者用户频繁重启有线电视的机顶盒。
欺诈检测。欺诈检测也被叫作异常检查,是一个非常广泛的领域,专注于捕捉系统
中的“作弊者”或不良分子,比如信用卡欺诈、股票交易欺诈、视频游戏作弊或者
网络安全风险。在这些欺诈行为造成大规模的破坏之前,越早将它们识别出来越好。
个几近实时的系统可以快速地对事件作出响应,停止一个还没有通过审核的交易
要比等待批次作业在3天之后才发现它是一个欺诈交易要更容易处理。这也是一个
在大规模事件流里识别模式的问题。
在网络安全领域,有一个被称为发信标( beaconing)的欺诈手法,黑客在组织内部
放置恶意软件,该软件时不时地连接到外部网络接收命令。一般来说,网络可以抵
挡来自外部的攻击,但难以阻止内部到外部的突围。通过处理大量的网络连接事件
流,识别出不正常的通信模式,检测出该主机不经常访问的某些P地址,在蒙受更
大的损失之前向安全组织发出告警。
11.7如何选择流式处理框架
在比较两个流式处理系统时,要着重考虑使用场景是什么。以下是一些需要考虑的应
用类别。
摄取
摄取的目的是将数据从一个系统移动到另一个系统,并在传输过程中对数据进行一些修
改,使其更适用于目标系统。
206第11章低延迟
任何要求立即得到响应的应用。有些欺诈检测场景就属于这一类。
异步微服务
这些微服务为大型的业务流程执行一些简单操作,比如更新仓储信息。这些应用需要通
过维护本地状态缓存来提升性能。
几近实时的数据分析
这些流式媒体应用程序执行复杂的聚合和连接,以便对数据进行切分,并生成有趣的业
务见解。
选择何种流式处理系统取决于要解决什么问题。
如果要解决摄取问题,那么需要考虑一下是需要一个流式处理系统还是一个更简单的专
注于摄取的系统,比如 Kafka connect。如果确定需要一个流式处理系统,那就要确保
它拥有可用的连接器,并且要保证目标系统也有高质量的连接器可用。
如果要解决的问题要求毫秒级的延迟,那么就要考虑一下是否一定要用流。一般来说
请求与响应模式更加适用于这种仼务。如果确定需要一个流式处理系统,那就需要选择
个支持低延迟的模型,而不是基于微批次的模型。
如果要构建异步微服务,那么需要一个可以很好地与消息总线(希望是Kaka)集成的
流式处理系统。它应该具备变更捕捉能力,这样就可以将上游的变更传递到微服务本地
的缓存里,而且它要支持本地存储,可以作为微服务数据的缓存和物化视图
如果要构建复杂的数据分析引擎,那么也需要一个支持本地存储的流式处理系统,不过
这次不是为了本地缓存和物化视图,而是为了支持高级的聚合、时间窗口和连接,因为
如果没有本地存储,就很难实现这些特性。API需要支持自定义聚合、基于时间窗口的
操作和多类型连接。
除了使用场景外,还有如下一些全局的考虑点。
系统的可操作性
它是否容易部署?是否容易监控和调试?是否易于伸缩?它是否能够很好地与已有的基
础设施集成起来?如果出现错误,需要重新处理数据,这个时候该怎么办?
API的可用性和调试的简单性
为了开发出高质量的应用,同一种框架的不同版本可能需要耗费不同的时间,这类情况
很常见。开发时间和上市时机太重要了,所以我们需要选择一个高效率的系统。
让复杂的事情简单化
几乎每一个系统都声称它们支持基于时间窗口的高级聚合操作和本地缓存,但问题是,
它们够简单吗?它们是处理了规模伸缩和故障恢复方面的细节问题,还是只提供了脆弱
的抽象,然后让你来处理剩下的事情?系统提供的API越简洁,封装的细节越多,开
发人员的效率就越高。
社区
大部分流式处理框架都是开源的。对于开源软件来说,一个充满生气的社区是不可替代
的。好的社区意味着用户可以定期获得新的功能特性,而且质量相对较高(没有人会使
流式处理207用糟糕的软件),缺陷可以很快地得到修复,而且用户的问题可以及时得到解答。这也
意味着,如果遇到一个奇怪的问题并在 Google上搜索,可以搜索到相关的信息,因为
其他人也在使用这个系统,而且也遇到了相同的问题。
11.8总结
本章的开头解释了流式处理,给出了流式处理范式的规范定义,介绍了它的一些常见属
性,并将它与其他编程范式进行了比较。
然后列举了3个基于 Kafka Streams开发的应用程序,以此来解释一些非常重要的流式处
理概念。
在详述了这些示例之后,我们给出了 Kafka streams的架构概览,并解释了它的内部原理
最后提供了一些流式处理的使用场景,给出了一些用于比较流式处理框架的建议,并以此
结束本书。
208第11章附录A
在其他操作系统上安装Kaa
Kafka基本上就是一个Java应用程序,所以它可以运行在任何一个可以安装JRE的系统
上。不过,它针对 Linux系统进行了优化,因此可以获得最好的性能。如果让它运行在
其他系统上,有可能会出现与特定操作系统相关的问题。所以,在桌面操作系统上进行
Kaka开发或测试时,最好能够让它运行在虚拟机里,这个虚拟机最好能与生产环境的配
置相匹配。
A.1在 Windows上安装 Kafka
截止到 Windows10,可以通过两种方式来运行Kaka。传统的方式是使用本地的Java安装
包。 Windows0用户还可以使用 Windows的 Linux子系统。推荐使用第二种方式,因为
它提供了与生产环境最为相似的配置,所以这里就从这种方式开始说起。
A.1.1使用 Windows的 Linux子系统
如果使用的是 Windows10,就可以通过 Windows的 Linux子系统(WSL)来安装原生的
Ubuntu支持。在本书英文版出版时,微软仍然只是把它当成一个实验特性。它有点像是
个虚拟机,但不像虚拟机那样需要那么多资源,却提供了更丰富的系统集成。
可以按照微软开发者网络 Bash on Ubuntu on Windows(htps:/ msdn. microsoft. com/en-us
commandline/ wsl/about)页面所提供的说明来安装WSL。安装完WSL后,需要通过apt
get来安装JDK。
s sudo apt-get install open jdk-7-jre-headless
[sudo] password for username:
209Reading package 1
Don
Building dependency tree
Reading state information.. Done
d
安装完JDK后,再根据第2章的说明来安装Kaka。
A.12使用本地Java
如果你使用的是旧版本的 Windows,或者不想使用WSL环境,那么可以使用 Windows
的Java环境来运行Kaka。不过要注意的是,这可能会引入 Windows环境所特有的问题。
Kaka开发社区可能不会注意到这些问题,因为Kaka的开发者很少会在 Windows上运行
Kafka。
在安装 Zookeeper和Kaka之前,必须有一个Java环境。建议安装最新版本的 Oracle java8,
可以在 Oracle Java SE下载页面找到最新版的JDK。下载完整版的JDK,这样就可以拥有所
有的Java工具,然后按照指示一步一步安装JDK即可
注意安装路径
在安装Java和Kaka时,建议把它们安装在不包含空格的目录里
Windows允许路径里包含空格,但基于UNX环境设计的应用程序不支持
包含空格的路径,而且要指定不同的路径也很困难。在安装Java时要谨
记这一点。例如,如果安装JDK1.8 update121,可以把它安装在 C: aval
jdk1.8.0_121路径中
在安装完Java后,要设置环境变量。环境变量可以在 Windows的控制面板里设置,根
据操作系统版本的不同,它的位置可能不一样。在 Windows10中,先选择“系统和安
全”,再选择“系统”,然后单击“计算机名、域和工作组设置”里的“更改设置”按钮,
这样就可以打开一个“系统属性”窗口,选择“高级”选项卡,然后单击“环境变量”
按钮。这里添加一个名为 JAVA HOME的用户变量(图A-1),并把它的值设为Java的
安装目录,然后修改Path系统变量,往里面添加一个新条目% JAVA HOME%bin。保
存配置,退出控制面板。
接下来可以安装Kaka了。安装包里已经包含了 Zookeeper,所以不需要再单独安装
Zookeeper。当前版本的Kaka可以从hp:/ kafka. apache. org// downloads html上下载。在本
书英文版出版时, Kafka的版本是0.10.1.0,相应的 Scala版本是211.0。下载的文件经过
Gzip压缩,并通过tr来打包,所以需要 Windows的加压缩工具(如8zip)来解压。与在
Linux系统上安装 Kafka类似,必须为Kaka选择一个解压目录。这里假设Kaka被解压
到C:kaka_2.11-0.10.1.0。
210附录Aystem Properties
Computer Name Hardware Advanced System Protection Remote
You must be logged on as an Admini
Environment Variables
Visual effects, processor schedul
User variables for live
Variable
Value
User Profiles
ChocolateyLastPathUpdate Mon Apr 17 10. 51:22 2017
Chocolatey ToolsLocation C:tools
Desktop settings related to your sig
C:Users\live\One Drive
%USERPROFILE%VAPp Data\Local \Microsoft\WindowsApps
TEMP
%USERPROFILE%VApp Data\Local\Temp
Startup and Recovery
TMP
%USERPROFILE%VAppData \Local\Temp
JAVA HOME
Variable value:
CAva\dk1.8.0_121
rowse Eil
MBER OF_ PROCESSORS 4
Windows NT
Path
PATHEXT
COM: EXE: BAT: CMD: VBS: VBE JS; JSE: WSF. WSH: MSC
PROCESSOR ARCHITECTURE AMD64
PROCESSOR_IDENTIFIER Intel64 Family 6 Model 78 Stepping 3, GenuineIntel
Cancel
图A-1:添加 JAVA HOME变量
在 Windows下运行 Zookeeper和Kaka有一点不一样,因为必须使用 Windows特有的批处
理文件而不是 shell脚本。批处理文件不支持在后端运行应用程序,所以每一个应用程序都
需要一个单独的 shell。先启动 Zookeeper
PSC:}> cd kafka_2.11-0.10.2.0
PS C:\kafka _2.11-0.10.2. 0> bin/windows/zookeeper-server-start bat C:
\kafka2. 11-0.10.2.\configzookeeper. properties
[2017-04-26 16: 41: 51, 529] INFO Reading configuration from: C:
\kafka_2.11-0.10.2.0\config\zookeeper. properties (org.apache.zoo
keeper server. quorum. QuorumPeer) pg
[2017-04-26 16: 41: 51, 595] INFO minSessionTimeout set to-1(org.apache.Zoo
keeper server ZooKeeper Server
[2017-04-26 16: 41: 51, 596] INFO max SessionTimeout set to-1 (org.apac
che. zoo
keeper server ZooKeeper Server)
[2017-04-2616:41:51,673]INF0 binding to port6.0.0.0/.0.0.0:2181
org. apache zookeeper. server. NIOServer Cnxn Factory
在 Zookeeper开始运行之后,打开另一个窗口来启动Kaka:
在其他操作系统上安装Kka|211PSc:> cd kafka_2.11-0,10.2.0
PS C: kafka_2.11-0.10.2.0>.\bin\windows\ kafka-server-start bat C
\kafka_2. 11-0.10.2.0\config\server properties
[2017-04-26 16: 45: 19, 804] INFO Kafka Config values
[2017-04-26 16: 45: 20, 697] INFO Kafka version: 0.10.2.0(org. apache. kafka. com
mon utils. AppInfoParser
[2617-04-2616:45:20,706] INFO Kafka commitId:576d93a8dc6cf421
org. apache. kafka. commonutils. AppInfoParser
[2017-04-26 16: 45: 20, 717] INFO [Kafka Server 01, started(kafka. server. Kafka
Server)
A2在 MacOs上安装Kaka
苹果的 Macos运行在 Darwin上,一个基于 FreeBSD的UNx操作系统。也就是说,在
MacOs上运行应用程序与在UNIX系统上是差不多的,所以在 Macos上安装为UNX而
设计的应用程序(如Kaka)不会有太大难度。可以通过包管理器(如 Homebrew)来安装
Java和Kaka,也可以手动安装它们,这样可以自由选择版本。
A.2.1使用 Homebrew
如果已经在 MacOs上安装了 Homebrew(htps:/hrew.sh),就可以用它来安装Kaka。这样
可以确保先安装Java,然后再安装Kaka0.1020(在本书英文版出版时)
如果还没有安装 Homebrew,请先按照 Homebrew安装页面(hps/ /docs brew sh/ Installation
htm)上提供的步骤安装 Homebrew,然后用它来安装 Kafka。 Homebrew会确保先安装所有
的依赖项,包括Java
s brew install kafk
Installing kafka dependency zookeeper
==> Summary
/usr/local/Cellar/kafka /0.10.2.0: 132 files, 37 2MB
Homebrew会将Kaka安装到/ usr/local/ cellar目录,不过文件会被链接到其他目录
二进制文件和脚本文件在/sr/ocal/bin目录下。
Kafka配置文件在/usr/oca/etc/ kafka目录下。
Zookeeper B配置文件在/ usr/local/etc/ zookeeper目录下。
log dirs( Kafka的数据目录)被设置为/ usr/local/var/ lib/kafka-logs。
安装完毕后,就可以启动 Zookeeper和 Kafka(把Kaka运行在前台)了:
s / usr/local/bin/zkServer start
JMX enabled by default
Using config: /usr/local/etc/zookeeper/zoo. cfg
Starting zookeeper .. STARTED
s kafka-server-start. sh /usr/local/etc/kafka/server properties
212附录A2017-02-09 20: 48: 22, 485] INFo [Kafka Server o], started(kafka. server. Kafka
erver
A.2.2手动安装
在 Macos上手动安装Kaka与在 Windows上类似,需要先安装JDK。可以从 Oracle java
SE下载页面上找到 MacOs的JDK版本,然后下载Kaka。这里假设下载的 Kafka被解压
到/usr/oca/kaka_2110.10.2.0目录。
在 Macos上启动 Zookeeper和Kaka与在 Linux上类似,只不过要确保设置了正确的
JAVA HOME变量:
s export JAVA_HOME='/usr/libexec/java_home
s echo $JAVA_ HOME
ibrary /Java/ Javavirtua LMachines /jdk180.131 jdk/ Contents/Home
s usr/local/kafka_ 2. 11-0.10.2.0/bin/zookeeper-server-start sh -daemon /usr.
local/kafka2. 11-0.10.2.0/config/zookeeper. properties
s/usr/local/kafka_2. 11-0.10.2.0/bin/kafka-server-start sh /usr /local/etc/kafka
server. propertles
[2017-04-26 16: 45: 19, 804]. INFO KafkaConfig values
[2017-04-2616:45:20,697]InFoKafkaversion:0.10.2.0(org.apachekafka.com
mon utils. AppInfoParser)
[2017-04-2616:45:20,706] INFO Kafka commitId:576d93a8dc0cf421
(org. apache. kafka. common utils. AppInfoParser)
[2017-04-26 16: 45: 20, 717] INFO [Kafka Server 0], started(kafka. server. Kafka
Server
在其他操作系统上安装Kaka|213作者介绍
Neha Narkhede, Confluent(Kaka背后的公司)的联合创始人兼工程主管。在创立 Confluent
之前,Neha在 LinkedIn领导流式基础设施团队,基于 Kafka和 Apache Samza构建PB级
别的流式基础设施。Neha擅长构建大型可伸缩的分布式系统,是Kaka的最初作者之一。
她曾经在 Oracle从事数据庳搜索方面的工作,拥有佐治亚理工大学计算机科学硕士学位。
Gwen Shapira, Confluent产品经理,同时也是Kaka项目的PMC( Apache项目管理委员
会)成员。她实现了Kaka与 Apache Flume的集成,同时也是 Apache Sqoop的贡献者之
。Gwen拥有15年与客户共同设计可伸缩数据架构的经验。她曾经是 Cloudera的一名软
件工程师、 Pythian的高级顾问、 Oracle ACE总监,以及 NoCOUG的董事会成员。Gwen
经常在各种行业大会上演讲,为多个行业博客撰写文章,包括 O Reilly radar
Todd palin, LinkedIn的高级网站可靠性工程师,负责部署和维护大型的 Kafka、
Zookeeper和 Hamza平台。他负责架构、日常运维和工具部署,包括创建高级的监控和通
知系统。Tod是开源项目 Burrow(一个 Kafka消费者监控工具)的开发者,经常在行业
大会和技术讲座上分享Kaka相关的经验。Todd有超过20年的基础设施服务相关经验,
在加λ LinkedIn之前,他是 Verisign的一名系统工程师,负责实施服务管理自动化,包括
DNS、网络和硬件的管理,并在公司范围内管理硬件和软件标准
封面介绍
本书封面上的动物是一只蓝翅笑翠鸟。笑翠鸟属于翠鸟科,生活在新几内亚南部和澳大科
亚北部。
雄性笑翠鸟拥有五颜六色的羽毛,翅膀和尾部的羽毛是蓝色的,雌性笑翠鸟的尾部则是红
棕色,带有黑色的条纹。雄性和雌性笑翠鸟都有奶油色的腹部,伴有棕色的条纹,它们的
虹膜是白色的。成年笑翠鸟的体型要比其他翠鸟小一些,平均身长38到43厘米,体重在
260克到330克之间。
蓝翅笑翠鸟偏爱肉食,捕食对象随季节稍有不同。在夏季,它们捕捉蜥蜴、昆虫和青蛙为
食,而在干燥的季节,它们则多以小龙虾、鱼、啮齿类动物和小型的鸟类为食。不过,在
以鸟类为食的食物链里,红鹰和红褐色的猫头鹰喜欢以蓝翅笑翠鸟为食。
9月到12月是蓝翅笑翠鸟的繁殖季节,它们把巢穴筑在树的上方,通过社区协作的方式养
育下一代,一对笑翠鸟至少会得到另外一只笑翠鸟的帮助。它们一般会花大概26天的时
间来孵蛋,毎次孵3到4只蛋。雏鸟如果能够正常破壳而出,那么大概36天后就会长出
羽毛。早出生的雏鸟会在第一周内尝试杀死更小的雏鸟。成年笑翠鸟会花上6到8周的时
间来训练那些存活下来的小鸟怎样捕食。
Oˆ Reilly封面上的很多动物都是濒危物种,它们对整个世界都很重要。如果你想为保护动
物做些贡献,可以访问animals.oreilly.com。
封面图片来自 English Cyclopedia
214微信连接
回回
回复“ Kafka”查看相关书单
微博连接
关注@图灵教育每日分享T好书
QQ连接
图灵读者官方群I:218139230
图灵读者官方群I:164939616
图灵社区
tUring. cn
在线出版,电子书,《码农》杂志,图灵访谈oRE|LLY°
Kafka权威指南
每个应用程序都会产生数据,包括日志消息、度量指标、用户活动记“ Kafka正在成为管理和处理流式
录、响应消息等。如何移动数据,几乎变得与数据本身一样重要。如果数据的利器…学会基于持续数
你是架构师、开发者或者产品工程师,同时也是 Apache Kafka新手,那据流构建应用程序着实是一个巨
么这本实践指南将会帮助你成为流式平台上处理实时数据的专家。
大的思维转变。借助本书来学习
Kafka再好不过了,从内部架构
本书由出身于 Linkedin的 Kafka核心作者和一线技术人员共同执笔,详细
到AP|,都是由极为了解 Kafka
介绍了如何部署 Kafka集群、开发可靠的基于事件驱动的微服务,以及基
的人亲手呈现的。我希望你们能
于 Kafka平台构建可伸缩的流式应用程序。通过详尽示例,你将会了解到
够像我一样喜欢这本书。”
afka的设计原则、可靠性保证、关键APl,以及复制协议、控制器和存
储层等架构细节。
Jay Kreps
Kafka核心作者,
Confluent联合创始人、CEO
■了解发布和订阅消息模型以及该模型如何被应用在大数据生态系
统中
Neha Narkhede, Confluent联合
■学习使用 Kafka生产者和消费者来生成消息和读取消息
创始人、CTO,曾在 LinkedIn主
■了解 Kafka保证可靠性数据传递的模式和场景需求
导基于 Kafka和 Apache Samza构
■使用 Kafka构建数据管道和应用程序的最佳实践
建流式基础设施,是 Kafka作者
之
■在生产环境中管理 Kafka,包括监控、调优和维护
Gwen Shapira, Confluent系统
■了解 Kafka的关键度量指标
架构师,帮助客户构建基于
■探索 Kafka如何成为流式处理利器
Kafka的系统,在可伸缩数据
架构方面拥有十余年经验;曾
任 Cloudera公司解决方案架构
师。另著有《 Hadoop应用架
构》
Todd palin, LinkedIn主任级
SRE,负责部署管理大型的
Kafka、 Zookeeper和 Hamza集
封面设计: Karen Montgomery张健
图灵社区: lUring.cn
TURING E废
热线:(010)51095186转600
www.ituring.com.cn
ISBN978-7-115-47327-1
来社区领银子
分类建议计算机/大数据技术/kaa
人民邮电出版社网址:www.ptpress.com.cn
OReilly Media,nc授权人民邮电出版社出版
9m787115473271
简体中文版仅限于中国大陆(不包含中国香港、澳门特别行政区和中国台湾地区)销售发行
This Authorized Edition for sale only in the territory of People's Republic of China(excluding
SBN9787-115-47327-1
Hong Kong, Macao and Taiwan)
定价:6900元版权注意事项:
1、书籍版权归作者和出版社所有
2、本PDF仅限用于个人获取知识,进行私底下的知识交流
3、PDF获得者不得在互联网上以任何目的进行传播
4、如觉得书籍内容很赞,请购买正版实体书,支持作者
5、请于下载PDF后24小时内删除本PDF。